{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e13715-3068-4562-9a60-30038676f048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:50:58.049503Z",
     "iopub.status.busy": "2023-04-06T05:50:58.049139Z",
     "iopub.status.idle": "2023-04-06T05:51:00.141779Z",
     "shell.execute_reply": "2023-04-06T05:51:00.140840Z",
     "shell.execute_reply.started": "2023-04-06T05:50:58.049474Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# import os\n",
    "# os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "\n",
    "\n",
    "# from dgl import function as fn\n",
    "# from dgl.nn.pytorch import GlobalAttentionPooling\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/global_attention.py\n",
    "from torch_geometric.nn import AttentionalAggregation, SAGEConv\n",
    "\n",
    "\n",
    "class GlobalAttentionNet(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.att = AttentionalAggregation(Linear(hidden, 1))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.att.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.att(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, dim_feat: int, edge_encoder: nn.Module, edge_feat: str=\"feat\"):\n",
    "        # https://mlabonne.github.io/blog/gin/\n",
    "        # TODO: must study this\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_feat, 2 * dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * dim_feat, dim_feat),\n",
    "        )\n",
    "        self.eps = nn.Parameter(th.Tensor([0]))\n",
    "        self.edge_encoder = edge_encoder\n",
    "        self._dim_feat = dim_feat\n",
    "        self._edge_feat = edge_feat\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        graph.ndata[\"h\"] = feat\n",
    "        graph.apply_edges(lambda edges: {\"e\": F.relu(edges.src[\"h\"] + self.edge_encoder(edges.data[self._edge_feat]).view((-1, self._dim_feat)))})\n",
    "        graph.update_all(fn.copy_e(\"e\", \"m\"), fn.sum(\"m\", \"a\"))\n",
    "        feat = self.mlp((1 + self.eps) * graph.ndata.pop(\"h\") + graph.ndata.pop(\"a\"))\n",
    "        return feat\n",
    "\n",
    "\n",
    "# class GraphNorm(nn.Module):\n",
    "#     def __init__(self, dim_feat: int):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.weight = nn.Parameter(th.ones(dim_feat))\n",
    "#         self.bias = nn.Parameter(th.zeros(dim_feat))\n",
    "#         self.mean_scale = nn.Parameter(th.ones(dim_feat))\n",
    "\n",
    "#     def forward(self, graph, feat):\n",
    "#         batch_list = graph.batch_num_nodes().to(device=graph.device, dtype=th.int64)\n",
    "#         batch_index = th.arange(graph.batch_size, device=graph.device).repeat_interleave(batch_list)\n",
    "#         batch_index = batch_index.view((-1,) + (1,) * (feat.dim() - 1)).expand_as(feat)\n",
    "\n",
    "#         mean = th.zeros(graph.batch_size, *feat.shape[1:], device=graph.device)\n",
    "#         mean = mean.scatter_add_(0, batch_index, feat)\n",
    "#         mean = (mean.T / batch_list).T\n",
    "#         mean = mean.repeat_interleave(batch_list, dim=0)\n",
    "\n",
    "#         sub = feat - mean * self.mean_scale\n",
    "\n",
    "#         std = th.zeros(graph.batch_size, *feat.shape[1:], device=graph.device)\n",
    "#         std = std.scatter_add_(0, batch_index, sub.pow(2))\n",
    "#         std = ((std.T / batch_list).T + 1e-6).sqrt()\n",
    "#         std = std.repeat_interleave(batch_list, dim=0)\n",
    "\n",
    "#         return self.weight * sub / std + self.bias\n",
    "\n",
    "    \n",
    "from pytorch_geometric.norm import GraphNorm\n",
    "\n",
    "class GNNBase(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer: str,\n",
    "            norm: str,\n",
    "            res: bool,\n",
    "            dim_feat: int,\n",
    "            depth: int,\n",
    "            edge_encoder: Callable[[], nn.Module],\n",
    "            *,\n",
    "            dropout: bool=True,\n",
    "            edge_feat: str=\"feat\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._dim_feat = dim_feat\n",
    "        self._depth = depth\n",
    "        self._res = res\n",
    "        self._dropout = dropout\n",
    "\n",
    "        if layer == \"gin\":\n",
    "            self.layers = nn.ModuleList(\n",
    "                [GINLayer(dim_feat, edge_encoder(), edge_feat) for _ in range(depth)]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"invalid layer type: {layer}\")\n",
    "\n",
    "        if norm == \"none\":\n",
    "            self.norms = None\n",
    "        elif norm == \"graphnorm\":\n",
    "            self.norms = nn.ModuleList([GraphNorm(dim_feat) for _ in range(depth)])\n",
    "        else:\n",
    "            raise ValueError(f\"invalid norm: {norm}.\")\n",
    "\n",
    "\n",
    "class GNNSimple(GNNBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.pooling = GlobalAttentionPooling(nn.Sequential(\n",
    "            nn.Linear(self._dim_feat, 2 * self._dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * self._dim_feat, 1),\n",
    "        ))\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        for d in range(self._depth):\n",
    "            feat_in = feat\n",
    "            feat = self.layers[d](graph, feat)\n",
    "            if self.norms:\n",
    "                feat = self.norms[d](graph, feat)\n",
    "            if d < self._depth - 1:\n",
    "                feat = F.relu(feat)\n",
    "            if self._dropout:\n",
    "                feat = F.dropout(feat, training=self.training)\n",
    "            if self._res:\n",
    "                feat = feat + feat_in\n",
    "\n",
    "        return self.pooling(graph, feat)\n",
    "\n",
    "    def get_emb(self, graph, feat):\n",
    "        return self.forward(graph, feat).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b110848-4b7a-4ce6-ba55-9d77b42e7915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:28:17.469805Z",
     "iopub.status.busy": "2023-04-06T05:28:17.468315Z",
     "iopub.status.idle": "2023-04-06T05:28:18.215818Z",
     "shell.execute_reply": "2023-04-06T05:28:18.215019Z",
     "shell.execute_reply.started": "2023-04-06T05:28:17.469778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 05:28:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   34C    P8     6W / 180W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d794c77-07ba-4060-be3e-0bf66b6024fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:04.036531Z",
     "iopub.status.busy": "2023-04-06T05:51:04.036034Z",
     "iopub.status.idle": "2023-04-06T05:51:04.049563Z",
     "shell.execute_reply": "2023-04-06T05:51:04.048514Z",
     "shell.execute_reply.started": "2023-04-06T05:51:04.036504Z"
    }
   },
   "outputs": [],
   "source": [
    "# from dgl.nn.pytorch import GlobalAttentionPooling\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLAPBase(GNNBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.poolings = nn.ModuleList(\n",
    "            [GlobalAttentionPooling(nn.Sequential(\n",
    "                nn.Linear(self._dim_feat, 2 * self._dim_feat),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2 * self._dim_feat, 1),\n",
    "            )) for _ in range(self._depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        self._graph_embs = []\n",
    "\n",
    "        for d in range(self._depth):\n",
    "            feat_in = feat\n",
    "            feat = self.layers[d](graph, feat)\n",
    "            if self.norms:\n",
    "                feat = self.norms[d](graph, feat)\n",
    "            if d < self._depth - 1:\n",
    "                feat = F.relu(feat)\n",
    "            if self._dropout:\n",
    "                feat = F.dropout(feat, training=self.training)\n",
    "            if self._res:\n",
    "                feat = feat + feat_in\n",
    "\n",
    "            self._graph_embs.append(self.poolings[d](graph, feat))\n",
    "\n",
    "        return self._aggregate()\n",
    "\n",
    "    def _aggregate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_emb(self, graph, feat):\n",
    "        out = self.forward(graph, feat)\n",
    "        self._graph_embs.append(out)\n",
    "        return th.stack(self._graph_embs, dim=0)\n",
    "\n",
    "\n",
    "class MLAPSum(MLAPBase):\n",
    "    def _aggregate(self):\n",
    "        return th.stack(self._graph_embs, dim=0).sum(dim=0)\n",
    "\n",
    "\n",
    "class MLAPWeighted(MLAPBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight = nn.Parameter(th.ones(self._depth, 1, 1))\n",
    "\n",
    "    def _aggregate(self):\n",
    "        a = F.softmax(self.weight, dim=0)\n",
    "        h = th.stack(self._graph_embs, dim=0)\n",
    "        return (a * h).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce5c5c5-ed3b-4590-95c4-9767d4c9ea0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:05.015642Z",
     "iopub.status.busy": "2023-04-06T05:51:05.015098Z",
     "iopub.status.idle": "2023-04-06T05:51:05.025705Z",
     "shell.execute_reply": "2023-04-06T05:51:05.024791Z",
     "shell.execute_reply.started": "2023-04-06T05:51:05.015642Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "def log(path: Optional[Path], msg: str):\n",
    "    print(msg)\n",
    "\n",
    "    if path:\n",
    "        with open(path, \"a\") as f:\n",
    "            f.write(msg)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    if cuda.is_available():\n",
    "        cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def get_repo_root(default_root=\"./mlap_root\") -> Path:\n",
    "    return Path(default_root)\n",
    "\n",
    "\n",
    "def encode_seq_to_arr(seq: List[str], vocab2idx: Dict[str, int], max_seq_len: int) -> th.Tensor:\n",
    "    seq = seq[:max_seq_len] + [\"__EOS__\"] * max(0, max_seq_len - len(seq))\n",
    "    return th.tensor([vocab2idx[w] if w in vocab2idx else vocab2idx[\"__UNK__\"] for w in seq], dtype=th.int64)\n",
    "\n",
    "\n",
    "def decode_arr_to_seq(arr: Union[List[int], th.Tensor], idx2vocab: List[str], vocab2idx: Dict[str, int]) -> List[str]:\n",
    "    if isinstance(arr, th.Tensor):\n",
    "        arr = arr.tolist()\n",
    "    if vocab2idx[\"__EOS__\"] in arr:\n",
    "        arr = arr[:arr.index(vocab2idx[\"__EOS__\"])]\n",
    "    return [idx2vocab[i] for i in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f614696b-aa3e-4e17-8b7b-4df1ad4fe454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:05.938323Z",
     "iopub.status.busy": "2023-04-06T05:51:05.937942Z",
     "iopub.status.idle": "2023-04-06T05:51:05.952969Z",
     "shell.execute_reply": "2023-04-06T05:51:05.952023Z",
     "shell.execute_reply.started": "2023-04-06T05:51:05.938297Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import dgl\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, dim_feat: int, max_seq_len: int, vocab2idx: Dict[str, int]):\n",
    "        super().__init__()\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._vocab2idx = vocab2idx\n",
    "\n",
    "        self.decoders = nn.ModuleList([nn.Linear(dim_feat, len(vocab2idx)) for _ in range(max_seq_len)])\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph, feats: th.Tensor, labels: List[List[str]]) -> List[th.Tensor]:\n",
    "        return [d(feats[-1]) for d in self.decoders]\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, dim_feat: int, max_seq_len: int, vocab2idx: Dict[str, int]):\n",
    "        super().__init__()\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._vocab2idx = vocab2idx\n",
    "\n",
    "        self.lstm = nn.LSTMCell(dim_feat, dim_feat)\n",
    "        self.w_hc = nn.Linear(dim_feat * 2, dim_feat)\n",
    "        self.layernorm = nn.LayerNorm(dim_feat)\n",
    "        self.vocab_encoder = nn.Embedding(len(vocab2idx), dim_feat)\n",
    "        self.vocab_bias = nn.Parameter(th.zeros(len(vocab2idx)))\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph, feats: th.Tensor, labels: List[List[str]]) -> List[th.Tensor]:\n",
    "        if self.training:\n",
    "            # teacher forcing\n",
    "            batched_label = th.vstack([encode_seq_to_arr(label, self._vocab2idx, self._max_seq_len - 1) for label in labels])\n",
    "            batched_label = th.hstack((th.zeros((graph.batch_size, 1), dtype=th.int64), batched_label))\n",
    "            true_emb = self.vocab_encoder(batched_label.to(device=graph.device))\n",
    "\n",
    "        h_t, c_t = feats[-1].clone(), feats[-1].clone()\n",
    "        feats = feats.transpose(0, 1)  # (batch_size, L + 1, dim_feat)\n",
    "        out = []\n",
    "        pred_emb = self.vocab_encoder(th.zeros((graph.batch_size), dtype=th.int64, device=graph.device))\n",
    "\n",
    "        vocab_mat = self.vocab_encoder(th.arange(len(self._vocab2idx), dtype=th.int64, device=graph.device))\n",
    "\n",
    "        for i in range(self._max_seq_len):\n",
    "            if self.training:\n",
    "                _in = true_emb[:, i]\n",
    "            else:\n",
    "                _in = pred_emb\n",
    "            h_t, c_t = self.lstm(_in, (h_t, c_t))\n",
    "\n",
    "            a = F.softmax(th.bmm(feats, h_t.unsqueeze(-1)).squeeze(-1), dim=1)  # (batch_size, L + 1)\n",
    "            context = th.bmm(a.unsqueeze(1), feats).squeeze(1)\n",
    "            pred_emb = th.tanh(self.layernorm(self.w_hc(th.hstack((h_t, context)))))  # (batch_size, dim_feat)\n",
    "\n",
    "            out.append(th.matmul(pred_emb, vocab_mat.T) + self.vocab_bias.unsqueeze(0))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8e24ee-c62c-4f48-82c7-d9caeb201983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:06.736882Z",
     "iopub.status.busy": "2023-04-06T05:51:06.736111Z",
     "iopub.status.idle": "2023-04-06T05:51:06.756569Z",
     "shell.execute_reply": "2023-04-06T05:51:06.755647Z",
     "shell.execute_reply.started": "2023-04-06T05:51:06.736854Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "GNN_CLASS = {\n",
    "    \"simple\": GNNSimple,\n",
    "\n",
    "    \"mlap-sum\": MLAPSum,\n",
    "    \"mlap-weighted\": MLAPWeighted,\n",
    "}\n",
    "\n",
    "\n",
    "class TeacherForcing(nn.Module):\n",
    "    def forward(self, graph: dgl.DGLGraph, labels: Any) -> th.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, dataset_name: str, device: th.device, save: bool) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.device = device\n",
    "        self.save = save\n",
    "\n",
    "        if not self.log_dir.exists():\n",
    "            self.log_dir.mkdir()\n",
    "        if not self.model_dir.exists():\n",
    "            self.model_dir.mkdir()\n",
    "\n",
    "        self.train_loader = None\n",
    "        self.valid_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    @property\n",
    "    def log_dir(self) -> Path:\n",
    "        return get_repo_root() / \"log\" / self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def model_dir(self) -> Path:\n",
    "        return get_repo_root() / \"model\" / self.dataset_name\n",
    "\n",
    "    def load_dataset(self, batch_size: int) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_model(self, arch: str, norm: str, res: bool, dim_feat: int, depth: int) -> None:\n",
    "        self.model = self._build_model(arch, norm, res, dim_feat, depth)\n",
    "\n",
    "    def _build_model(self, arch: str, norm: str, res: bool, dim_feat: int, depth: int) -> nn.Module:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_gnn(self, arch: str, *args, **kwargs) -> nn.Module:\n",
    "        i = arch.index(\"-\")\n",
    "        self._current_gnn = GNN_CLASS[arch[(i + 1):]](arch[:i], *args, **kwargs)\n",
    "        return self._current_gnn\n",
    "\n",
    "    def set_seed(self, seed: int):\n",
    "        self._seed = seed\n",
    "        set_seed(seed)\n",
    "\n",
    "    def evaluate(self, loader, *, silent=False) -> float:\n",
    "        self.model.eval()\n",
    "        batch_sizes = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for batch in tqdm(loader, disable=silent):\n",
    "            g, labels = batch\n",
    "            with th.no_grad():\n",
    "                if isinstance(self.model, TeacherForcing):\n",
    "                    out = [t.detach().cpu() for t in self.model(g.to(self.device), None)]\n",
    "                else:\n",
    "                    out = self.model(g.to(self.device)).detach().cpu()\n",
    "            batch_sizes.append(g.batch_size)\n",
    "            y_true.append(labels)\n",
    "            y_pred.append(out)\n",
    "\n",
    "        return self._evaluate_score(y_true, y_pred, batch_sizes)\n",
    "\n",
    "    def _evaluate_score(self, y_true: List[Any], y_pred: List[th.Tensor], batch_sizes: List[int]) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            epochs: int,\n",
    "            optimizer: optim.Optimizer,\n",
    "            scheduler: Optional[Union[optim.lr_scheduler.StepLR, optim.lr_scheduler.ReduceLROnPlateau]],\n",
    "            *,\n",
    "            save: Optional[Tuple[Path, Path, str]]=None,\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "        train_curve = []\n",
    "        valid_curve = []\n",
    "\n",
    "        log_path = save[0] if save else None\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(\"Training...\")\n",
    "            self.model.train()\n",
    "            with tqdm(self.train_loader) as pbar:\n",
    "                for i, batch in enumerate(pbar):\n",
    "                    g, labels = batch\n",
    "                    if isinstance(self.model, TeacherForcing):\n",
    "                        out = self.model(g.to(self.device), labels)\n",
    "                    else:\n",
    "                        out = self.model(g.to(self.device))\n",
    "                    loss = self._loss(out, labels)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    pbar.set_description(f\"Epoch {epoch: >3}, batch {i: > 5} loss: {loss.data:.3f}\")\n",
    "\n",
    "            print(\"Evaluating...\")\n",
    "            # train_perf = self.evaluate(self.train_loader)\n",
    "            train_perf = 0.0\n",
    "            valid_perf = self.evaluate(self.valid_loader)\n",
    "            log(log_path, f\"Epoch {epoch}, train {train_perf}, valid {valid_perf}\")\n",
    "            train_curve.append(train_perf)\n",
    "            valid_curve.append(valid_perf)\n",
    "\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(valid_perf)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if save:\n",
    "                _, save_dir, save_name = save\n",
    "                th.save(self.model.state_dict(), save_dir / f\"{save_name}_e{epoch}\")\n",
    "\n",
    "        log(log_path, f\"Best validation score: {np.max(valid_curve)}\")\n",
    "        return train_curve, valid_curve\n",
    "\n",
    "    def _loss(self, out: th.Tensor, labels: th.Tensor) -> th.Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60be815-c02c-4603-8195-a54660c19177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:07.742265Z",
     "iopub.status.busy": "2023-04-06T05:51:07.741417Z",
     "iopub.status.idle": "2023-04-06T05:51:09.189832Z",
     "shell.execute_reply": "2023-04-06T05:51:09.188918Z",
     "shell.execute_reply.started": "2023-04-06T05:51:07.742236Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class OGBCodeTask(Task):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name: str,\n",
    "            device: th.device,\n",
    "            save: bool,\n",
    "            use_subtoken: bool,\n",
    "            decoder_type: str\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(dataset_name, device, save)\n",
    "        self._use_subtoken = use_subtoken\n",
    "        self._decoder_type = decoder_type\n",
    "\n",
    "    def load_dataset(self, batch_size: int) -> None:\n",
    "        self.dataset = DglGraphPropPredDataset(name=self.dataset_name)\n",
    "        if \"feat\" in self.dataset[0][0].ndata:\n",
    "            self.dataset.dim_node = self.dataset[0][0].ndata[\"feat\"].shape[1]\n",
    "        else:\n",
    "            self.dataset.dim_node = 0\n",
    "        if \"feat\" in self.dataset[0][0].edata:\n",
    "            self.dataset.dim_edge = self.dataset[0][0].edata[\"feat\"].shape[1]\n",
    "        else:\n",
    "            self.dataset.dim_edge = 0\n",
    "\n",
    "        self._max_depth = 20\n",
    "        self._max_seq_len = 5\n",
    "        self._num_vocab = 5000\n",
    "        self._num_nodetypes = len(pd.read_csv(Path(self.dataset.root) / \"mapping\" / \"typeidx2type.csv.gz\")[\"type\"])\n",
    "\n",
    "        split_idx = self.dataset.get_idx_split()\n",
    "        self._vocab2idx, self._id2vocab = _get_vocab_mapping([l for _, l in self.dataset[split_idx[\"train\"]]], self._num_vocab)\n",
    "\n",
    "        if self._use_subtoken:\n",
    "            pickle_path = Path(self.dataset.root) / \"saved\" / \"loaders-subtoken.pkl\"\n",
    "        else:\n",
    "            pickle_path = Path(self.dataset.root) / \"saved\" / \"loaders.pkl\"\n",
    "\n",
    "        if pickle_path.exists():\n",
    "            with open(pickle_path, \"rb\") as f:\n",
    "                train_samples = pickle.load(f)\n",
    "                valid_samples = pickle.load(f)\n",
    "                test_samples = pickle.load(f)\n",
    "                self._num_nodeattrs = pickle.load(f)\n",
    "\n",
    "        else:\n",
    "            if self._use_subtoken:\n",
    "                def _subtokenize_attr(attr: str) -> List[str]:\n",
    "                    def camel_case_split(s: str) -> List[str]:\n",
    "                        matches = re.finditer(\".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\", s)\n",
    "                        return [m.group(0).lower() for m in matches]\n",
    "\n",
    "                    if attr in [\"__NONE__\", \"__UNK__\"]:\n",
    "                        res = [attr]\n",
    "                    else:\n",
    "                        res = []\n",
    "                        for part in str(attr).split('_'):\n",
    "                            res.extend(camel_case_split(part))\n",
    "\n",
    "                    if len(res) >= self._max_seq_len:\n",
    "                        return res[:self._max_seq_len]\n",
    "                    else:\n",
    "                        return res + [\"__NONE__\"] * (self._max_seq_len - len(res))\n",
    "\n",
    "                attr_subtokens = {}\n",
    "                for i, _, a in pd.read_csv(Path(self.dataset.root) / \"mapping\" / \"attridx2attr.csv.gz\").itertuples():\n",
    "                    attr_subtokens[i] = _subtokenize_attr(a)\n",
    "\n",
    "                def get_split(idx, is_training):\n",
    "                    samples = []\n",
    "                    for i in tqdm(idx):\n",
    "                        samples.append((_augment_edge(_subtokenize(self.dataset[i][0], attr_subtokens, used_subtokens, is_training)), self.dataset[i][1]))\n",
    "                    return samples\n",
    "\n",
    "                used_subtokens = {\"__NONE__\": 0, \"__UNK__\": 1, \"__NUM__\": 2}\n",
    "                train_samples = get_split(split_idx[\"train\"], True)\n",
    "                self._num_nodeattrs = len(used_subtokens)\n",
    "                valid_samples = get_split(split_idx[\"valid\"], False)\n",
    "                test_samples = get_split(split_idx[\"test\"], False)\n",
    "\n",
    "            else:\n",
    "                self._num_nodeattrs = len(pd.read_csv(Path(self.dataset.root) / \"mapping\" / \"attridx2attr.csv.gz\")[\"attr\"])\n",
    "\n",
    "                def get_split(idx):\n",
    "                    samples = []\n",
    "                    for i in tqdm(idx):\n",
    "                        samples.append((_augment_edge(self.dataset[i][0]), self.dataset[i][1]))\n",
    "                    return samples\n",
    "\n",
    "                train_samples = get_split(split_idx[\"train\"])\n",
    "                valid_samples = get_split(split_idx[\"valid\"])\n",
    "                test_samples = get_split(split_idx[\"test\"])\n",
    "\n",
    "            pickle_path.parent.mkdir(exist_ok=True)\n",
    "            with open(pickle_path, \"wb\") as f:\n",
    "                pickle.dump(train_samples, f)\n",
    "                pickle.dump(valid_samples, f)\n",
    "                pickle.dump(test_samples, f)\n",
    "                pickle.dump(self._num_nodeattrs, f)\n",
    "\n",
    "        def collate(samples) -> Tuple[dgl.DGLGraph, List[List[str]]]:\n",
    "            graphs, labels = map(list, zip(*samples))\n",
    "            batched_graph = dgl.batch(graphs)\n",
    "            return batched_graph, labels\n",
    "\n",
    "        self.train_loader = DataLoader(train_samples, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "        self.valid_loader = DataLoader(valid_samples, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "        self.test_loader = DataLoader(test_samples, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    def _build_model(self, arch: str, norm: str, residual: bool, dim_feat: int, depth: int) -> nn.Module:\n",
    "        edge_encoder = lambda: nn.Embedding(4, dim_feat)\n",
    "        gnn = self._build_gnn(arch, norm, residual, dim_feat, depth, edge_encoder)\n",
    "        return CodeEncDec(\n",
    "            gnn,\n",
    "            dim_feat,\n",
    "            self._max_depth,\n",
    "            self._max_seq_len,\n",
    "            self._num_nodetypes,\n",
    "            self._num_nodeattrs,\n",
    "            self._vocab2idx,\n",
    "            self._decoder_type,\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _evaluate_score(self, y_true: List[Any], y_pred: List[th.Tensor], batch_sizes: List[int]) -> float:\n",
    "        y_true = sum(y_true, [])\n",
    "        y_pred = th.vstack([th.hstack([t.argmax(dim=1).view(b, -1) for t in l]) for l, b in zip(y_pred, batch_sizes)])\n",
    "        y_pred = [decode_arr_to_seq(a, self._id2vocab, self._vocab2idx) for a in y_pred]\n",
    "        metric: str = self.dataset.eval_metric\n",
    "        return Evaluator(self.dataset_name).eval({\"seq_ref\": y_true, \"seq_pred\": y_pred})[metric]\n",
    "\n",
    "    def _loss(self, out: th.Tensor, labels: List[List[str]]) -> th.Tensor:\n",
    "        batched_label = th.vstack([encode_seq_to_arr(label, self._vocab2idx, self._max_seq_len) for label in labels])\n",
    "        return sum([nn.CrossEntropyLoss()(out[i], batched_label[:, i].to(device=self.device)) for i in range(self._max_seq_len)])\n",
    "\n",
    "    def get_emb(self, save_emb_name: str):\n",
    "        for name, loader in {\"train\": self.train_loader, \"valid\": self.valid_loader, \"test\": self.test_loader}.items():\n",
    "            embs = []\n",
    "            labels = []\n",
    "            for batch in tqdm(loader):\n",
    "                g, l = batch\n",
    "                with th.no_grad():\n",
    "                    embs.append(self.model.get_emb(g.to(self.device)).detach().cpu())\n",
    "                    labels.extend(l)\n",
    "            embs = th.cat(embs, dim=1)\n",
    "            labels = th.vstack([encode_seq_to_arr(label, self._vocab2idx, self._max_seq_len) for label in labels])\n",
    "\n",
    "            save_path = self.emb_dir / f\"{save_emb_name}_{name}\"\n",
    "            np.savez(save_path, embs=embs.numpy(), labels=labels.numpy())\n",
    "\n",
    "\n",
    "class CodeEncDec(TeacherForcing, nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            gnn: nn.Module,\n",
    "            dim_feat: int,\n",
    "            max_depth: int,\n",
    "            max_seq_len: int,\n",
    "            num_nodetypes: int,\n",
    "            num_nodeattrs: int,\n",
    "            vocab2idx: Dict[str, int],\n",
    "            decoder_type: str,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self._dim_feat = dim_feat\n",
    "        self._max_depth = max_depth\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._vocab2idx = vocab2idx\n",
    "        self._decoder_type = decoder_type\n",
    "\n",
    "        self.type_encoder = nn.Embedding(num_nodetypes, dim_feat)\n",
    "        self.attr_encoder = nn.Embedding(num_nodeattrs, dim_feat)\n",
    "        self.depth_encoder = nn.Embedding(max_depth + 1, dim_feat)\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(3 * dim_feat, 2 * dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * dim_feat, dim_feat),\n",
    "        )\n",
    "        self.gnn = gnn\n",
    "\n",
    "        if decoder_type == \"linear\":\n",
    "            self.linear_decoder = LinearDecoder(dim_feat, max_seq_len, vocab2idx)\n",
    "        elif decoder_type == \"lstm\":\n",
    "            self.lstm_decoder = LSTMDecoder(dim_feat, max_seq_len, vocab2idx)\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph, labels: Any) -> List[th.Tensor]:\n",
    "        feats = self.get_emb(graph)  # (L+1, batch_size, dim_feat)\n",
    "\n",
    "        if self._decoder_type == \"linear\":\n",
    "            return self.linear_decoder(graph, feats, labels)\n",
    "        elif self._decoder_type == \"lstm\":\n",
    "            return self.lstm_decoder(graph, feats, labels)\n",
    "\n",
    "    def get_emb(self, graph: dgl.DGLGraph) -> th.Tensor:\n",
    "        type_emb = self.type_encoder(graph.ndata[\"feat\"][:, 0])\n",
    "        attr_emb = (self.attr_encoder(graph.ndata[\"feat\"][:, 1:]) * (graph.ndata[\"feat\"][:, 1:] > 0).unsqueeze(-1)).sum(dim=1)\n",
    "        depth = graph.ndata[\"depth\"].view(-1)\n",
    "        depth[depth > self._max_depth] = self._max_depth\n",
    "        depth_emb = self.depth_encoder(depth)\n",
    "        feat = self.node_mlp(th.hstack((type_emb, attr_emb, depth_emb)))\n",
    "        return self.gnn.get_emb(graph, feat)\n",
    "\n",
    "\n",
    "def _get_vocab_mapping(words_list: List[List[str]], num_vocab: int) -> Tuple[Dict[str, int], List[str]]:\n",
    "    vocab_count = defaultdict(int)\n",
    "    for words in tqdm(words_list):\n",
    "        for word in words:\n",
    "            vocab_count[word] += 1\n",
    "    idx2vocab = [\"__SOS__\", \"__UNK__\", \"__EOS__\"]\n",
    "    idx2vocab += list(list(zip(*sorted([(c, w) for w, c in vocab_count.items()], reverse=True)[:num_vocab]))[1])\n",
    "    vocab2idx = {w: i for i, w in enumerate(idx2vocab)}\n",
    "\n",
    "    # test\n",
    "    for idx, vocab in enumerate(idx2vocab):\n",
    "        assert(idx == vocab2idx[vocab])\n",
    "    assert(vocab2idx[\"__SOS__\"] == 0)\n",
    "    assert(vocab2idx[\"__UNK__\"] == 1)\n",
    "    assert(vocab2idx[\"__EOS__\"] == 2)\n",
    "\n",
    "    return vocab2idx, idx2vocab\n",
    "\n",
    "\n",
    "def _augment_edge(graph: dgl.DGLGraph) -> dgl.DGLGraph:\n",
    "    num_ast_edges = graph.num_edges()\n",
    "    src_ast = th.hstack((graph.edges()[0], graph.edges()[1]))\n",
    "    dst_ast = th.hstack((graph.edges()[1], graph.edges()[0]))\n",
    "    attr_ast = th.vstack((th.zeros((num_ast_edges, 1)), th.ones((num_ast_edges, 1))))\n",
    "\n",
    "    terminals = th.where(graph.ndata[\"is_attributed\"] == 1)[0]\n",
    "    num_nt_edges = terminals.shape[0] - 1\n",
    "    src_nt = th.hstack((terminals[:-1], terminals[1:]))\n",
    "    dst_nt = th.hstack((terminals[1:], terminals[:-1]))\n",
    "    attr_nt = th.vstack((th.ones((num_nt_edges, 1)) * 2, th.ones((num_nt_edges, 1)) * 3))\n",
    "\n",
    "    graph.remove_edges(np.arange(num_ast_edges))\n",
    "    graph.add_edges(th.hstack((src_ast, src_nt)), th.hstack((dst_ast, dst_nt)), {\"feat\": th.vstack((attr_ast, attr_nt)).to(th.int64)})\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def _subtokenize(\n",
    "        graph: dgl.DGLGraph,\n",
    "        attr_subtokens: Dict[int, List[str]],\n",
    "        used_subtokens: Dict[str, int],\n",
    "        is_training: bool,\n",
    ") -> dgl.DGLGraph:\n",
    "\n",
    "    feat = th.hstack((graph.ndata[\"feat\"][:, 0].view(-1, 1), th.zeros((graph.ndata[\"feat\"].shape[0], 5)).to(graph.ndata[\"feat\"])))\n",
    "\n",
    "    for i in range(graph.ndata[\"feat\"].shape[0]):\n",
    "        if int(graph.ndata[\"feat\"][i, 0]) == 67:  # Num\n",
    "            feat[i, 1] = used_subtokens[\"__NUM__\"]\n",
    "        elif int(graph.ndata[\"feat\"][i, 1]) == 10028:  # __NONE__\n",
    "            pass\n",
    "        else:\n",
    "            for j, s in enumerate(attr_subtokens[int(graph.ndata[\"feat\"][i, 1])]):\n",
    "                if is_training:\n",
    "                    if s not in used_subtokens:\n",
    "                        used_subtokens[s] = len(used_subtokens)\n",
    "                    feat[i, j + 1] = used_subtokens[s]\n",
    "                else:\n",
    "                    if s in used_subtokens:\n",
    "                        feat[i, j + 1] = used_subtokens[s]\n",
    "                    else:\n",
    "                        feat[i, j + 1] = used_subtokens[\"__UNK__\"]\n",
    "\n",
    "    graph.ndata[\"feat\"] = feat\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f9d8d8-135f-4bf0-8637-b4fe5fb88f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T05:51:09.193203Z",
     "iopub.status.busy": "2023-04-06T05:51:09.192098Z",
     "iopub.status.idle": "2023-04-06T05:51:09.482196Z",
     "shell.execute_reply": "2023-04-06T05:51:09.480971Z",
     "shell.execute_reply.started": "2023-04-06T05:51:09.193158Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import cuda, optim\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "DEVICE = th.device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n",
    "TIMESTR = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "GITHASH = os.environ.get(\"GITHASH\") or subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).strip().decode()\n",
    "MODEL_REGEX = re.compile(r\".*models/(?P<dataset>[^\\\\]+)/[^\\\\]+/(?P<arch>gin-[^_]+)_(?P<norm>[^_]+)_d(?P<dim>\\d+)_l(?P<depth>\\d+)_s(?P<seed>\\d+)_e\\d+\")\n",
    "\n",
    "if cudnn.enabled:\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_pathname(\n",
    "        *,\n",
    "        prefix: bool=False,\n",
    "        arch: Optional[str]=None,\n",
    "        norm: Optional[str]=None,\n",
    "        residual: bool=False,\n",
    "        dim_feat: Optional[int]=None,\n",
    "        depth: Optional[int]=None,\n",
    "        seed: Optional[int]=None,\n",
    "        batch_size: Optional[int]=None,\n",
    "        learning_rate: Optional[Tuple[float, int, float]]=None,\n",
    "        suffix: Optional[str]=None\n",
    ") -> str:\n",
    "\n",
    "    name = \"\"\n",
    "\n",
    "    if prefix:\n",
    "        name = f\"{TIMESTR}_{GITHASH}\"\n",
    "    if arch:\n",
    "        name += f\"_{arch}\"\n",
    "    if norm:\n",
    "        name += f\"_{norm}\"\n",
    "    if residual:\n",
    "        name += f\"_res\"\n",
    "    if dim_feat:\n",
    "        name += f\"_d{dim_feat}\"\n",
    "    if depth:\n",
    "        name += f\"_l{depth}\"\n",
    "    if seed is not None:\n",
    "        name += f\"_s{seed}\"\n",
    "    if batch_size is not None:\n",
    "        name += f\"_b{batch_size}\"\n",
    "    if learning_rate is not None:\n",
    "        name += f\"_r{learning_rate[0]}_{learning_rate[1]}_{learning_rate[2]}\"\n",
    "    if suffix:\n",
    "        name += suffix\n",
    "\n",
    "    if name.startswith(\"_\"):\n",
    "        name = name[1:]\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def run_training(\n",
    "        args_str: str,\n",
    "        task: Task,\n",
    "        arch: str,\n",
    "        norm: str,\n",
    "        residual: bool,\n",
    "        dim_feat: int,\n",
    "        depth: int,\n",
    "        seed: int,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        initial_lr: float,\n",
    "        lr_interval: int,\n",
    "        lr_scale: float,\n",
    "        *,\n",
    "        save: bool=True,\n",
    "):\n",
    "    print(f\"Starting the training for arch: {arch}\")\n",
    "    task.build_model(arch, norm, residual, dim_feat, depth)\n",
    "    task.set_seed(seed)\n",
    "\n",
    "    log_path = task.log_dir / build_pathname(prefix=True, arch=arch, norm=norm, residual=residual, dim_feat=dim_feat, depth=depth, seed=seed, batch_size=batch_size, learning_rate=(initial_lr, lr_interval, lr_scale), suffix=\"_train.log\")\n",
    "    save_dir = task.model_dir / build_pathname(prefix=True, arch=arch, norm=norm, residual=residual, dim_feat=dim_feat, depth=depth, seed=seed, batch_size=batch_size, learning_rate=(initial_lr, lr_interval, lr_scale))\n",
    "    save_dir.mkdir()\n",
    "    save_name = build_pathname(arch=arch, norm=norm, residual=residual, dim_feat=dim_feat, depth=depth, seed=seed, batch_size=batch_size, learning_rate=(initial_lr, lr_interval, lr_scale))\n",
    "\n",
    "    log(log_path, args_str)\n",
    "\n",
    "    optimizer = optim.Adam(task.model.parameters(), lr=initial_lr)\n",
    "    if lr_interval > 0:\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_interval, gamma=lr_scale, verbose=True)\n",
    "    elif lr_interval < 0:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\", factor=lr_scale, patience=-lr_interval, verbose=True)\n",
    "    else:\n",
    "        raise ValueError(\"lr_interval cannot be 0\")\n",
    "\n",
    "    task.train(epochs, optimizer, scheduler, save=(log_path, save_dir, save_name) if save else None)\n",
    "\n",
    "\n",
    "def run_test(task: Task, model_paths: List[str]):\n",
    "    for model_path in model_paths:\n",
    "        if not Path(model_path).exists():\n",
    "            continue\n",
    "\n",
    "        match = MODEL_REGEX.search(model_path)\n",
    "        assert match is not None\n",
    "        arch = match.group(\"arch\")\n",
    "        norm = match.group(\"norm\")\n",
    "        dim_feat = int(match.group(\"dim\"))\n",
    "        depth = int(match.group(\"depth\"))\n",
    "\n",
    "        task.build_model(arch, norm, False, dim_feat, depth)\n",
    "        task.model.load_state_dict(th.load(model_path, map_location=DEVICE))\n",
    "        valid_perf = task.evaluate(task.valid_loader, silent=True)\n",
    "        test_perf = task.evaluate(task.test_loader, silent=True)\n",
    "\n",
    "        print(f\"{model_path}: val={valid_perf} test={test_perf}\")\n",
    "\n",
    "\n",
    "def main(\n",
    "        args_str: str=\"args\",\n",
    "        dataset_name: str=\"ogbg-code2\",\n",
    "        batch_size: int=50,\n",
    "        arch: str=\"gin-simple\",\n",
    "        norm: str=\"none\",\n",
    "        residual: bool=False,\n",
    "        dim_feat: int=20, # default was 200\n",
    "        depth: int=5,\n",
    "        seed: int=50,\n",
    "        epochs: int=50,\n",
    "        initial_lr: float=1e-3,\n",
    "        lr_interval: int=15,\n",
    "        lr_scale: float=0.2,\n",
    "        train: bool=True,\n",
    "        test: Optional[List[str]]=None,\n",
    "        save: bool=True,\n",
    "        code2_use_subtoken: bool=False,\n",
    "        code2_decoder_type: Optional[str]=None,\n",
    "):\n",
    "    if dataset_name == \"ogbg-code2\":\n",
    "        task = OGBCodeTask(dataset_name, DEVICE, save, code2_use_subtoken, code2_decoder_type)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    task.load_dataset(batch_size)\n",
    "\n",
    "    if train:\n",
    "        run_training(args_str, task, arch, norm, residual, dim_feat, depth, seed, epochs, batch_size, initial_lr, lr_interval, lr_scale, save=save)\n",
    "    elif test:\n",
    "        run_test(task, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4c1010-f8c6-47c7-8773-87f74035885f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T06:08:58.138920Z",
     "iopub.status.busy": "2023-04-06T06:08:58.138551Z",
     "iopub.status.idle": "2023-04-06T06:10:30.605142Z",
     "shell.execute_reply": "2023-04-06T06:10:30.604099Z",
     "shell.execute_reply.started": "2023-04-06T06:08:58.138892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9404, Train: 0.6643, Val: 0.4200, Test: 0.4650\n",
      "Epoch: 050, Loss: 0.0189, Train: 1.0000, Val: 0.7640, Test: 0.7920\n",
      "Epoch: 100, Loss: 0.0234, Train: 1.0000, Val: 0.7620, Test: 0.7920\n",
      "Epoch: 150, Loss: 0.0140, Train: 1.0000, Val: 0.7560, Test: 0.7920\n",
      "Epoch: 200, Loss: 0.0157, Train: 1.0000, Val: 0.7700, Test: 0.7920\n",
      "Epoch: 250, Loss: 0.0114, Train: 1.0000, Val: 0.7720, Test: 0.7920\n",
      "Epoch: 300, Loss: 0.0105, Train: 1.0000, Val: 0.7520, Test: 0.7920\n",
      "Epoch: 350, Loss: 0.0100, Train: 1.0000, Val: 0.7740, Test: 0.7920\n",
      "Epoch: 400, Loss: 0.0106, Train: 1.0000, Val: 0.7680, Test: 0.7920\n",
      "Epoch: 450, Loss: 0.0119, Train: 1.0000, Val: 0.7620, Test: 0.7920\n",
      "Epoch: 500, Loss: 0.0063, Train: 1.0000, Val: 0.7700, Test: 0.7920\n",
      "Epoch: 550, Loss: 0.0059, Train: 1.0000, Val: 0.7580, Test: 0.8020\n",
      "Epoch: 600, Loss: 0.0059, Train: 1.0000, Val: 0.7760, Test: 0.8020\n",
      "Epoch: 650, Loss: 0.0169, Train: 1.0000, Val: 0.7700, Test: 0.8020\n",
      "Epoch: 700, Loss: 0.0071, Train: 1.0000, Val: 0.7660, Test: 0.8020\n",
      "Epoch: 750, Loss: 0.0064, Train: 1.0000, Val: 0.7560, Test: 0.8020\n",
      "Epoch: 800, Loss: 0.0061, Train: 1.0000, Val: 0.7660, Test: 0.8020\n",
      "Epoch: 850, Loss: 0.0053, Train: 1.0000, Val: 0.7680, Test: 0.8020\n",
      "Epoch: 900, Loss: 0.0055, Train: 1.0000, Val: 0.7660, Test: 0.8020\n",
      "Epoch: 950, Loss: 0.0049, Train: 1.0000, Val: 0.7760, Test: 0.8020\n",
      "Epoch: 1000, Loss: 0.0038, Train: 1.0000, Val: 0.7640, Test: 0.8020\n",
      "Epoch: 1050, Loss: 0.0033, Train: 1.0000, Val: 0.7600, Test: 0.8020\n",
      "Epoch: 1100, Loss: 0.0037, Train: 1.0000, Val: 0.7580, Test: 0.8020\n",
      "Epoch: 1150, Loss: 0.0038, Train: 1.0000, Val: 0.7580, Test: 0.8020\n",
      "Epoch: 1200, Loss: 0.0039, Train: 1.0000, Val: 0.7780, Test: 0.8020\n",
      "Epoch: 1250, Loss: 0.0038, Train: 1.0000, Val: 0.7400, Test: 0.8020\n",
      "Epoch: 1300, Loss: 0.0093, Train: 1.0000, Val: 0.7600, Test: 0.8020\n",
      "Epoch: 1350, Loss: 0.0125, Train: 1.0000, Val: 0.7480, Test: 0.8020\n",
      "Epoch: 1400, Loss: 0.0039, Train: 1.0000, Val: 0.7480, Test: 0.8020\n",
      "Epoch: 1450, Loss: 0.0034, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 1500, Loss: 0.0021, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 1550, Loss: 0.0034, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 1600, Loss: 0.0023, Train: 1.0000, Val: 0.7820, Test: 0.7990\n",
      "Epoch: 1650, Loss: 0.0028, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 1700, Loss: 0.0025, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 1750, Loss: 0.0022, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 1800, Loss: 0.0032, Train: 1.0000, Val: 0.7800, Test: 0.7990\n",
      "Epoch: 1850, Loss: 0.0028, Train: 1.0000, Val: 0.7800, Test: 0.7990\n",
      "Epoch: 1900, Loss: 0.0020, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch: 1950, Loss: 0.0019, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 2000, Loss: 0.0023, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 2050, Loss: 0.0020, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 2100, Loss: 0.0029, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 2150, Loss: 0.0020, Train: 1.0000, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 2200, Loss: 0.0028, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 2250, Loss: 0.0018, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 2300, Loss: 0.0113, Train: 1.0000, Val: 0.7660, Test: 0.7990\n",
      "Epoch: 2350, Loss: 0.0048, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 2400, Loss: 0.0074, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 2450, Loss: 0.0023, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 2500, Loss: 0.0010, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 2550, Loss: 0.0013, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 2600, Loss: 0.0017, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 2650, Loss: 0.0027, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 2700, Loss: 0.0013, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 2750, Loss: 0.0024, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 2800, Loss: 0.0022, Train: 1.0000, Val: 0.7840, Test: 0.7990\n",
      "Epoch: 2850, Loss: 0.0033, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 2900, Loss: 0.0012, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 2950, Loss: 0.0008, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 3000, Loss: 0.0010, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 3050, Loss: 0.0019, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 3100, Loss: 0.0016, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 3150, Loss: 0.0023, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 3200, Loss: 0.0015, Train: 1.0000, Val: 0.7660, Test: 0.7990\n",
      "Epoch: 3250, Loss: 0.0038, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 3300, Loss: 0.0082, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 3350, Loss: 0.0022, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 3400, Loss: 0.0014, Train: 1.0000, Val: 0.7880, Test: 0.7990\n",
      "Epoch: 3450, Loss: 0.0013, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch: 3500, Loss: 0.0023, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 3550, Loss: 0.0008, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 3600, Loss: 0.0012, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 3650, Loss: 0.0017, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 3700, Loss: 0.0020, Train: 1.0000, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 3750, Loss: 0.0019, Train: 1.0000, Val: 0.7660, Test: 0.7990\n",
      "Epoch: 3800, Loss: 0.0012, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 3850, Loss: 0.0013, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 3900, Loss: 0.0020, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 3950, Loss: 0.0013, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 4000, Loss: 0.0012, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 4050, Loss: 0.0046, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 4100, Loss: 0.0045, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 4150, Loss: 0.0015, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 4200, Loss: 0.0041, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 4250, Loss: 0.0019, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 4300, Loss: 0.0173, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 4350, Loss: 0.0004, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 4400, Loss: 0.0003, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 4450, Loss: 0.0016, Train: 1.0000, Val: 0.7000, Test: 0.7990\n",
      "Epoch: 4500, Loss: 0.0023, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 4550, Loss: 0.0017, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 4600, Loss: 0.0007, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 4650, Loss: 0.0012, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 4700, Loss: 0.0131, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 4750, Loss: 0.0036, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 4800, Loss: 0.0009, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 4850, Loss: 0.0017, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 4900, Loss: 0.0005, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 4950, Loss: 0.0022, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 5000, Loss: 0.0031, Train: 1.0000, Val: 0.7760, Test: 0.7990\n",
      "Epoch: 5050, Loss: 0.0010, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 5100, Loss: 0.0008, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 5150, Loss: 0.0024, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 5200, Loss: 0.0006, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 5250, Loss: 0.0007, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 5300, Loss: 0.0012, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 5350, Loss: 0.0004, Train: 1.0000, Val: 0.7660, Test: 0.7990\n",
      "Epoch: 5400, Loss: 0.0011, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 5450, Loss: 0.0009, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch: 5500, Loss: 0.0005, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 5550, Loss: 0.0027, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 5600, Loss: 0.0011, Train: 1.0000, Val: 0.7100, Test: 0.7990\n",
      "Epoch: 5650, Loss: 0.0007, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 5700, Loss: 0.0032, Train: 1.0000, Val: 0.7120, Test: 0.7990\n",
      "Epoch: 5750, Loss: 0.0097, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 5800, Loss: 0.0006, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 5850, Loss: 0.0020, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 5900, Loss: 0.0012, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 5950, Loss: 0.0123, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 6000, Loss: 0.0015, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 6050, Loss: 0.0009, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 6100, Loss: 0.0005, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 6150, Loss: 0.0011, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 6200, Loss: 0.0008, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 6250, Loss: 0.0015, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 6300, Loss: 0.0024, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 6350, Loss: 0.0003, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 6400, Loss: 0.0010, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 6450, Loss: 0.0035, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 6500, Loss: 0.0004, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 6550, Loss: 0.0005, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 6600, Loss: 0.0031, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 6650, Loss: 0.0014, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 6700, Loss: 0.0007, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 6750, Loss: 0.0005, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 6800, Loss: 0.0006, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 6850, Loss: 0.0017, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 6900, Loss: 0.0030, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 6950, Loss: 0.0010, Train: 1.0000, Val: 0.7000, Test: 0.7990\n",
      "Epoch: 7000, Loss: 0.0115, Train: 1.0000, Val: 0.7040, Test: 0.7990\n",
      "Epoch: 7050, Loss: 0.0007, Train: 1.0000, Val: 0.7040, Test: 0.7990\n",
      "Epoch: 7100, Loss: 0.0020, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 7150, Loss: 0.0009, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 7200, Loss: 0.0003, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 7250, Loss: 0.0006, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 7300, Loss: 0.0009, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 7350, Loss: 0.0003, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 7400, Loss: 0.0005, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 7450, Loss: 0.0004, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 7500, Loss: 0.0027, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 7550, Loss: 0.0019, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 7600, Loss: 0.0143, Train: 1.0000, Val: 0.7100, Test: 0.7990\n",
      "Epoch: 7650, Loss: 0.0010, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 7700, Loss: 0.0023, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 7750, Loss: 0.0019, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 7800, Loss: 0.0001, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 7850, Loss: 0.0013, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 7900, Loss: 0.0052, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 7950, Loss: 0.0147, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 8000, Loss: 0.0025, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 8050, Loss: 0.0004, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 8100, Loss: 0.0003, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 8150, Loss: 0.0004, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 8200, Loss: 0.0019, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 8250, Loss: 0.0006, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 8300, Loss: 0.0026, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 8350, Loss: 0.0002, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 8400, Loss: 0.0142, Train: 1.0000, Val: 0.7080, Test: 0.7990\n",
      "Epoch: 8450, Loss: 0.0030, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 8500, Loss: 0.0006, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 8550, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 8600, Loss: 0.0007, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 8650, Loss: 0.0009, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 8700, Loss: 0.0004, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 8750, Loss: 0.0003, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 8800, Loss: 0.0049, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 8850, Loss: 0.0136, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 8900, Loss: 0.0033, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 8950, Loss: 0.0007, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 9000, Loss: 0.0017, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 9050, Loss: 0.0134, Train: 1.0000, Val: 0.7760, Test: 0.7990\n",
      "Epoch: 9100, Loss: 0.0002, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 9150, Loss: 0.0008, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 9200, Loss: 0.0003, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 9250, Loss: 0.0009, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 9300, Loss: 0.0004, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 9350, Loss: 0.0005, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 9400, Loss: 0.0002, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 9450, Loss: 0.0031, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 9500, Loss: 0.0003, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 9550, Loss: 0.0011, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 9600, Loss: 0.0014, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 9650, Loss: 0.0003, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 9700, Loss: 0.0016, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 9750, Loss: 0.0004, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 9800, Loss: 0.0003, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 9850, Loss: 0.0003, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 9900, Loss: 0.0003, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 9950, Loss: 0.0023, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 10000, Loss: 0.0036, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 10050, Loss: 0.0005, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 10100, Loss: 0.0007, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 10150, Loss: 0.0007, Train: 1.0000, Val: 0.7060, Test: 0.7990\n",
      "Epoch: 10200, Loss: 0.0002, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 10250, Loss: 0.0002, Train: 0.9929, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 10300, Loss: 0.0004, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 10350, Loss: 0.0007, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 10400, Loss: 0.0004, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 10450, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 10500, Loss: 0.0047, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 10550, Loss: 0.0002, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 10600, Loss: 0.0016, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 10650, Loss: 0.0024, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 10700, Loss: 0.0002, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 10750, Loss: 0.0006, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 10800, Loss: 0.0003, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 10850, Loss: 0.0003, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 10900, Loss: 0.0013, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 10950, Loss: 0.0009, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 11000, Loss: 0.0003, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 11050, Loss: 0.0013, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 11100, Loss: 0.0002, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 11150, Loss: 0.0002, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 11200, Loss: 0.0007, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 11250, Loss: 0.0003, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 11300, Loss: 0.0003, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 11350, Loss: 0.0001, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 11400, Loss: 0.0012, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 11450, Loss: 0.0003, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 11500, Loss: 0.0049, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 11550, Loss: 0.0003, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 11600, Loss: 0.0005, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 11650, Loss: 0.0001, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 11700, Loss: 0.0008, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 11750, Loss: 0.0002, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 11800, Loss: 0.0013, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 11850, Loss: 0.0007, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 11900, Loss: 0.0018, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 11950, Loss: 0.0005, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 12000, Loss: 0.0002, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 12050, Loss: 0.0028, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 12100, Loss: 0.0033, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 12150, Loss: 0.0014, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 12200, Loss: 0.0034, Train: 1.0000, Val: 0.7160, Test: 0.7990\n",
      "Epoch: 12250, Loss: 0.0011, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 12300, Loss: 0.0006, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 12350, Loss: 0.0005, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 12400, Loss: 0.0010, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 12450, Loss: 0.0007, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 12500, Loss: 0.0010, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 12550, Loss: 0.0001, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 12600, Loss: 0.0003, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 12650, Loss: 0.0004, Train: 1.0000, Val: 0.7140, Test: 0.7990\n",
      "Epoch: 12700, Loss: 0.0003, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 12750, Loss: 0.0001, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 12800, Loss: 0.0002, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 12850, Loss: 0.0009, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 12900, Loss: 0.0004, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 12950, Loss: 0.0010, Train: 1.0000, Val: 0.6600, Test: 0.7990\n",
      "Epoch: 13000, Loss: 0.0001, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 13050, Loss: 0.0001, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 13100, Loss: 0.0006, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 13150, Loss: 0.0037, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 13200, Loss: 0.0008, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 13250, Loss: 0.0002, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 13300, Loss: 0.0009, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 13350, Loss: 0.0015, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 13400, Loss: 0.0002, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 13450, Loss: 0.0001, Train: 1.0000, Val: 0.7120, Test: 0.7990\n",
      "Epoch: 13500, Loss: 0.0004, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 13550, Loss: 0.0002, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 13600, Loss: 0.0004, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 13650, Loss: 0.0004, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 13700, Loss: 0.0003, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 13750, Loss: 0.0010, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 13800, Loss: 0.0005, Train: 1.0000, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 13850, Loss: 0.0018, Train: 1.0000, Val: 0.7120, Test: 0.7990\n",
      "Epoch: 13900, Loss: 0.0272, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 13950, Loss: 0.0001, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 14000, Loss: 0.0001, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 14050, Loss: 0.0009, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 14100, Loss: 0.0003, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 14150, Loss: 0.0014, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 14200, Loss: 0.0021, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 14250, Loss: 0.0026, Train: 1.0000, Val: 0.7140, Test: 0.7990\n",
      "Epoch: 14300, Loss: 0.0028, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 14350, Loss: 0.0045, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 14400, Loss: 0.0014, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 14450, Loss: 0.0049, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 14500, Loss: 0.0001, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 14550, Loss: 0.0004, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 14600, Loss: 0.0008, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 14650, Loss: 0.0016, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 14700, Loss: 0.0036, Train: 0.9929, Val: 0.7040, Test: 0.7990\n",
      "Epoch: 14750, Loss: 0.0001, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 14800, Loss: 0.0002, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 14850, Loss: 0.0005, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 14900, Loss: 0.0002, Train: 1.0000, Val: 0.7140, Test: 0.7990\n",
      "Epoch: 14950, Loss: 0.0001, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 15000, Loss: 0.0017, Train: 1.0000, Val: 0.7180, Test: 0.7990\n",
      "Epoch: 15050, Loss: 0.0006, Train: 1.0000, Val: 0.7160, Test: 0.7990\n",
      "Epoch: 15100, Loss: 0.0002, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 15150, Loss: 0.0031, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 15200, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 15250, Loss: 0.0007, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 15300, Loss: 0.0004, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 15350, Loss: 0.0001, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 15400, Loss: 0.0045, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 15450, Loss: 0.0006, Train: 1.0000, Val: 0.7220, Test: 0.7990\n",
      "Epoch: 15500, Loss: 0.0003, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 15550, Loss: 0.0017, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 15600, Loss: 0.0006, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 15650, Loss: 0.0186, Train: 0.9857, Val: 0.6440, Test: 0.7990\n",
      "Epoch: 15700, Loss: 0.0017, Train: 1.0000, Val: 0.6640, Test: 0.7990\n",
      "Epoch: 15750, Loss: 0.0000, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 15800, Loss: 0.0006, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 15850, Loss: 0.0015, Train: 1.0000, Val: 0.7340, Test: 0.7990\n",
      "Epoch: 15900, Loss: 0.0005, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 15950, Loss: 0.0002, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 16000, Loss: 0.0002, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 16050, Loss: 0.0002, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 16100, Loss: 0.0004, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 16150, Loss: 0.0006, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 16200, Loss: 0.0001, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 16250, Loss: 0.0002, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 16300, Loss: 0.0026, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 16350, Loss: 0.0026, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 16400, Loss: 0.0007, Train: 1.0000, Val: 0.7260, Test: 0.7990\n",
      "Epoch: 16450, Loss: 0.0019, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 16500, Loss: 0.0001, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 16550, Loss: 0.0004, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 16600, Loss: 0.0001, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 16650, Loss: 0.0002, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 16700, Loss: 0.0011, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 16750, Loss: 0.0005, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 16800, Loss: 0.0005, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 16850, Loss: 0.0001, Train: 1.0000, Val: 0.7700, Test: 0.7990\n",
      "Epoch: 16900, Loss: 0.0002, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 16950, Loss: 0.0003, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 17000, Loss: 0.0002, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 17050, Loss: 0.0009, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 17100, Loss: 0.0005, Train: 1.0000, Val: 0.7280, Test: 0.7990\n",
      "Epoch: 17150, Loss: 0.0022, Train: 1.0000, Val: 0.7160, Test: 0.7990\n",
      "Epoch: 17200, Loss: 0.0037, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 17250, Loss: 0.0097, Train: 1.0000, Val: 0.7600, Test: 0.7990\n",
      "Epoch: 17300, Loss: 0.0006, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 17350, Loss: 0.0007, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 17400, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 17450, Loss: 0.0001, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 17500, Loss: 0.0072, Train: 1.0000, Val: 0.6880, Test: 0.7990\n",
      "Epoch: 17550, Loss: 0.0013, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 17600, Loss: 0.0031, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 17650, Loss: 0.0000, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 17700, Loss: 0.0003, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 17750, Loss: 0.0010, Train: 1.0000, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 17800, Loss: 0.0003, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 17850, Loss: 0.0005, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 17900, Loss: 0.0004, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 17950, Loss: 0.0002, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 18000, Loss: 0.0009, Train: 1.0000, Val: 0.7460, Test: 0.7990\n",
      "Epoch: 18050, Loss: 0.0002, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 18100, Loss: 0.0002, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 18150, Loss: 0.0003, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 18200, Loss: 0.0003, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 18250, Loss: 0.0004, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 18300, Loss: 0.0005, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 18350, Loss: 0.0014, Train: 1.0000, Val: 0.7160, Test: 0.7990\n",
      "Epoch: 18400, Loss: 0.0035, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 18450, Loss: 0.0006, Train: 1.0000, Val: 0.7380, Test: 0.7990\n",
      "Epoch: 18500, Loss: 0.0010, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 18550, Loss: 0.0011, Train: 1.0000, Val: 0.7160, Test: 0.7990\n",
      "Epoch: 18600, Loss: 0.0002, Train: 1.0000, Val: 0.7420, Test: 0.7990\n",
      "Epoch: 18650, Loss: 0.0022, Train: 1.0000, Val: 0.7440, Test: 0.7990\n",
      "Epoch: 18700, Loss: 0.0026, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 18750, Loss: 0.0003, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 18800, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 18850, Loss: 0.0010, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 18900, Loss: 0.0155, Train: 1.0000, Val: 0.7400, Test: 0.7990\n",
      "Epoch: 18950, Loss: 0.0001, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 19000, Loss: 0.0047, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 19050, Loss: 0.0012, Train: 1.0000, Val: 0.7240, Test: 0.7990\n",
      "Epoch: 19100, Loss: 0.0005, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 19150, Loss: 0.0001, Train: 1.0000, Val: 0.7520, Test: 0.7990\n",
      "Epoch: 19200, Loss: 0.0006, Train: 1.0000, Val: 0.7580, Test: 0.7990\n",
      "Epoch: 19250, Loss: 0.0001, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 19300, Loss: 0.0028, Train: 1.0000, Val: 0.7660, Test: 0.7990\n",
      "Epoch: 19350, Loss: 0.0058, Train: 1.0000, Val: 0.7200, Test: 0.7990\n",
      "Epoch: 19400, Loss: 0.0003, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 19450, Loss: 0.0012, Train: 1.0000, Val: 0.7500, Test: 0.7990\n",
      "Epoch: 19500, Loss: 0.0000, Train: 1.0000, Val: 0.7320, Test: 0.7990\n",
      "Epoch: 19550, Loss: 0.0009, Train: 1.0000, Val: 0.7360, Test: 0.7990\n",
      "Epoch: 19600, Loss: 0.0004, Train: 1.0000, Val: 0.7560, Test: 0.7990\n",
      "Epoch: 19650, Loss: 0.0033, Train: 1.0000, Val: 0.7300, Test: 0.7990\n",
      "Epoch: 19700, Loss: 0.0005, Train: 1.0000, Val: 0.7680, Test: 0.7990\n",
      "Epoch: 19750, Loss: 0.0001, Train: 1.0000, Val: 0.7640, Test: 0.7990\n",
      "Epoch: 19800, Loss: 0.0006, Train: 1.0000, Val: 0.7620, Test: 0.7990\n",
      "Epoch: 19850, Loss: 0.0001, Train: 1.0000, Val: 0.7540, Test: 0.7990\n",
      "Epoch: 19900, Loss: 0.0005, Train: 1.0000, Val: 0.7480, Test: 0.7990\n",
      "Epoch: 19950, Loss: 0.0030, Train: 1.0000, Val: 0.7220, Test: 0.7990\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import dropout_edge\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DATASETS_DIR = \"./pyg_datasets\"\n",
    "\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py\n",
    "dataset = Planetoid(root=DATASETS_DIR, name='Cora')\n",
    "data = dataset[0].to(DEVICE)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        #edge_index, edge_mask = dropout_edge(edge_index, p=0.8)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(dataset.num_features, 16, dataset.num_classes)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=0.01)  # Only perform weight-decay on first convolution.\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(20000):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a019b3f-800b-4966-9d9d-c373b7b2b02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356be44d-6107-4287-bdda-0d70efe26ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
