{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFQxATSSWofs",
        "outputId": "f2c0b6a4-f9f4-481f-e14a-034d1ac13872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Google Drive Folder already existed.\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = \"MLAP_test_eval_final_full_model\"\n",
        "\n",
        "# --- do not change below this ---\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "drive.mount(DRIVE_PATH, force_remount=True)\n",
        "\n",
        "# shell commands for directory with space must be\n",
        "# quoted, but not necessary in python\n",
        "COLAB_PATH = \"Colab Notebooks\"\n",
        "COLLAB_PATH_ESC = f\"\\\"{COLAB_PATH}\\\"\"\n",
        "\n",
        "# python path\n",
        "nb_path = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLAB_PATH, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "# shell path\n",
        "nb_path_bash = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLLAB_PATH_ESC, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "  os.makedirs(nb_path)\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Google Drive Folder already existed.\")\n",
        "\n",
        "try:\n",
        "  # create symlink from drive to workspace\n",
        "  os.symlink(nb_path, \"/content/notebooks\")\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Symlink already existed.\")\n",
        "\n",
        "sys.path.insert(0, nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJIm9RQtWpcW",
        "outputId": "38c3c861-e26d-4e7c-9356-316e8be12349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torch_geometric ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkKs0jaewe9y"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F5LoicGwwajW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "\n",
        "class ASTNodeEncoder(torch.nn.Module):\n",
        "    '''\n",
        "        Input:\n",
        "            x: default node feature. the first and second column represents node type and node attributes.\n",
        "            depth: The depth of the node in the AST.\n",
        "\n",
        "        Output:\n",
        "            emb_dim-dimensional vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, emb_dim, num_nodetypes, num_nodeattributes, max_depth):\n",
        "        super(ASTNodeEncoder, self).__init__()\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        self.type_encoder = torch.nn.Embedding(num_nodetypes, emb_dim)\n",
        "        self.attribute_encoder = torch.nn.Embedding(num_nodeattributes, emb_dim)\n",
        "        self.depth_encoder = torch.nn.Embedding(self.max_depth + 1, emb_dim)\n",
        "\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * emb_dim, 2 * emb_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2 * emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, depth):\n",
        "        depth[depth > self.max_depth] = self.max_depth\n",
        "        mlp_input = torch.hstack(\n",
        "            (\n",
        "                self.type_encoder(x[:,0]), \n",
        "                self.attribute_encoder(x[:,1]), \n",
        "                self.depth_encoder(depth)\n",
        "             )\n",
        "        )\n",
        "        return self.node_mlp(mlp_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FamuAT6rV3G6"
      },
      "source": [
        "### Utils - AST / MLAP\n",
        "\n",
        "Utilities for editing and parsing the AST inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a8ZPzicd-pjb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "def get_vocab_mapping(seq_list, num_vocab):\n",
        "    '''\n",
        "        Input:\n",
        "            seq_list: a list of sequences\n",
        "            num_vocab: vocabulary size\n",
        "        Output:\n",
        "            vocab2idx:\n",
        "                A dictionary that maps vocabulary into integer index.\n",
        "                Additioanlly, we also index '__UNK__' and '__EOS__'\n",
        "                '__UNK__' : out-of-vocabulary term\n",
        "                '__EOS__' : end-of-sentence\n",
        "\n",
        "            idx2vocab:\n",
        "                A list that maps idx to actual vocabulary.\n",
        "    '''\n",
        "\n",
        "    vocab_cnt = {}\n",
        "    vocab_list = []\n",
        "    for seq in seq_list:\n",
        "        for w in seq:\n",
        "            if w in vocab_cnt:\n",
        "                vocab_cnt[w] += 1\n",
        "            else:\n",
        "                vocab_cnt[w] = 1\n",
        "                vocab_list.append(w)\n",
        "\n",
        "    cnt_list = np.array([vocab_cnt[w] for w in vocab_list])\n",
        "    topvocab = np.argsort(-cnt_list, kind = 'stable')[:num_vocab]\n",
        "\n",
        "    print('Coverage of top {} vocabulary:'.format(num_vocab))\n",
        "    print(float(np.sum(cnt_list[topvocab]))/np.sum(cnt_list))\n",
        "\n",
        "    vocab2idx = {vocab_list[vocab_idx]: idx for idx, vocab_idx in enumerate(topvocab)}\n",
        "    idx2vocab = [vocab_list[vocab_idx] for vocab_idx in topvocab]\n",
        "\n",
        "    vocab2idx['__UNK__'] = num_vocab\n",
        "    idx2vocab.append('__UNK__')\n",
        "\n",
        "    vocab2idx['__EOS__'] = num_vocab + 1\n",
        "    idx2vocab.append('__EOS__')\n",
        "\n",
        "    # test the correspondence between vocab2idx and idx2vocab\n",
        "    for idx, vocab in enumerate(idx2vocab):\n",
        "        assert(idx == vocab2idx[vocab])\n",
        "\n",
        "    # test that the idx of '__EOS__' is len(idx2vocab) - 1.\n",
        "    # This fact will be used in decode_arr_to_seq, when finding __EOS__\n",
        "    assert(vocab2idx['__EOS__'] == len(idx2vocab) - 1)\n",
        "\n",
        "    return vocab2idx, idx2vocab\n",
        "\n",
        "def augment_edge(data):\n",
        "    '''\n",
        "        Input:\n",
        "            data: PyG data object\n",
        "        Output:\n",
        "            data (edges are augmented in the following ways):\n",
        "                data.edge_index: Added next-token edge. The inverse edges were also added.\n",
        "                data.edge_attr (torch.Long):\n",
        "                    data.edge_attr[:,0]: whether it is AST edge (0) for next-token edge (1)\n",
        "                    data.edge_attr[:,1]: whether it is original direction (0) or inverse direction (1)\n",
        "    '''\n",
        "    ##### AST edge\n",
        "    edge_index_ast = data.edge_index\n",
        "    edge_attr_ast = torch.zeros((edge_index_ast.size(1), 2))\n",
        "\n",
        "    ##### Inverse AST edge\n",
        "    edge_index_ast_inverse = torch.stack([edge_index_ast[1], edge_index_ast[0]], dim = 0)\n",
        "    edge_attr_ast_inverse = torch.cat([torch.zeros(edge_index_ast_inverse.size(1), 1), torch.ones(edge_index_ast_inverse.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Next-token edge\n",
        "\n",
        "    ## Obtain attributed nodes and get their indices in dfs order\n",
        "    # attributed_node_idx = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "    # attributed_node_idx_in_dfs_order = attributed_node_idx[torch.argsort(data.node_dfs_order[attributed_node_idx].view(-1,))]\n",
        "\n",
        "    ## Since the nodes are already sorted in dfs ordering in our case, we can just do the following.\n",
        "    attributed_node_idx_in_dfs_order = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "\n",
        "    ## build next token edge\n",
        "    # Given: attributed_node_idx_in_dfs_order\n",
        "    #        [1, 3, 4, 5, 8, 9, 12]\n",
        "    # Output:\n",
        "    #    [[1, 3, 4, 5, 8, 9]\n",
        "    #     [3, 4, 5, 8, 9, 12]\n",
        "    edge_index_nextoken = torch.stack([attributed_node_idx_in_dfs_order[:-1], attributed_node_idx_in_dfs_order[1:]], dim = 0)\n",
        "    edge_attr_nextoken = torch.cat([torch.ones(edge_index_nextoken.size(1), 1), torch.zeros(edge_index_nextoken.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Inverse next-token edge\n",
        "    edge_index_nextoken_inverse = torch.stack([edge_index_nextoken[1], edge_index_nextoken[0]], dim = 0)\n",
        "    edge_attr_nextoken_inverse = torch.ones((edge_index_nextoken.size(1), 2))\n",
        "\n",
        "    data.edge_index = torch.cat([edge_index_ast, edge_index_ast_inverse, edge_index_nextoken, edge_index_nextoken_inverse], dim = 1)\n",
        "    data.edge_attr = torch.cat([edge_attr_ast,   edge_attr_ast_inverse, edge_attr_nextoken,  edge_attr_nextoken_inverse], dim = 0)\n",
        "\n",
        "    return data\n",
        "\n",
        "def encode_y_to_arr(data, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        data: PyG graph object\n",
        "        output: add y_arr to data \n",
        "    '''\n",
        "    # PyG >= 1.5.0\n",
        "    seq = data.y\n",
        "    data.y_arr = encode_seq_to_arr(seq, vocab2idx, max_seq_len)\n",
        "    return data\n",
        "\n",
        "def encode_seq_to_arr(seq, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        seq: A list of words\n",
        "        output: add y_arr (torch.Tensor)\n",
        "    '''\n",
        "    augmented_seq = seq[:max_seq_len] + ['__EOS__'] * max(0, max_seq_len - len(seq))\n",
        "    return torch.tensor([[vocab2idx[w] if w in vocab2idx else vocab2idx['__UNK__'] for w in augmented_seq]], dtype = torch.long)\n",
        "\n",
        "\n",
        "def decode_arr_to_seq(arr, idx2vocab):\n",
        "    '''\n",
        "        Input: torch 1d array: y_arr\n",
        "        Output: a sequence of words.\n",
        "    '''\n",
        "    # find the position of __EOS__ (the last vocab in idx2vocab)\n",
        "    eos_idx_list = (arr == len(idx2vocab) - 1).nonzero() \n",
        "    if len(eos_idx_list) > 0:\n",
        "        # find the smallest __EOS__\n",
        "        clippted_arr = arr[: torch.min(eos_idx_list)] \n",
        "    else:\n",
        "        clippted_arr = arr\n",
        "\n",
        "    return list(map(lambda x: idx2vocab[x], clippted_arr.cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP9s6wJ1-0Cj"
      },
      "source": [
        "### Utils - CAP / GRACE\n",
        "Utilities for generating Graph Contrastive Pairs\n",
        "\n",
        "See: [Graph Contrastive Learning with Adaptive Augmentation (2020) - Zhu, Xu, Yu, Liu, Wu, Wang](https://arxiv.org/abs/2010.14945)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6QXwE3BnV2vf"
      },
      "outputs": [],
      "source": [
        "# ---- CAP functions ----\n",
        "# from: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/functional.py\n",
        "# and: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/utils.py\n",
        "def compute_pr(edge_index, damp: float = 0.85, k: int = 10):\n",
        "    # page rank\n",
        "    # interesting comment: https://github.com/CRIPAC-DIG/GCA/issues/4\n",
        "    num_nodes = edge_index.max().item() + 1\n",
        "    deg_out = degree(edge_index[0])\n",
        "    x = torch.ones((num_nodes, )).to(edge_index.device).to(torch.float32)\n",
        "\n",
        "    for i in range(k):\n",
        "        edge_msg = x[edge_index[0]] / deg_out[edge_index[0]]\n",
        "        agg_msg = scatter(edge_msg, edge_index[1], reduce='sum')\n",
        "\n",
        "        x = (1 - damp) * x + damp * agg_msg\n",
        "\n",
        "    return x\n",
        "\n",
        "def eigenvector_centrality(data):\n",
        "    graph = to_networkx(data)\n",
        "    x = nx.eigenvector_centrality_numpy(graph)\n",
        "    x = [x[i] for i in range(data.num_nodes)]\n",
        "    return torch.tensor(x, dtype=torch.float32).to(data.edge_index.device)\n",
        "\n",
        "def drop_feature(x, drop_prob):\n",
        "    drop_mask = torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1) < drop_prob\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0\n",
        "    return x\n",
        "\n",
        "def drop_feature_weighted(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w.repeat(x.size(0)).view(x.size(0), -1)\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "    x = x.clone()\n",
        "    x[drop_mask] = 0.\n",
        "    return x\n",
        "\n",
        "def drop_feature_weighted_2(x, w, p: float, threshold: float = 0.7, dgi_task=False):\n",
        "    w = w / w.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w\n",
        "\n",
        "    if (dgi_task):\n",
        "        drop_mask = torch.bernoulli(1. - drop_prob).to(torch.bool)\n",
        "    else:\n",
        "        drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0.\n",
        "    return x\n",
        "\n",
        "def feature_drop_weights(x, node_c):\n",
        "    x = x.to(torch.bool).to(torch.float32)\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "    return s\n",
        "\n",
        "def feature_drop_weights_dense(x, node_c):\n",
        "    x = x.abs()\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "    return s\n",
        "\n",
        "\n",
        "def drop_edge_weighted(edge_index, edge_weights, p: float, threshold: float = 1., dgi_task=False):\n",
        "    edge_weights = edge_weights / edge_weights.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "\n",
        "    edge_weights = edge_weights.where(edge_weights < threshold, torch.ones_like(edge_weights) * threshold)\n",
        "\n",
        "    if (dgi_task): \n",
        "        # drop edges by importance\n",
        "        sel_mask = torch.bernoulli(edge_weights).to(torch.bool)\n",
        "    else:\n",
        "        sel_mask = torch.bernoulli(1. - edge_weights).to(torch.bool)\n",
        "    return edge_index[:, sel_mask]\n",
        "\n",
        "def degree_drop_weights(edge_index):\n",
        "    edge_index_ = to_undirected(edge_index)\n",
        "    deg = degree(edge_index_[1])\n",
        "    deg_col = deg[edge_index[1]].to(torch.float32)\n",
        "    s_col = torch.log(deg_col)\n",
        "    weights = (s_col.max() - s_col) / (s_col.max() - s_col.mean())\n",
        "    return weights\n",
        "\n",
        "def pr_drop_weights(edge_index, aggr: str = 'sink', k: int = 10):\n",
        "    pv = compute_pr(edge_index, k=k)\n",
        "    pv_row = pv[edge_index[0]].to(torch.float32)\n",
        "    pv_col = pv[edge_index[1]].to(torch.float32)\n",
        "    s_row = torch.log(pv_row)\n",
        "    s_col = torch.log(pv_col)\n",
        "    if aggr == 'sink':\n",
        "        s = s_col\n",
        "    elif aggr == 'source':\n",
        "        s = s_row\n",
        "    elif aggr == 'mean':\n",
        "        s = (s_col + s_row) * 0.5\n",
        "    else:\n",
        "        s = s_col\n",
        "    weights = (s.max() - s) / (s.max() - s.mean())\n",
        "    return weights\n",
        "\n",
        "def evc_drop_weights(data):\n",
        "    evc = eigenvector_centrality(data)\n",
        "    evc = evc.where(evc > 0, torch.zeros_like(evc))\n",
        "    evc = evc + 1e-8\n",
        "    s = evc.log()\n",
        "\n",
        "    edge_index = data.edge_index\n",
        "    s_row, s_col = s[edge_index[0]], s[edge_index[1]]\n",
        "    s = s_col\n",
        "    return (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "def graph_perturb(data, drop_scheme='pr'):\n",
        "  if drop_scheme == 'degree':\n",
        "      drop_weights = degree_drop_weights(data.edge_index)\n",
        "      edge_index_ = to_undirected(data.edge_index)\n",
        "      node_deg = degree(edge_index_[1])\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_deg)\n",
        "  elif drop_scheme == 'pr':\n",
        "      drop_weights = pr_drop_weights(data.edge_index, aggr='sink', k=200)\n",
        "      node_pr = compute_pr(data.edge_index)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_pr)\n",
        "  elif drop_scheme == 'evc':\n",
        "      drop_weights = evc_drop_weights(data)\n",
        "      node_evc = eigenvector_centrality(data)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_evc)\n",
        "  else:\n",
        "      feature_weights = torch.ones((data.x.size(1),))\n",
        "      drop_weights = None\n",
        "  \n",
        "  return feature_weights, drop_weights\n",
        "\n",
        "def drop_edge(data, drop_edge_rate, drop_weights, drop_scheme='pr', drop_edge_weighted_threshold=0.7, dgi_task=False):\n",
        "  if drop_scheme == 'uniform':\n",
        "      return dropout_edge(data.edge_index, p=drop_edge_rate)[0]\n",
        "  elif drop_scheme in ['degree', 'evc', 'pr']:\n",
        "      return drop_edge_weighted(\n",
        "          data.edge_index, \n",
        "          drop_weights, \n",
        "          p=drop_edge_rate, \n",
        "          threshold=drop_edge_weighted_threshold,\n",
        "          dgi_task=dgi_task\n",
        "        )\n",
        "  else:\n",
        "      raise Exception(f'undefined drop scheme: {drop_scheme}')\n",
        "\n",
        "# @torch.no_grad()\n",
        "def get_contrastive_graph_pair(data, drop_scheme='pr', drop_feature_rates=(0.7, 0.7), drop_edge_rates=(0.5, 0.5), dgi_task=False):\n",
        "  # use augmentation scheme to determine the weights of each node\n",
        "  # i.e. pagerank, eigenvector centrality, node degree\n",
        "  feat_weights, drop_weights = graph_perturb(data, drop_scheme)\n",
        "\n",
        "  # apply drop edge according to computed features\n",
        "  dr_e_1, dr_e_2 = drop_edge_rates\n",
        "  edge_index_1 = drop_edge(data, dr_e_1, drop_weights, drop_scheme, dgi_task=dgi_task)\n",
        "\n",
        "  if (not dgi_task):\n",
        "    edge_index_2 = drop_edge(data, dr_e_2, drop_weights, drop_scheme)\n",
        "\n",
        "  dr_f_1, dr_f_2 = drop_feature_rates\n",
        "\n",
        "  if drop_scheme in ['pr', 'degree', 'evc']:\n",
        "    # graph-aware drop feature\n",
        "    x_1 = drop_feature_weighted_2(data.x, feat_weights, dr_f_1, dgi_task=dgi_task)\n",
        "    #e_1 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_1)\n",
        "\n",
        "    if not dgi_task:\n",
        "        x_2 = drop_feature_weighted_2(data.x, feat_weights, dr_f_2, dgi_task=dgi_task)\n",
        "        #e_2 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_2)\n",
        "  else:\n",
        "    # naive drop feature\n",
        "    x_1 = drop_feature(data.x, dr_f_1)\n",
        "    #e_1 = drop_feature(data.edge_attr, dr_f_1)\n",
        "    \n",
        "    x_2 = drop_feature(data.x, dr_f_2)\n",
        "    e_2 = drop_feature(data.edge_attr, dr_f_2)\n",
        "  \n",
        "  if dgi_task:\n",
        "      return (x_1, edge_index_1)\n",
        "\n",
        "  return (\n",
        "      # graph 1\n",
        "      (x_1, edge_index_1),\n",
        "      # graph 2\n",
        "      (x_2, edge_index_2)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAs0mAoVjXQ"
      },
      "source": [
        "### Decoders\n",
        "\n",
        "Specific to the AST code task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JhqdRjDJVpbo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LinearDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        \"\"\"\n",
        "        Noted in the MLAP paper to have performed better than the LSTM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [nn.Linear(dim_h, len(vocab2idx)) for _ in range(max_seq_len)]\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        return [d(layer_reps[-1]) for d in self.decoders]\n",
        "\n",
        "\n",
        "class LSTMDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "        self.lstm = nn.LSTMCell(dim_h, dim_h)\n",
        "        self.w_hc = nn.Linear(dim_h * 2, dim_h)\n",
        "        self.layernorm = nn.LayerNorm(dim_h)\n",
        "        self.vocab_encoder = nn.Embedding(len(vocab2idx), dim_h)\n",
        "        self.vocab_bias = nn.Parameter(torch.zeros(len(vocab2idx)))\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        if (training):\n",
        "            batched_label = torch.vstack(\n",
        "                [\n",
        "                    encode_seq_to_arr(label, self.vocab2idx, self.max_seq_len - 1) \n",
        "                    for label in labels\n",
        "                ]\n",
        "            )\n",
        "            batched_label = torch.hstack((torch.zeros((batch_size, 1), dtype=torch.int64), batched_label))\n",
        "            true_emb = self.vocab_encoder(batched_label.to(device=self.device))\n",
        "        \n",
        "        h_t, c_t = layer_reps[-1].clone(), layer_reps[-1].clone()\n",
        "\n",
        "        layer_reps = layer_reps.transpose(0,1)\n",
        "        output = []\n",
        "\n",
        "        pred_emb = self.vocab_encoder(torch.zeros((batch_size), dtype=torch.int64, device=self.device))\n",
        "        vocab_mat = self.vocab_encoder(torch.arange(len(self.vocab2idx), dtype=torch.int64, device=self.device))\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            if training: \n",
        "                # teacher forcing\n",
        "                input = true_emb[:, i]\n",
        "            else:\n",
        "                input = pred_emb\n",
        "            \n",
        "            h_t, c_t = self.lstm(input, (h_t, c_t))\n",
        "\n",
        "            # (batch_size, L + 1)\n",
        "            a = F.softmax(torch.bmm(layer_reps, h_t.unsqueeze(-1)).squeeze(-1), dim=1)  \n",
        "            context = torch.bmm(a.unsqueeze(1), layer_reps).squeeze(1)\n",
        "\n",
        "            # (batch_size, dim_h)\n",
        "            pred_emb = torch.tanh(self.layernorm(self.w_hc(torch.hstack((h_t, context)))))  \n",
        "\n",
        "            # (batch_size, len(vocab)) x max_seq_len\n",
        "            output.append(torch.matmul(pred_emb, vocab_mat.T) + self.vocab_bias.unsqueeze(0))\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnvaaBCBTUfU"
      },
      "source": [
        "### GIN\n",
        "\n",
        "Graph Isomorphism Network\n",
        "\n",
        "See: [How Powerful are Graph Neural Networks? (2018) - Xu, Hu, Leskovec, Jegelka](https://arxiv.org/abs/1810.00826v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T0NfVLOlTSx_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, dim_h, mlp, **kwargs):\n",
        "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.mlp = mlp\n",
        "        self.bn = BatchNorm1d(dim_h)\n",
        "        self.edge_encoder = Linear(2, dim_h)\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        output = self.mlp(\n",
        "            self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "        )\n",
        "        return self.bn(output)\n",
        "    \n",
        "    def message(self, x_j, edge_attr):\n",
        "        return x_j + edge_attr\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        return aggr_out + x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBwnEmwhVxwQ"
      },
      "source": [
        "### MLAP / DGI\n",
        "\n",
        "MLAP\n",
        "- see: [Multi-Level Attention Pooling for Graph Neural Networks - Unifying Graph Representations with Multiple Localities (2021) - Itoh, Kubo, Ikeda](https://arxiv.org/abs/2103.01488)\n",
        "\n",
        "\n",
        "DGI\n",
        "- see: [Deep Graph Infomax (2018) - Velickovic, Fedus, Hamilton, Lio, Bengio, Hjelm](https://arxiv.org/abs/1809.10341)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NDywjQj9VyVd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear, Sequential, ReLU, ELU, Sigmoid\n",
        "\n",
        "from torch_geometric.nn.conv import GINConv\n",
        "from torch_geometric.nn.norm import GraphNorm\n",
        "from torch_geometric.nn.glob import AttentionalAggregation\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class DISC(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super(DISC, self).__init__()\n",
        "        W = torch.empty(dim_h, dim_h)\n",
        "        torch.nn.init.xavier_normal_(W)\n",
        "        self.W = torch.nn.Parameter(W)\n",
        "        self.W.requires_grad = True\n",
        "        self.sig = Sigmoid()\n",
        "    \n",
        "    def forward(self, h, s):\n",
        "        out = torch.matmul(self.W, s)\n",
        "        out = torch.matmul(h, out.unsqueeze(-1))\n",
        "        return self.sig(out)\n",
        "\n",
        "\n",
        "class MLAP_GIN(torch.nn.Module):\n",
        "    def __init__(self, dim_h, batch_size, depth, node_encoder, norm=False, residual=False, dropout=False):\n",
        "        super(MLAP_GIN, self).__init__()\n",
        "\n",
        "        self.dim_h = dim_h\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "\n",
        "        self.node_encoder = node_encoder\n",
        "\n",
        "        self.norm = norm\n",
        "        self.residual = residual\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.loss_fn = torch.nn.BCELoss(reduction='sum')\n",
        "        self.discriminator = DISC(dim_h)\n",
        "\n",
        "        # non-linear projection function for cl task\n",
        "        self.projection = Sequential(\n",
        "            Linear(dim_h, int(dim_h/8)),\n",
        "            ELU(),\n",
        "            Linear(int(dim_h/8), dim_h)\n",
        "        )\n",
        "\n",
        "        # GIN layers\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [GINConv(Sequential(\n",
        "                Linear(dim_h, dim_h),\n",
        "                ReLU(),\n",
        "                Linear(dim_h, dim_h))) for _ in range(depth)])\n",
        "            \n",
        "        # normalization layers\n",
        "        self.norm = torch.nn.ModuleList([GraphNorm(dim_h) for _ in range(self.depth)])\n",
        "        \n",
        "        # layer-wise attention poolings\n",
        "        self.att_poolings = torch.nn.ModuleList(\n",
        "            [\n",
        "                AttentionalAggregation(\n",
        "                Sequential(Linear(self.dim_h, 2*self.dim_h), \n",
        "                           ReLU(), \n",
        "                           Linear(2*self.dim_h, 1))) \n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def contrastive_loss(self, g1_x, g2_x):\n",
        "        # compute projections + L2 row-wise normalizations\n",
        "        g1_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g1_x), p=2, dim=1\n",
        "        )\n",
        "        g2_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g2_x), p=2, dim=1\n",
        "        )\n",
        "        \n",
        "        g1_proj_T = torch.transpose(g1_projections, 0, 1)\n",
        "        g2_proj_T = torch.transpose(g2_projections, 0, 1)\n",
        "\n",
        "        inter_g1 = torch.exp(torch.matmul(g1_projections, g1_proj_T))\n",
        "        inter_g2 = torch.exp(torch.matmul(g2_projections, g2_proj_T))\n",
        "        intra_view = torch.exp(torch.matmul(g1_projections, g2_proj_T))\n",
        "\n",
        "        # main diagonal\n",
        "        corresponding_terms = torch.diagonal(intra_view, 0) \n",
        "        non_matching_intra = torch.diagonal(intra_view, -1).sum()\n",
        "        non_matching_inter_g1 = torch.diagonal(inter_g1, -1).sum()\n",
        "        non_matching_inter_g2 = torch.diagonal(inter_g2, -1).sum()\n",
        "\n",
        "        # inter-view pairs using g1\n",
        "        corresponding_terms_g1 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g1 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g1 = torch.log(corresponding_terms_g1)\n",
        "\n",
        "        # inter-view pairs using g2\n",
        "        corresponding_terms_g2 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g2 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g2 = torch.log(corresponding_terms_g2)\n",
        "\n",
        "        # contrasting terms of both divided by total nodes\n",
        "        loss = (\n",
        "            corresponding_terms_g1.sum() + \n",
        "            corresponding_terms_g2.sum()\n",
        "        ) / (\n",
        "            g1_x.shape[0] + \n",
        "            g2_x.shape[0]\n",
        "        )\n",
        "        \n",
        "        loss = loss / self.batch_size\n",
        "        return loss\n",
        "    \n",
        "    def layer_loop(self, x, edge_index, batch, cl=False, cl_all=False, dgi_task=False):\n",
        "        cl_embs = []\n",
        "        for d in range(self.depth):\n",
        "            x_in = x\n",
        "\n",
        "            # get node representation at layer d\n",
        "            x = self.layers[d](x, edge_index)\n",
        "            \n",
        "            if self.norm:\n",
        "                x = self.norm[d](x, batch)\n",
        "            \n",
        "            if d < self.depth - 1:\n",
        "                x = F.relu(x)\n",
        "            \n",
        "            if self.dropout:\n",
        "                x = F.dropout(x)\n",
        "            \n",
        "            if self.residual:\n",
        "                x = x + x_in\n",
        "\n",
        "            if not cl:\n",
        "                # use attention pooling for given depth\n",
        "                h_g = self.att_poolings[d](x, batch)\n",
        "                self.graph_embs.append(h_g)\n",
        "\n",
        "            if (\n",
        "                (cl and cl_all) or \n",
        "                (cl and (d == self.depth-1)) or \n",
        "                (dgi_task and (d == self.depth-1))\n",
        "            ):\n",
        "                # if using contrastive learning or DGI\n",
        "                cl_embs += [x]\n",
        "            \n",
        "        return cl_embs\n",
        "\n",
        "    def forward(self, batched_data, cl=False, cl_all=False, dgi_task=False):\n",
        "        self.graph_embs = []\n",
        "        # non-augmented graph\n",
        "        # note: populates self.graph_embs\n",
        "\n",
        "        node_depth = batched_data.node_depth\n",
        "        x_emb = self.node_encoder(batched_data.x, node_depth.view(-1,))\n",
        "        edge_index = batched_data.edge_index\n",
        "        batch = batched_data.batch\n",
        "\n",
        "        self.layer_loop(x_emb, edge_index, batch, dgi_task=dgi_task)\n",
        "\n",
        "        agg = self.aggregate()\n",
        "        self.graph_embs.append(agg)\n",
        "        output = torch.stack(self.graph_embs, dim=0)\n",
        "\n",
        "        # dgi task\n",
        "        dgi_loss = 0\n",
        "        if dgi_task:\n",
        "            # batch size // 5 to perform additional objectives\n",
        "            # only on 1/5th of the batch, for speed reasons\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                nd = g.node_depth\n",
        "                b = g.batch\n",
        "                \n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=True)\n",
        "                g_diff_embs = self.layer_loop(g1, g2, b, dgi_task=True)[0]\n",
        "\n",
        "                g.x = self.node_encoder(g.x, nd.view(-1,).clone())\n",
        "                g_embs = self.layer_loop(g.x, g.edge_index, g.batch, dgi_task=True)[0]\n",
        "\n",
        "                # dgi objective on final_layer_embs, g_diff_embs, and output\n",
        "                agg = agg.clone()\n",
        "                positive = self.discriminator(g_embs, agg[i])\n",
        "                ones = torch.ones_like(positive)\n",
        "                negative = self.discriminator(g_diff_embs, agg[i])\n",
        "                zeros = torch.zeros_like(negative)\n",
        "\n",
        "                dgi_loss += (\n",
        "                    self.loss_fn(positive, ones) + self.loss_fn(negative, zeros)\n",
        "                ) / (positive.shape[0] + negative.shape[0])\n",
        "            \n",
        "            dgi_loss /= (self.batch_size // 5)\n",
        "\n",
        "        # contrastive learning task\n",
        "        cl_loss = 0\n",
        "        if cl:\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=False)\n",
        "                g1_embs = self.get_node_embedding(g.batch, g1, cl=True, cl_all=cl_all)\n",
        "                g2_embs = self.get_node_embedding(g.batch, g2, cl=True, cl_all=cl_all)\n",
        "\n",
        "                batch_cl_loss = 0\n",
        "                for j in range(len(g1_embs)):\n",
        "                    batch_cl_loss += self.contrastive_loss(g1_embs[j], g2_embs[j])\n",
        "                \n",
        "                batch_cl_loss = batch_cl_loss / len(g1_embs)\n",
        "                cl_loss += batch_cl_loss\n",
        "            \n",
        "            cl_loss /= (self.batch_size // 5)\n",
        "\n",
        "        return output, cl_loss, dgi_loss\n",
        "\n",
        "    def get_node_embedding(self, batch, g, cl, cl_all):\n",
        "        g_x, g_edge_index = g\n",
        "        return self.layer_loop(\n",
        "            g_x.clone(), \n",
        "            g_edge_index, \n",
        "            batch, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all\n",
        "        )\n",
        "\n",
        "    def get_contrastive_pair_from_batch(self, g, dgi_task=False):\n",
        "        g_clone = g.clone()\n",
        "        nd = g.node_depth\n",
        "        # encode the nodes in the clone of g using given encoding network\n",
        "        g_clone.x = self.node_encoder(g_clone.x, nd.view(-1,).clone())\n",
        "\n",
        "        # create contrastive pairs from the input graph\n",
        "        return get_contrastive_graph_pair(g_clone, dgi_task=dgi_task)\n",
        "\n",
        "    def aggregate(self):\n",
        "        pass\n",
        "\n",
        "class MLAP_Sum(MLAP_GIN):\n",
        "    def aggregate(self):\n",
        "        return torch.stack(self.graph_embs, dim=0).sum(dim=0)\n",
        "\n",
        "class MLAP_Weighted(MLAP_GIN):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weight = torch.nn.Parameter(torch.ones(self.depth, 1, 1))\n",
        "\n",
        "    def aggregate(self):\n",
        "        a = F.softmax(self.weight, dim=0)\n",
        "        h = torch.stack(self.graph_embs, dim=0)\n",
        "        return (a * h).sum(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fnFG6_RV714"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7ft4PTPWV8eO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, batch_size, depth, dim_h, max_seq_len, node_encoder, vocab2idx, device):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "        self.dim_h = dim_h\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # token to idx lookup\n",
        "        self.vocab2idx = vocab2idx \n",
        "        \n",
        "        # architecture choices\n",
        "        self.node_encoder = node_encoder\n",
        "        self.gnn = MLAP_Weighted(\n",
        "            dim_h, batch_size, depth, \n",
        "            node_encoder, \n",
        "            norm=True, \n",
        "            residual=True, \n",
        "            dropout=True\n",
        "        )\n",
        "        self.decoder = LinearDecoder(\n",
        "            dim_h, max_seq_len, vocab2idx, device\n",
        "        )\n",
        "\n",
        "    def forward(self, batched_data, labels, training=False, cl=False, cl_all=False, dgi_task=False):\n",
        "        # GNN layer, contrastive work done here\n",
        "        embeddings, cl_loss, dgi_loss = self.gnn(\n",
        "            batched_data, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task\n",
        "        )\n",
        "\n",
        "        predictions = self.decoder(len(labels), embeddings, labels, training=training)\n",
        "\n",
        "        # for each batch, the prediction for the ith word is a logit\n",
        "        # decoding each prediction to a word is done in the evaluation task in main\n",
        "        return predictions, cl_loss, dgi_loss, embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2nPyD6v3DEn"
      },
      "source": [
        "### MAD and MADGap\n",
        "\n",
        "[Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View (2019) - Chen, Lin, Li, Li, Zhou, Sun](https://arxiv.org/abs/1909.03211)\n",
        "\n",
        ">We can observe that as the number of GNN layers increases, the MAD values become smaller. Apart from this, the MAD value of high-layer GNNs gets close to 0, which means that all the node representations become indistinguishable. GNN models update the node representation based on the features from neighboring nodes. We observe that the interaction between nodes makes their representations similar to each other. Similar phenomenons that the smoothness rises as the layer increases are also observed in other datasets as presented in Appendix B. Therefore, we conclude that smoothing is an essential nature for GNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oNHMj22e2_jS"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import k_hop_subgraph\n",
        "import numpy as np\n",
        "\n",
        "def get_d(H):\n",
        "  \"\"\"\n",
        "  Pairwise Cosine Distance between nodes\n",
        "  \"\"\"\n",
        "  # asssuming row dimension are nodes\n",
        "  # column dim are hidden dimension\n",
        "  normed = torch.nn.functional.normalize(H, dim=1)\n",
        "  return (1 - normed @ normed.T)\n",
        "\n",
        "def get_mad(H, mask):\n",
        "  D = get_d(H)\n",
        "  n = torch.count_nonzero(D[mask])\n",
        "  return (D[mask].sum() / n).item()\n",
        "\n",
        "def get_mad_global(H):\n",
        "  # MAD: mean average distance\n",
        "  D = get_d(H)\n",
        "  return D.mean().item()\n",
        "\n",
        "def get_all_node_ids(num_nodes):\n",
        "  return torch.linspace(0, num_nodes - 1, steps=num_nodes, dtype=int)\n",
        "\n",
        "def get_mad_gap(node_id, embedding, data):\n",
        "  # MADgap\n",
        "  # find nodes that are 3 or fewer edges away for MAD_neb\n",
        "  neb, _, _, _ = k_hop_subgraph(node_id, 3, data.edge_index)\n",
        "  mad_neb = get_mad(embedding, neb)\n",
        "  \n",
        "  # find nodes that are 8 or more edges away for MAD_rmt\n",
        "  subset, _, _, _ = k_hop_subgraph(node_id, 7, data.edge_index)\n",
        "  \n",
        "  # get compliment of nodes within 7 steps of node_id\n",
        "  num_nodes = data.x.shape[0]\n",
        "  all_node_ids = get_all_node_ids(num_nodes)\n",
        "  mask = torch.ones_like(all_node_ids, dtype=bool)\n",
        "  mask[subset] = 0\n",
        "  rmt = all_node_ids[mask]\n",
        "\n",
        "  mad_rmt = get_mad(embedding, rmt)\n",
        "\n",
        "  return (mad_rmt - mad_neb)\n",
        "\n",
        "def randomly_sample_node_ids(num_samples, data):\n",
        "    num_nodes = data.x.shape[0]\n",
        "    all_node_ids = get_all_node_ids(num_nodes)\n",
        "    bool_mask = np.zeros_like(all_node_ids, dtype=bool)\n",
        "    bool_mask[:num_samples] = True\n",
        "    np.random.shuffle(bool_mask)\n",
        "    return all_node_ids[bool_mask]\n",
        "\n",
        "def get_sampled_mad_gap(num_samples, data, embedding):\n",
        "  \"\"\"\n",
        "  According to our assumption, large MADGap value indicates that the \n",
        "  useful information received by the node is more than noise. ...\n",
        "  On the contrary, small or negative MADGap means over-smoothing and \n",
        "  inferior performance. - Chen, Lin, Li, et. al (2019) pp. 4\n",
        "  \"\"\"\n",
        "  random_sample = randomly_sample_node_ids(num_samples, data)\n",
        "  mad_gaps = []\n",
        "  for node_id in random_sample:\n",
        "    mad_gaps.append(get_mad_gap(node_id.item(), embedding, data))\n",
        "  return np.array(mad_gaps).mean()\n",
        "\n",
        "def get_global_mad_gap(data, embedding, perc_to_sample=1.0):\n",
        "  \"\"\"\n",
        "  Makes more sense for larger graphs. In the AST task, graphs are relatively\n",
        "  small... but are also tree-like. \n",
        "  \"\"\"\n",
        "  num_samples = int(data.x.shape[0] * perc_to_sample)\n",
        "  return get_sampled_mad_gap(num_samples, data, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1iErB7DcYCXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2ed4e5-3956-4eba-dd72-dc5c00287be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ]
        }
      ],
      "source": [
        "!cd $nb_path_bash && mkdir \"checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwGxp_PRWAMt"
      },
      "source": [
        "### Main\n",
        "\n",
        "Model configuration and training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i_qjQrjlCqTA"
      },
      "outputs": [],
      "source": [
        "def get_training_tag(alpha, cl, cl_all, dgi_task, depth):\n",
        "  return f\"alpha__{alpha}_cl__{cl}_cl_all__{cl_all}_dgi_task__{dgi_task}_depth__{depth}\"\n",
        "\n",
        "def randomly_mask(dataset, size):\n",
        "  bool_mask = np.zeros(len(dataset), dtype=bool)\n",
        "  bool_mask[:size] = True\n",
        "  np.random.shuffle(bool_mask)\n",
        "  out = dataset[bool_mask]\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mh7dpE0hWAx3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "\n",
        "\n",
        "def train(\n",
        "      model, \n",
        "      device, \n",
        "      loader, \n",
        "      optimizer, \n",
        "      scheduler, \n",
        "      multicls_criterion, \n",
        "      epoch, \n",
        "      alpha=0.05, \n",
        "      cl=False, \n",
        "      cl_all=False, \n",
        "      dgi_task=False,\n",
        "      eval_hook=lambda x: x,\n",
        "      save_checkpoint_every_n_step=35,\n",
        "      csv_f_name=\"\",\n",
        "    ):\n",
        "    if cl and dgi_task:\n",
        "        # check settings\n",
        "        raise Exception(\"Cannot use both a contrastive and dgi loss term\\n\")\n",
        "\n",
        "    # setup recording and checkpoints\n",
        "    tag = get_training_tag(alpha, cl, cl_all, dgi_task, model.depth)\n",
        "    chkpt_folder = nb_path + f'/checkpoints/{tag}__epoch_{epoch}'\n",
        "    if not os.path.exists(chkpt_folder):\n",
        "        os.mkdir(chkpt_folder)\n",
        "\n",
        "    # total loss for this epoch\n",
        "    loss_accum = 0\n",
        "    cl_loss = 0\n",
        "    loss = 0\n",
        "    total_steps = len(loader)\n",
        "    for step, batch in enumerate(loader):\n",
        "        avg_loss = loss_accum / (step + 1)\n",
        "\n",
        "        # --- train the model ---\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            # train\n",
        "            labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "            pred_list, cl_loss, dgi_loss, embeddings = model(\n",
        "                batch, labels, training=True,\n",
        "                cl=cl, \n",
        "                cl_all=cl_all, \n",
        "                dgi_task=dgi_task\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss + update\n",
        "            loss = 0\n",
        "            for i in range(len(pred_list)):\n",
        "                loss += (1-alpha) * multicls_criterion(\n",
        "                    pred_list[i].to(torch.float32), \n",
        "                    batch.y_arr[:, i].detach()\n",
        "                )\n",
        "\n",
        "            loss /= len(pred_list)\n",
        "            if cl:\n",
        "                loss -= alpha * cl_loss\n",
        "            if dgi_task:\n",
        "                loss -= alpha * dgi_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # report loss after nth batch\n",
        "            loss_no_grad = loss.detach().item()\n",
        "            cl_loss_no_grad = cl_loss.detach().item()\n",
        "            loss_accum += loss_no_grad\n",
        "            print(\n",
        "                f'\\rStep: {step}/{total_steps}. Average loss after batch {step}: {avg_loss:.4f}. Contrastive Term: {cl_loss_no_grad:.3f}.', \n",
        "                end=''\n",
        "            )\n",
        "\n",
        "            # try to de-allocate predictions\n",
        "            for p in pred_list:\n",
        "              p.detach()\n",
        "        \n",
        "        # run eval if requested\n",
        "        with torch.no_grad():\n",
        "          e = eval_hook(step)\n",
        "          if e:\n",
        "            # unpack the metrics if evaluation ran\n",
        "            (\n",
        "              train_perf, \n",
        "              valid_perf, \n",
        "              test_perf, \n",
        "              mad_global_train, \n",
        "              mad_global_valid, \n",
        "              mad_global_test\n",
        "            ) = e\n",
        "\n",
        "            # append it to the csv record for this model\n",
        "            with open(csv_f_name, 'a') as record:\n",
        "              writer = csv.writer(record)\n",
        "              writer.writerow(\n",
        "                  [\n",
        "                      alpha,\n",
        "                      cl,\n",
        "                      cl_all,\n",
        "                      dgi_task,\n",
        "                      epoch, \n",
        "                      step, \n",
        "                      loss_no_grad, \n",
        "                      cl_loss_no_grad, \n",
        "                      train_perf[dataset.eval_metric], \n",
        "                      valid_perf[dataset.eval_metric],\n",
        "                      test_perf[dataset.eval_metric],\n",
        "                      mad_global_train.mean(),\n",
        "                      mad_global_valid.mean(),\n",
        "                      mad_global_test.mean()\n",
        "                  ]\n",
        "              )\n",
        "        \n",
        "        # --- checkpoints ---\n",
        "        if (\n",
        "            (step+1) % save_checkpoint_every_n_step == 0 or \n",
        "            step == len(loader)-1\n",
        "          ): \n",
        "            # save model after every n batches\n",
        "            \n",
        "            step_name = str((step+1) // save_checkpoint_every_n_step)\n",
        "            checkpoint_f_name = chkpt_folder + '/model' + step_name + '.pt'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_f_name)\n",
        "            print(f\"\\nCheckpoint saved here: {checkpoint_f_name}\")\n",
        "\n",
        "            with open(f\"{chkpt_folder}/model{step_name}_metrics.txt\", \"w\") as f:\n",
        "              # just for reference save associated metrics\n",
        "              f.write(f\"Avg Loss: {avg_loss:.15f}\\n\")\n",
        "              f.write(f\"Contrastive Term: {cl_loss_no_grad:.15f}\")\n",
        "\n",
        "    # end of this epoch\n",
        "    print(f'Average training loss: {avg_loss}')\n",
        "    return avg_loss, checkpoint_f_name\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, device, loader, evaluator, arr_to_seq):\n",
        "    \"\"\"\n",
        "    Use official OGB evaluator to test results of model output\n",
        "    \"\"\"\n",
        "    seq_ref_list = []\n",
        "    seq_pred_list = []\n",
        "    mad_values = []\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "            # no cl by default\n",
        "            pred_list, _, _, embeddings = model(batch, labels) \n",
        "\n",
        "            mat = []\n",
        "            for i in range(len(pred_list)):\n",
        "                # get model's predictions\n",
        "                mat.append(torch.argmax(pred_list[i], dim=1).view(-1, 1))\n",
        "            \n",
        "            # embeddings are stacked batchwise along the \n",
        "            # row dimension. Take the embeddings of the final\n",
        "            # batch, or take the mean of all embeddings along the 0th dim\n",
        "            mad_global = get_mad_global(embeddings[-1])\n",
        "            \n",
        "            # save for eval\n",
        "            seq_ref_list.extend(labels)\n",
        "            mat = torch.cat(mat, dim=1)\n",
        "            seq_pred = [arr_to_seq(arr) for arr in mat]\n",
        "            seq_pred_list.extend(seq_pred)\n",
        "            mad_values.append(mad_global)\n",
        "\n",
        "    input_dict = {\"seq_ref\": seq_ref_list, \"seq_pred\": seq_pred_list}\n",
        "    return evaluator.eval(input_dict), np.array(mad_values)\n",
        "\n",
        "def main(\n",
        "      starting_chkpt=None, \n",
        "      cl=False, \n",
        "      cl_all=False, \n",
        "      dgi_task=False, \n",
        "      run_eval_every_n_batches=None, \n",
        "      # CL hyperparameter\n",
        "      alpha=0.05,\n",
        "      num_epochs=50,\n",
        "      save_checkpoint_every_n_step=35\n",
        "  ):\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # model & training conf\n",
        "    depth = 3\n",
        "    epochs = num_epochs\n",
        "    learning_rate = 0.001\n",
        "    step_size = 10\n",
        "    decay_rate = 0.1\n",
        "    weight_decay = 0.00005\n",
        "    dim_h = 512\n",
        "\n",
        "    # model initialization\n",
        "    node_encoder = ASTNodeEncoder(\n",
        "        dim_h, \n",
        "        num_nodetypes=len(nodetypes_mapping['type']), \n",
        "        num_nodeattributes=len(nodeattributes_mapping['attr']), \n",
        "        max_depth=20\n",
        "    )\n",
        "    model = Model(\n",
        "        batch_size, \n",
        "        depth, \n",
        "        dim_h, \n",
        "        max_seq_len, \n",
        "        node_encoder, \n",
        "        vocab2idx, \n",
        "        DEVICE\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model # Params: {num_params}')\n",
        "    print(\"-------------\\n\\n\\n\")\n",
        "\n",
        "    # training configuration\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)\n",
        "    multicls_criterion = torch.nn.CrossEntropyLoss()\n",
        "    starting_epoch = 1\n",
        "\n",
        "    if starting_chkpt:\n",
        "        checkpoint = torch.load(starting_chkpt)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        starting_epoch = checkpoint['epoch']\n",
        "        print(f\"Successfully loaded checkpoint: {starting_chkpt}\")\n",
        "        print(f\"Starting at epoch: {starting_epoch}\")\n",
        "\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_curve = []\n",
        "    trainL_curve = []\n",
        "    train_mad_curve = []\n",
        "    test_mad_curve = []\n",
        "    valid_mad_curve = []\n",
        "\n",
        "    # create a csv for recording\n",
        "    tag = get_training_tag(alpha, cl, cl_all, dgi_task, model.depth)\n",
        "    csv_f_name = f'{nb_path}/checkpoints/{tag}.csv'\n",
        "    with open(csv_f_name, 'w') as record:\n",
        "      writer = csv.writer(record)\n",
        "      writer.writerow(\n",
        "          [\n",
        "              \"Alpha\",\n",
        "              \"CL\",\n",
        "              \"CL All\",\n",
        "              \"DGI\",\n",
        "              \"Epoch\", \n",
        "              \"Batch\", \n",
        "              \"Loss\", \n",
        "              \"Contrastive Term\", \n",
        "              \"Train Performance\", \n",
        "              \"Validation Performance\",\n",
        "              \"Test Performance\",\n",
        "              \"Train MAD Global Average\",\n",
        "              \"Validation MAD Global Average\",\n",
        "              \"Test MAD Global Average\"\n",
        "          ]\n",
        "      )\n",
        "\n",
        "    def eval_hook():\n",
        "      print('\\n\\nEvaluating...')\n",
        "      train_perf, mad_global_train = eval(\n",
        "          model, DEVICE, train_loader, evaluator, \n",
        "          arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab)\n",
        "      )\n",
        "      valid_perf, mad_global_valid = eval(\n",
        "          model, DEVICE, valid_loader, evaluator, \n",
        "          arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab)\n",
        "      )\n",
        "      test_perf, mad_global_test = eval(\n",
        "          model, DEVICE, test_loader, evaluator, \n",
        "          arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab)\n",
        "      )\n",
        "\n",
        "      print(\"Eval Criteria:\")\n",
        "      print(\n",
        "          '\\tTrain:', train_perf[dataset.eval_metric],\n",
        "          'Validation:', valid_perf[dataset.eval_metric],\n",
        "          'Test:', test_perf[dataset.eval_metric]\n",
        "      )\n",
        "      print(\"MAD global batch averages\")\n",
        "      print(\n",
        "          '\\tTrain:', mad_global_train.mean(),\n",
        "          'Validation:', mad_global_valid.mean(),\n",
        "          'Test:', mad_global_test.mean()\n",
        "      )\n",
        "      \n",
        "      return (\n",
        "          train_perf, \n",
        "          valid_perf, \n",
        "          test_perf, \n",
        "          mad_global_train, \n",
        "          mad_global_valid, \n",
        "          mad_global_test\n",
        "      )\n",
        "\n",
        "\n",
        "    for epoch in range(starting_epoch, epochs + 1):\n",
        "        print(f\"Training epoch: {epoch}/{epochs+1}\")\n",
        "        \n",
        "        # training model\n",
        "        train_loss, latest_checkpoint_path = train(\n",
        "            model, \n",
        "            DEVICE, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            multicls_criterion, \n",
        "            epoch, \n",
        "            cl=cl, \n",
        "            alpha=alpha,\n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task,\n",
        "            # run evaluation every n batches\n",
        "            eval_hook=lambda x: (\n",
        "                eval_hook() \n",
        "                  if run_eval_every_n_batches is not None and \n",
        "                  (x != 0 and x % run_eval_every_n_batches == 0) \n",
        "                else None\n",
        "            ),\n",
        "            save_checkpoint_every_n_step=save_checkpoint_every_n_step,\n",
        "            csv_f_name=csv_f_name\n",
        "        )\n",
        "        scheduler.step()\n",
        "\n",
        "        # run evaluation after each epoch\n",
        "        (\n",
        "            train_perf, \n",
        "            valid_perf, \n",
        "            test_perf, \n",
        "            mad_global_train, \n",
        "            mad_global_valid, \n",
        "            mad_global_test\n",
        "        ) = eval_hook()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss}\")\n",
        "\n",
        "        train_curve.append(train_perf[dataset.eval_metric])\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "        trainL_curve.append(train_loss)\n",
        "        train_mad_curve.append(mad_global_train.mean())\n",
        "        test_mad_curve.append(mad_global_valid.mean())\n",
        "        valid_mad_curve.append(mad_global_test.mean())\n",
        "\n",
        "    print('\\n\\n\\nFinished training!')\n",
        "    print('F1')\n",
        "    best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    best_train = max(train_curve)\n",
        "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
        "    print('Finished test: {}, Validation: {}, Train: {}, epoch: {}, best train: {}, best loss: {}'\n",
        "          .format(\n",
        "              test_curve[best_val_epoch], \n",
        "              valid_curve[best_val_epoch], \n",
        "              train_curve[best_val_epoch],\n",
        "              best_val_epoch, \n",
        "              best_train, \n",
        "              min(trainL_curve)\n",
        "          )\n",
        "    )\n",
        "\n",
        "    # return everything about this training\n",
        "    return (\n",
        "        csv_f_name, \n",
        "        latest_checkpoint_path,\n",
        "        train_curve,\n",
        "        valid_curve,\n",
        "        test_curve,\n",
        "        trainL_curve,\n",
        "        train_mad_curve,\n",
        "        test_mad_curve,\n",
        "        valid_mad_curve\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmenm81UkSeJ",
        "outputId": "b047c3d3-2929-4ad5-fbb1-12bc3f702550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/code2.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.91 GB: 100%|██████████| 934/934 [01:35<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/code2.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 452741/452741 [00:01<00:00, 363917.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 452741/452741 [00:20<00:00, 22392.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage of top 5000 vocabulary:\n",
            "0.9025832389087423\n"
          ]
        }
      ],
      "source": [
        "num_vocab = 5000\n",
        "max_seq_len = 5\n",
        "\n",
        "# original MLAP paper was 50\n",
        "batch_size = 50\n",
        "\n",
        "# dataset objects\n",
        "# best to load these only once in colab\n",
        "# otherwise, memory never freed and runtime crashes\n",
        "dataset_name = \"ogbg-code2\"\n",
        "dataset = PygGraphPropPredDataset(dataset_name)\n",
        "evaluator = Evaluator(dataset_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "vocab2idx, idx2vocab = get_vocab_mapping([dataset.data.y[i] for i in split_idx['train']], num_vocab)\n",
        "dataset.transform = transforms.Compose([augment_edge, lambda data: encode_y_to_arr(data, vocab2idx, max_seq_len)])\n",
        "\n",
        "nodetypes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'typeidx2type.csv.gz'))\n",
        "nodeattributes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'attridx2attr.csv.gz'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EoDIrwXl9UUY"
      },
      "outputs": [],
      "source": [
        "testing_limit = None # change to None if no limit\n",
        "\n",
        "full_training = dataset[split_idx[\"train\"]][:testing_limit]\n",
        "full_valid = dataset[split_idx[\"valid\"]][:testing_limit]\n",
        "full_test = dataset[split_idx[\"test\"]][:testing_limit]\n",
        "\n",
        "train_loader = DataLoader(full_training, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(full_valid, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(full_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jba_Lx_uZ-Dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def generate_result_presentation(csv_f_name):\n",
        "  # load the csv\n",
        "  train_details = pd.read_csv(csv_f_name)\n",
        "\n",
        "  # placeholder title for now\n",
        "  plt.figure(figsize=(40,10))\n",
        "\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.title(csv_f_name.split(\"/\")[-1].split(\".csv\")[0])\n",
        "  plt.xlabel('Recording (every n batches)')\n",
        "  plt.xticks(range(1, train_details.shape[0]))\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.plot(train_details['Train Performance'], label = 'Train Performance')\n",
        "  plt.plot(train_details['Test Performance'], label = 'Test Performance')\n",
        "  plt.plot(train_details['Validation Performance'], label = 'Validation Performance')\n",
        "  plt.legend()\n",
        "  plt.plot()\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.title(csv_f_name.split(\"/\")[-1].split(\".csv\")[0])\n",
        "  plt.xlabel('Recording (every n batches)')\n",
        "  plt.xticks(range(1, train_details.shape[0]))\n",
        "  plt.ylabel('Batchwise MAD')\n",
        "  plt.plot(train_details['Train MAD Global Average'], label = 'Train MAD Global Average')\n",
        "  plt.plot(train_details['Test MAD Global Average'], label = 'Test MAD Global Average')\n",
        "  plt.plot(train_details['Validation MAD Global Average'], label = 'Validation MAD Global Average')\n",
        "  plt.legend()\n",
        "  plt.plot()\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.title(csv_f_name.split(\"/\")[-1].split(\".csv\")[0])\n",
        "  plt.xlabel('Recording (every n batches)')\n",
        "  plt.xticks(range(1, train_details.shape[0]))\n",
        "  plt.ylabel('Cross Entropy Loss')\n",
        "  plt.plot(train_details['Loss'], label = 'Train Loss')\n",
        "  plt.plot(train_details['Contrastive Term'], label = 'Train Loss Contrastive Term')\n",
        "  plt.legend()\n",
        "  plt.plot()\n",
        "\n",
        "  # print the table\n",
        "  return train_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lf44lVYfY2KN"
      },
      "outputs": [],
      "source": [
        "# google drive folder: https://drive.google.com/drive/u/1/folders/1--9W3ejvp-3cASkWlPJs3CcZXMjYs1kj\n",
        "\n",
        "# all_epochs = 50\n",
        "# model_configs = [\n",
        "#   # No CL\n",
        "#   # {\"alpha\": 0.05, \"cl\": False, \"cl_all\": False, \"dgi_task\": False, \"num_epochs\": all_epochs},\n",
        "#   # CL 1 layer, ablating alpha\n",
        "#   # {\"alpha\": 0.05, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"num_epochs\": all_epochs},\n",
        "#   # {\"alpha\": 0.10, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "#   # {\"alpha\": 0.15, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "#   # {\"alpha\": 0.20, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "#   # TODO: DGI\n",
        "# ]\n",
        "\n",
        "all_epochs = 10\n",
        "model_configs = [\n",
        "  # No CL\n",
        "  #{\"alpha\": 0.0, \"cl\": False, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  # CL 1 layer, ablating alpha\n",
        "  {\"alpha\": 0.05, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": None, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 500},\n",
        "  #{\"alpha\": 0.10, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  #{\"alpha\": 0.15, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  #{\"alpha\": 0.20, \"cl\": True, \"cl_all\": False, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  # CL all layers, ablating alpha\n",
        "  #{\"alpha\": 0.05, \"cl\": False, \"cl_all\": True, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  #{\"alpha\": 0.10, \"cl\": False, \"cl_all\": True, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  #{\"alpha\": 0.15, \"cl\": False, \"cl_all\": True, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "  #{\"alpha\": 0.20, \"cl\": False, \"cl_all\": True, \"dgi_task\": False, \"run_eval_every_n_batches\": 50, \"num_epochs\": all_epochs, \"save_checkpoint_every_n_step\": 300},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Npm9s8GOQ3iA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4be3b6-979d-4f59-b935-e9f85859ee4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1',\n",
              " 'alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# TODO: next time you run load from the checkpoint\n",
        "import os\n",
        "\n",
        "os.listdir(nb_path + \"/checkpoints\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1-OwAGxD1v8q",
        "outputId": "08c77525-ddb0-4b74-8180-88f11757f045"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "4YKn7YRZLMms"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.memory_stats()"
      ],
      "metadata": {
        "id": "eySFejnzLV0Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #print(torch.cuda.memory_summary())\n",
        " torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "m13WtXIVLkYA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig1WfVU5Z0XL",
        "outputId": "46c681a5-6d15-4aa6-891c-8a2b5e6067d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alpha': 0.05, 'cl': True, 'cl_all': False, 'dgi_task': False, 'run_eval_every_n_batches': None, 'num_epochs': 10, 'save_checkpoint_every_n_step': 500}\n",
            "Model # Params: 23612920\n",
            "-------------\n",
            "\n",
            "\n",
            "\n",
            "Successfully loaded checkpoint: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model1.pt\n",
            "Starting at epoch: 1\n",
            "Training epoch: 1/11\n",
            "Step: 499/8160. Average loss after batch 499: 2.4760. Contrastive Term: -0.079.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model1.pt\n",
            "Step: 999/8160. Average loss after batch 999: 2.4732. Contrastive Term: -0.087.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model2.pt\n",
            "Step: 1499/8160. Average loss after batch 1499: 2.4691. Contrastive Term: -0.078.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model3.pt\n",
            "Step: 1999/8160. Average loss after batch 1999: 2.4636. Contrastive Term: -0.081.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model4.pt\n",
            "Step: 2499/8160. Average loss after batch 2499: 2.4639. Contrastive Term: -0.081.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model5.pt\n",
            "Step: 2999/8160. Average loss after batch 2999: 2.4590. Contrastive Term: -0.082.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model6.pt\n",
            "Step: 3499/8160. Average loss after batch 3499: 2.4550. Contrastive Term: -0.082.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model7.pt\n",
            "Step: 3999/8160. Average loss after batch 3999: 2.4514. Contrastive Term: -0.080.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model8.pt\n",
            "Step: 4499/8160. Average loss after batch 4499: 2.4471. Contrastive Term: -0.090.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model9.pt\n",
            "Step: 4999/8160. Average loss after batch 4999: 2.4404. Contrastive Term: -0.080.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model10.pt\n",
            "Step: 5499/8160. Average loss after batch 5499: 2.4365. Contrastive Term: -0.081.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model11.pt\n",
            "Step: 5999/8160. Average loss after batch 5999: 2.4327. Contrastive Term: -0.080.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model12.pt\n",
            "Step: 6499/8160. Average loss after batch 6499: 2.4275. Contrastive Term: -0.094.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model13.pt\n",
            "Step: 6999/8160. Average loss after batch 6999: 2.4246. Contrastive Term: -0.084.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model14.pt\n",
            "Step: 7499/8160. Average loss after batch 7499: 2.4204. Contrastive Term: -0.080.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model15.pt\n",
            "Step: 7999/8160. Average loss after batch 7999: 2.4162. Contrastive Term: -0.076.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model16.pt\n",
            "Step: 8159/8160. Average loss after batch 8159: 2.4150. Contrastive Term: -0.085.\n",
            "Checkpoint saved here: drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model16.pt\n",
            "Average training loss: 2.414962229570922\n",
            "\n",
            "\n",
            "Evaluating...\n",
            "Eval Criteria:\n",
            "\tTrain: 0.1676620942315358 Validation: 0.12245701905385475 Test: 0.1330696246223857\n",
            "MAD global batch averages\n",
            "\tTrain: 0.8515288997806755 Validation: 0.7349498917224371 Test: 0.7461366150400904\n",
            "Train Loss: 2.414962229570922\n",
            "Training epoch: 2/11\n",
            "Step: 136/8160. Average loss after batch 136: 2.1896. Contrastive Term: -0.077."
          ]
        }
      ],
      "source": [
        "# when training on the full dataset, each epoch has 2039 - 8159 elements\n",
        "latest_checkpoint_path = \"drive/MyDrive/Colab Notebooks/venv_MLAP_test_eval_final_full_model/checkpoints/alpha__0.05_cl__True_cl_all__False_dgi_task__False_depth__3__epoch_1/model1.pt\"\n",
        "\n",
        "for conf in model_configs:\n",
        "  print(conf)\n",
        "  (\n",
        "    csv_f_name, \n",
        "    latest_checkpoint_path,\n",
        "    train_curve,\n",
        "    valid_curve,\n",
        "    test_curve,\n",
        "    trainL_curve,\n",
        "    train_mad_curve,\n",
        "    test_mad_curve,\n",
        "    valid_mad_curve\n",
        "  ) = main(starting_chkpt=latest_checkpoint_path, **conf)\n",
        "\n",
        "  \n",
        "  train_df = generate_result_presentation(csv_f_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prints currently alive Tensors and Variables\n",
        "import torch\n",
        "import gc\n",
        "for obj in gc.get_objects():\n",
        "    try:\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "            print(type(obj), obj.size())\n",
        "            if isinstance(torch.nn.parameter.Parameter, obj):\n",
        "              print(\"parmeter requries gradient:\")\n",
        "              print(obj.requires_grad())\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "s6EUtoqdyaAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee40316f-e01a-414a-89a4-dfd45ae33c19"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> torch.Size([19])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([452742])\n",
            "<class 'torch.Tensor'> torch.Size([22817])\n",
            "<class 'torch.Tensor'> torch.Size([21948])\n",
            "<class 'torch.Tensor'> torch.Size([407976])\n",
            "<class 'torch.Tensor'> torch.Size([2, 56235837])\n",
            "<class 'torch.Tensor'> torch.Size([56688578, 2])\n",
            "<class 'torch.Tensor'> torch.Size([56688578, 1])\n",
            "<class 'torch.Tensor'> torch.Size([56688578, 1])\n",
            "<class 'torch.Tensor'> torch.Size([56688578, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMirXE6DUpZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}