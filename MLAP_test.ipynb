{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HXxFz2k46n_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d539b7-bd5b-44fd-aa38-60de291ba87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Google Drive Folder already existed.\n",
            "Symlink already existed.\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = \"MLAP_test\"\n",
        "\n",
        "# --- do not change below this ---\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "drive.mount(DRIVE_PATH, force_remount=True)\n",
        "\n",
        "# shell commands for directory with space must be\n",
        "# quoted, but not necessary in python\n",
        "COLAB_PATH = \"Colab Notebooks\"\n",
        "COLLAB_PATH_ESC = f\"\\\"{COLAB_PATH}\\\"\"\n",
        "\n",
        "# python path\n",
        "nb_path = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLAB_PATH, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "# shell path\n",
        "nb_path_bash = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLLAB_PATH_ESC, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "  os.makedirs(nb_path)\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Google Drive Folder already existed.\")\n",
        "\n",
        "try:\n",
        "  # create symlink from drive to workspace\n",
        "  os.symlink(nb_path, \"/content/notebooks\")\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Symlink already existed.\")\n",
        "\n",
        "sys.path.insert(0, nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $nb_path_bash && pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrlWbQBN4-wd",
        "outputId": "abf5cfd0-fe43-4a22-f742-91e256fb91f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/venv_MLAP_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet --target=$nb_path_bash torch"
      ],
      "metadata": {
        "id": "Z84TB2cjWaix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet --target=$nb_path_bash torchvision"
      ],
      "metadata": {
        "id": "jh5RY6F-WeDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet --target=$nb_path_bash torch_geometric -q"
      ],
      "metadata": {
        "id": "B8uJBcC02UEh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet --target=$nb_path_bash ogb"
      ],
      "metadata": {
        "id": "JDnahoWx7i7Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch-scatter"
      ],
      "metadata": {
        "id": "9mQmdgfjw_AB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, dim_h, mlp, **kwargs):\n",
        "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.mlp = mlp\n",
        "        self.bn = BatchNorm1d(dim_h)\n",
        "        self.edge_encoder = Linear(2, dim_h)\n",
        "\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        output = self.mlp(self.propagate(edge_index, x=x, edge_attr=edge_attr))\n",
        "        return self.bn(output)\n",
        "    \n",
        "    def message(self, x_j, edge_attr):\n",
        "        return x_j + edge_attr\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        return aggr_out + x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ],
      "metadata": {
        "id": "sPdTJLwx7EVv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class ASTNodeEncoder(torch.nn.Module):\n",
        "    '''\n",
        "        Input:\n",
        "            x: default node feature. the first and second column represents node type and node attributes.\n",
        "            depth: The depth of the node in the AST.\n",
        "        Output:\n",
        "            emb_dim-dimensional vector\n",
        "    '''\n",
        "    def __init__(self, emb_dim, num_nodetypes, num_nodeattributes, max_depth):\n",
        "        super(ASTNodeEncoder, self).__init__()\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        self.type_encoder = torch.nn.Embedding(num_nodetypes, emb_dim)\n",
        "        self.attribute_encoder = torch.nn.Embedding(num_nodeattributes, emb_dim)\n",
        "        self.depth_encoder = torch.nn.Embedding(self.max_depth + 1, emb_dim)\n",
        "\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * emb_dim, 2 * emb_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2 * emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, depth):\n",
        "        depth[depth > self.max_depth] = self.max_depth\n",
        "        return self.type_encoder(x[:,0]) + self.attribute_encoder(x[:,1]) + self.depth_encoder(depth)\n",
        "\n",
        "\n",
        "\n",
        "def get_vocab_mapping(seq_list, num_vocab):\n",
        "    '''\n",
        "        Input:\n",
        "            seq_list: a list of sequences\n",
        "            num_vocab: vocabulary size\n",
        "        Output:\n",
        "            vocab2idx:\n",
        "                A dictionary that maps vocabulary into integer index.\n",
        "                Additioanlly, we also index '__UNK__' and '__EOS__'\n",
        "                '__UNK__' : out-of-vocabulary term\n",
        "                '__EOS__' : end-of-sentence\n",
        "            idx2vocab:\n",
        "                A list that maps idx to actual vocabulary.\n",
        "    '''\n",
        "\n",
        "    vocab_cnt = {}\n",
        "    vocab_list = []\n",
        "    for seq in seq_list:\n",
        "        for w in seq:\n",
        "            if w in vocab_cnt:\n",
        "                vocab_cnt[w] += 1\n",
        "            else:\n",
        "                vocab_cnt[w] = 1\n",
        "                vocab_list.append(w)\n",
        "\n",
        "    cnt_list = np.array([vocab_cnt[w] for w in vocab_list])\n",
        "    topvocab = np.argsort(-cnt_list, kind = 'stable')[:num_vocab]\n",
        "\n",
        "    print('Coverage of top {} vocabulary:'.format(num_vocab))\n",
        "    print(float(np.sum(cnt_list[topvocab]))/np.sum(cnt_list))\n",
        "\n",
        "    vocab2idx = {vocab_list[vocab_idx]: idx for idx, vocab_idx in enumerate(topvocab)}\n",
        "    idx2vocab = [vocab_list[vocab_idx] for vocab_idx in topvocab]\n",
        "\n",
        "    vocab2idx['__UNK__'] = num_vocab\n",
        "    idx2vocab.append('__UNK__')\n",
        "\n",
        "    vocab2idx['__EOS__'] = num_vocab + 1\n",
        "    idx2vocab.append('__EOS__')\n",
        "\n",
        "    # test the correspondence between vocab2idx and idx2vocab\n",
        "    for idx, vocab in enumerate(idx2vocab):\n",
        "        assert(idx == vocab2idx[vocab])\n",
        "\n",
        "    # test that the idx of '__EOS__' is len(idx2vocab) - 1.\n",
        "    # This fact will be used in decode_arr_to_seq, when finding __EOS__\n",
        "    assert(vocab2idx['__EOS__'] == len(idx2vocab) - 1)\n",
        "\n",
        "    return vocab2idx, idx2vocab\n",
        "\n",
        "def augment_edge(data):\n",
        "    '''\n",
        "        Input:\n",
        "            data: PyG data object\n",
        "        Output:\n",
        "            data (edges are augmented in the following ways):\n",
        "                data.edge_index: Added next-token edge. The inverse edges were also added.\n",
        "                data.edge_attr (torch.Long):\n",
        "                    data.edge_attr[:,0]: whether it is AST edge (0) for next-token edge (1)\n",
        "                    data.edge_attr[:,1]: whether it is original direction (0) or inverse direction (1)\n",
        "    '''\n",
        "\n",
        "    ##### AST edge\n",
        "    edge_index_ast = data.edge_index\n",
        "    edge_attr_ast = torch.zeros((edge_index_ast.size(1), 2))\n",
        "\n",
        "    ##### Inverse AST edge\n",
        "    edge_index_ast_inverse = torch.stack([edge_index_ast[1], edge_index_ast[0]], dim = 0)\n",
        "    edge_attr_ast_inverse = torch.cat([torch.zeros(edge_index_ast_inverse.size(1), 1), torch.ones(edge_index_ast_inverse.size(1), 1)], dim = 1)\n",
        "\n",
        "\n",
        "    ##### Next-token edge\n",
        "\n",
        "    ## Obtain attributed nodes and get their indices in dfs order\n",
        "    # attributed_node_idx = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "    # attributed_node_idx_in_dfs_order = attributed_node_idx[torch.argsort(data.node_dfs_order[attributed_node_idx].view(-1,))]\n",
        "\n",
        "    ## Since the nodes are already sorted in dfs ordering in our case, we can just do the following.\n",
        "    attributed_node_idx_in_dfs_order = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "\n",
        "    ## build next token edge\n",
        "    # Given: attributed_node_idx_in_dfs_order\n",
        "    #        [1, 3, 4, 5, 8, 9, 12]\n",
        "    # Output:\n",
        "    #    [[1, 3, 4, 5, 8, 9]\n",
        "    #     [3, 4, 5, 8, 9, 12]\n",
        "    edge_index_nextoken = torch.stack([attributed_node_idx_in_dfs_order[:-1], attributed_node_idx_in_dfs_order[1:]], dim = 0)\n",
        "    edge_attr_nextoken = torch.cat([torch.ones(edge_index_nextoken.size(1), 1), torch.zeros(edge_index_nextoken.size(1), 1)], dim = 1)\n",
        "\n",
        "\n",
        "    ##### Inverse next-token edge\n",
        "    edge_index_nextoken_inverse = torch.stack([edge_index_nextoken[1], edge_index_nextoken[0]], dim = 0)\n",
        "    edge_attr_nextoken_inverse = torch.ones((edge_index_nextoken.size(1), 2))\n",
        "\n",
        "    data.edge_index = torch.cat([edge_index_ast, edge_index_ast_inverse, edge_index_nextoken, edge_index_nextoken_inverse], dim = 1)\n",
        "    data.edge_attr = torch.cat([edge_attr_ast,   edge_attr_ast_inverse, edge_attr_nextoken,  edge_attr_nextoken_inverse], dim = 0)\n",
        "\n",
        "    return data\n",
        "\n",
        "def encode_y_to_arr(data, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        data: PyG graph object\n",
        "        output: add y_arr to data \n",
        "    '''\n",
        "\n",
        "    # PyG >= 1.5.0\n",
        "    seq = data.y\n",
        "\n",
        "    data.y_arr = encode_seq_to_arr(seq, vocab2idx, max_seq_len)\n",
        "\n",
        "    return data\n",
        "\n",
        "def encode_seq_to_arr(seq, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        seq: A list of words\n",
        "        output: add y_arr (torch.Tensor)\n",
        "    '''\n",
        "\n",
        "    augmented_seq = seq[:max_seq_len] + ['__EOS__'] * max(0, max_seq_len - len(seq))\n",
        "    return torch.tensor([[vocab2idx[w] if w in vocab2idx else vocab2idx['__UNK__'] for w in augmented_seq]], dtype = torch.long)\n",
        "\n",
        "\n",
        "def decode_arr_to_seq(arr, idx2vocab):\n",
        "    '''\n",
        "        Input: torch 1d array: y_arr\n",
        "        Output: a sequence of words.\n",
        "    '''\n",
        "\n",
        "    eos_idx_list = (arr == len(idx2vocab) - 1).nonzero() # find the position of __EOS__ (the last vocab in idx2vocab)\n",
        "    if len(eos_idx_list) > 0:\n",
        "        clippted_arr = arr[: torch.min(eos_idx_list)] # find the smallest __EOS__\n",
        "    else:\n",
        "        clippted_arr = arr\n",
        "\n",
        "    return list(map(lambda x: idx2vocab[x], clippted_arr.cpu()))"
      ],
      "metadata": {
        "id": "ug_S7HC97W2E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "        self.lstm = nn.LSTMCell(dim_h, dim_h)\n",
        "        self.w_hc = nn.Linear(dim_h * 2, dim_h)\n",
        "        self.layernorm = nn.LayerNorm(dim_h)\n",
        "        self.vocab_encoder = nn.Embedding(len(vocab2idx), dim_h)\n",
        "        self.vocab_bias = nn.Parameter(torch.zeros(len(vocab2idx)))\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        if (training):\n",
        "            batched_label = torch.vstack([encode_seq_to_arr(label, self.vocab2idx, self.max_seq_len - 1) for label in labels])\n",
        "            batched_label = torch.hstack((torch.zeros((batch_size, 1), dtype=torch.int64), batched_label))\n",
        "            true_emb = self.vocab_encoder(batched_label.to(device=self.device))\n",
        "        \n",
        "        h_t, c_t = layer_reps[-1].clone(), layer_reps[-1].clone()\n",
        "\n",
        "        layer_reps = layer_reps.transpose(0,1)\n",
        "        output = []\n",
        "\n",
        "        pred_emb = self.vocab_encoder(torch.zeros((batch_size), dtype=torch.int64, device=self.device))\n",
        "        vocab_mat = self.vocab_encoder(torch.arange(len(self.vocab2idx), dtype=torch.int64, device=self.device))\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            if (training): # teacher forcing\n",
        "                input = true_emb[:, i]\n",
        "            else:\n",
        "                input = pred_emb\n",
        "            \n",
        "            h_t, c_t = self.lstm(input, (h_t, c_t))\n",
        "\n",
        "            a = F.softmax(torch.bmm(layer_reps, h_t.unsqueeze(-1)).squeeze(-1), dim=1)  # (batch_size, L + 1)\n",
        "            context = torch.bmm(a.unsqueeze(1), layer_reps).squeeze(1)\n",
        "            pred_emb = torch.tanh(self.layernorm(self.w_hc(torch.hstack((h_t, context)))))  # (batch_size, dim_h)\n",
        "\n",
        "            # (batch_size, len(vocab)) x max_seq_len\n",
        "            output.append(torch.matmul(pred_emb, vocab_mat.T) + self.vocab_bias.unsqueeze(0))\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "XGah0XWb6ylP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.utils import dropout_adj, degree, to_undirected\n",
        "import networkx as nx\n",
        "\n",
        "# from: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/functional.py\n",
        "# and: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/utils.py\n",
        "\n",
        "def compute_pr(edge_index, damp: float = 0.85, k: int = 10):\n",
        "    # page rank\n",
        "    # interesting comment: https://github.com/CRIPAC-DIG/GCA/issues/4\n",
        "    num_nodes = edge_index.max().item() + 1\n",
        "    deg_out = degree(edge_index[0])\n",
        "    x = torch.ones((num_nodes, )).to(edge_index.device).to(torch.float32)\n",
        "\n",
        "    for i in range(k):\n",
        "        edge_msg = x[edge_index[0]] / deg_out[edge_index[0]]\n",
        "        agg_msg = scatter(edge_msg, edge_index[1], reduce='sum')\n",
        "\n",
        "        x = (1 - damp) * x + damp * agg_msg\n",
        "\n",
        "    return x\n",
        "\n",
        "def eigenvector_centrality(data):\n",
        "    graph = to_networkx(data)\n",
        "    x = nx.eigenvector_centrality_numpy(graph)\n",
        "    x = [x[i] for i in range(data.num_nodes)]\n",
        "    return torch.tensor(x, dtype=torch.float32).to(data.edge_index.device)\n",
        "\n",
        "\n",
        "def drop_feature(x, drop_prob):\n",
        "    drop_mask = torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1) < drop_prob\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_feature_weighted(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w.repeat(x.size(0)).view(x.size(0), -1)\n",
        "\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_feature_weighted_2(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w\n",
        "\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def feature_drop_weights(x, node_c):\n",
        "    x = x.to(torch.bool).to(torch.float32)\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def feature_drop_weights_dense(x, node_c):\n",
        "    x = x.abs()\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def drop_edge_weighted(edge_index, edge_weights, p: float, threshold: float = 1.):\n",
        "    edge_weights = edge_weights / edge_weights.mean() * p\n",
        "    edge_weights = edge_weights.where(edge_weights < threshold, torch.ones_like(edge_weights) * threshold)\n",
        "    sel_mask = torch.bernoulli(1. - edge_weights).to(torch.bool)\n",
        "\n",
        "    return edge_index[:, sel_mask]\n",
        "\n",
        "\n",
        "def degree_drop_weights(edge_index):\n",
        "    edge_index_ = to_undirected(edge_index)\n",
        "    deg = degree(edge_index_[1])\n",
        "    deg_col = deg[edge_index[1]].to(torch.float32)\n",
        "    s_col = torch.log(deg_col)\n",
        "    weights = (s_col.max() - s_col) / (s_col.max() - s_col.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def pr_drop_weights(edge_index, aggr: str = 'sink', k: int = 10):\n",
        "    pv = compute_pr(edge_index, k=k)\n",
        "    pv_row = pv[edge_index[0]].to(torch.float32)\n",
        "    pv_col = pv[edge_index[1]].to(torch.float32)\n",
        "    s_row = torch.log(pv_row)\n",
        "    s_col = torch.log(pv_col)\n",
        "    if aggr == 'sink':\n",
        "        s = s_col\n",
        "    elif aggr == 'source':\n",
        "        s = s_row\n",
        "    elif aggr == 'mean':\n",
        "        s = (s_col + s_row) * 0.5\n",
        "    else:\n",
        "        s = s_col\n",
        "    weights = (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def evc_drop_weights(data):\n",
        "    evc = eigenvector_centrality(data)\n",
        "    evc = evc.where(evc > 0, torch.zeros_like(evc))\n",
        "    evc = evc + 1e-8\n",
        "    s = evc.log()\n",
        "\n",
        "    edge_index = data.edge_index\n",
        "    s_row, s_col = s[edge_index[0]], s[edge_index[1]]\n",
        "    s = s_col\n",
        "\n",
        "    return (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "def graph_perturb(data, drop_scheme='pr'):\n",
        "  if drop_scheme == 'degree':\n",
        "      drop_weights = degree_drop_weights(data.edge_index)\n",
        "      edge_index_ = to_undirected(data.edge_index)\n",
        "      node_deg = degree(edge_index_[1])\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_deg)\n",
        "  elif drop_scheme == 'pr':\n",
        "      drop_weights = pr_drop_weights(data.edge_index, aggr='sink', k=200)\n",
        "      node_pr = compute_pr(data.edge_index)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_pr)\n",
        "  elif drop_scheme == 'evc':\n",
        "      drop_weights = evc_drop_weights(data)\n",
        "      node_evc = eigenvector_centrality(data)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_evc)\n",
        "  else:\n",
        "      feature_weights = torch.ones((data.x.size(1),))\n",
        "      drop_weights = None\n",
        "  \n",
        "  return feature_weights, drop_weights\n",
        "\n",
        "def drop_edge(data, drop_edge_rate, drop_weights, drop_scheme='pr', drop_edge_weighted_threshold=0.7):\n",
        "  if drop_scheme == 'uniform':\n",
        "      return dropout_adj(data.edge_index, p=drop_edge_rate)[0]\n",
        "  elif drop_scheme in ['degree', 'evc', 'pr']:\n",
        "      return drop_edge_weighted(\n",
        "          data.edge_index, \n",
        "          drop_weights, \n",
        "          p=drop_edge_rate, \n",
        "          threshold=drop_edge_weighted_threshold\n",
        "      )\n",
        "  else:\n",
        "      raise Exception(f'undefined drop scheme: {drop_scheme}')\n",
        "\n",
        "def get_contrastive_graph_pair(data, drop_scheme='pr', drop_feature_rates=(0.7, 0.7), drop_edge_rates=(0.5, 0.5)):\n",
        "  # use augmentation scheme to determine the weights of each node\n",
        "  # i.e. pagerank, eigenvector centrality, node degree\n",
        "  feat_weights, drop_weights = graph_perturb(data, drop_scheme)\n",
        "\n",
        "  # apply drop edge according to computed features\n",
        "  dr_e_1, dr_e_2 = drop_edge_rates\n",
        "  edge_index_1 = drop_edge(data, dr_e_1, drop_weights, drop_scheme)\n",
        "  edge_index_2 = drop_edge(data, dr_e_2, drop_weights, drop_scheme)\n",
        "\n",
        "  dr_f_1, dr_f_2 = drop_feature_rates\n",
        "\n",
        "  if drop_scheme in ['pr', 'degree', 'evc']:\n",
        "    # graph-aware drop feature\n",
        "    x_1 = drop_feature_weighted_2(data.x, feat_weights, dr_f_1)\n",
        "    e_1 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_1)\n",
        "    \n",
        "    x_2 = drop_feature_weighted_2(data.x, feat_weights, dr_f_2)\n",
        "    e_2 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_2)\n",
        "  else:\n",
        "    # naive drop feature\n",
        "    x_1 = drop_feature(data.x, dr_f_1)\n",
        "    e_1 = drop_feature(data.edge_attr, dr_f_1)\n",
        "    \n",
        "    x_2 = drop_feature(data.x, dr_f_2)\n",
        "    e_2 = drop_feature(data.edge_attr, dr_f_2)\n",
        "\n",
        "  return (\n",
        "      # graph 1\n",
        "      (x_1, edge_index_1, e_1),\n",
        "      # graph 2\n",
        "      (x_2, edge_index_2, e_2),\n",
        "  )"
      ],
      "metadata": {
        "id": "lHE4pQgKr-jN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear, Sequential, ReLU\n",
        "from torch_geometric.nn.norm import GraphNorm\n",
        "from torch_geometric.nn.glob import AttentionalAggregation\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class MLAP_GIN(torch.nn.Module):\n",
        "    def __init__(self, dim_h, depth, node_encoder, norm=False, residual=False):\n",
        "        super(MLAP_GIN, self).__init__()\n",
        "\n",
        "        self.dim_h = dim_h\n",
        "        self.depth = depth\n",
        "\n",
        "        self.node_encoder = node_encoder\n",
        "\n",
        "        self.norm = norm\n",
        "        self.residual = residual\n",
        "\n",
        "        # GIN layers\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [GINConv(dim_h, Sequential(\n",
        "                Linear(dim_h, dim_h),\n",
        "                ReLU(),\n",
        "                Linear(dim_h, dim_h))) for _ in range(depth)]\n",
        "        )\n",
        "            \n",
        "        # normalization layers\n",
        "        self.norm = torch.nn.ModuleList([GraphNorm(dim_h) for _ in range(self.depth)])\n",
        "        \n",
        "        # layer-wise attention poolings\n",
        "        self.att_poolings = torch.nn.ModuleList(\n",
        "            [AttentionalAggregation(\n",
        "                Sequential(\n",
        "                    Linear(self.dim_h, 2*self.dim_h), \n",
        "                    ReLU(), \n",
        "                    Linear(2*self.dim_h, 1))) for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def forward(self, batched_data):\n",
        "        self.graph_embs = []\n",
        "\n",
        "        x = batched_data.x\n",
        "        edge_index = batched_data.edge_index\n",
        "        edge_attr = batched_data.edge_attr\n",
        "        node_depth = batched_data.node_depth\n",
        "        batch = batched_data.batch\n",
        "\n",
        "        # graph augmentation step\n",
        "        g_1, g_2 = get_contrastive_graph_pair(batched_data)\n",
        "        print(g_1)\n",
        "        print(g_2)\n",
        "        \n",
        "        x = self.node_encoder(x, node_depth.view(-1,))\n",
        "\n",
        "        for d in range(self.depth):\n",
        "            x_in = x\n",
        "\n",
        "            x = self.layers[d](x, edge_index, edge_attr)\n",
        "            if (self.norm):\n",
        "                x = self.norm[d](x, batch)\n",
        "            if (d < self.depth - 1):\n",
        "                x = F.relu(x)\n",
        "            if (self.residual):\n",
        "                x = x + x_in\n",
        "            \n",
        "            h_g = self.att_poolings[d](x, batch)\n",
        "            self.graph_embs.append(h_g)\n",
        "        \n",
        "        agg = self.aggregate()\n",
        "        self.graph_embs.append(agg)\n",
        "        output = torch.stack(self.graph_embs, dim=0)\n",
        "        return output\n",
        "    \n",
        "    def aggregate(self):\n",
        "        pass\n",
        "\n",
        "class MLAP_Sum(MLAP_GIN):\n",
        "    def aggregate(self):\n",
        "        return torch.stack(self.graph_embs, dim=0).sum(dim=0)\n",
        "\n",
        "class MLAP_Weighted(MLAP_GIN):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weight = torch.nn.Parameter(torch.ones(self.depth, 1, 1))\n",
        "\n",
        "    def aggregate(self):\n",
        "        a = F.softmax(self.weight, dim=0)\n",
        "        h = torch.stack(self.graph_embs, dim=0)\n",
        "        return (a * h).sum(dim=0)"
      ],
      "metadata": {
        "id": "cdeasWun61lO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, batch_size, depth, dim_h, max_seq_len, node_encoder, vocab2idx, device):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.depth = depth\n",
        "        self.dim_h = dim_h\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.node_encoder = node_encoder\n",
        "\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # contrastive learning must go here over each of the layers\n",
        "        self.gnn = MLAP_Weighted(dim_h, depth, node_encoder, norm=True, residual=True)\n",
        "\n",
        "        self.decoder = LSTMDecoder(dim_h, max_seq_len, vocab2idx, device)\n",
        "\n",
        "    def forward(self, batched_data, labels, training=False):\n",
        "        embeddings = self.gnn(batched_data)\n",
        "        predictions = self.decoder(self.batch_size, embeddings, labels, training=training)\n",
        "\n",
        "        # for each batch, the prediction for the ith word is a logit\n",
        "        # decoding each prediction to a word is done in the evaluation task in main\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "H2kwGBXj63ud"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# importing OGB\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "DATA_ROOT = nb_path + \"/ogb\"\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "def train(model, device, loader, optimizer, multicls_criterion):\n",
        "    loss_accum = 0\n",
        "    print('New epoch: ', 'loader size = ' + str(len(loader)))\n",
        "\n",
        "    # TODO: cut down on loader size\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            # pyg does batching in an interesting way\n",
        "            labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "            pred_list = model(batch, labels, training=True)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = 0\n",
        "            for i in range(len(pred_list)):\n",
        "                loss += multicls_criterion(\n",
        "                    pred_list[i].to(torch.float32), \n",
        "                    batch.y_arr[:, i]\n",
        "                )\n",
        "\n",
        "            # TODO: add contrastive learning objective\n",
        "            loss = loss / len(pred_list)\n",
        "\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_accum += loss.item()\n",
        "            print('Average loss after batch ' + str(step) + ': ' + str(loss_accum / (step + 1)))\n",
        "\n",
        "    print('Average training loss: {}'.format(loss_accum / (step + 1)))\n",
        "    return loss_accum / (step + 1)\n",
        "\n",
        "def eval(model, device, loader, evaluator, arr_to_seq):\n",
        "    seq_ref_list = []\n",
        "    seq_pred_list = []\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "                pred_list = model(batch, labels) # training=False by default\n",
        "\n",
        "            mat = []\n",
        "            for i in range(len(pred_list)):\n",
        "                mat.append(torch.argmax(pred_list[i], dim=1).view(-1, 1))\n",
        "            mat = torch.cat(mat, dim=1)\n",
        "\n",
        "            seq_pred = [arr_to_seq(arr) for arr in mat]\n",
        "\n",
        "            seq_ref = labels\n",
        "\n",
        "            seq_ref_list.extend(seq_ref)\n",
        "            seq_pred_list.extend(seq_pred)\n",
        "\n",
        "    input_dict = {\"seq_ref\": seq_ref_list, \"seq_pred\": seq_pred_list}\n",
        "    return evaluator.eval(input_dict)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # constants\n",
        "    dataset_name = \"ogbg-code2\"\n",
        "\n",
        "    num_vocab = 5000\n",
        "    max_seq_len = 5\n",
        "\n",
        "    depth = 3\n",
        "    batch_size = 50\n",
        "    epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    step_size = 10\n",
        "    decay_rate = 0.1\n",
        "    weight_decay = 0.00005\n",
        "\n",
        "    dim_h = 512\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    # TODO: add colab path as root\n",
        "    dataset = PygGraphPropPredDataset(dataset_name, root=\"\")\n",
        "\n",
        "\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    vocab2idx, idx2vocab = get_vocab_mapping(\n",
        "        [dataset.data.y[i] for i in split_idx['train']], \n",
        "        num_vocab\n",
        "    )\n",
        "    dataset.transform = transforms.Compose(\n",
        "        [augment_edge, lambda data: encode_y_to_arr(data, vocab2idx, max_seq_len)]\n",
        "    )\n",
        "\n",
        "    evaluator = Evaluator(dataset_name)\n",
        "\n",
        "    train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    nodetypes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'typeidx2type.csv.gz'))\n",
        "    nodeattributes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'attridx2attr.csv.gz'))\n",
        "\n",
        "    node_encoder = ASTNodeEncoder(\n",
        "        dim_h, \n",
        "        num_nodetypes=len(nodetypes_mapping['type']), \n",
        "        num_nodeattributes=len(nodeattributes_mapping['attr']), \n",
        "        max_depth=20\n",
        "    )\n",
        "\n",
        "    model = Model(\n",
        "        batch_size, \n",
        "        depth, \n",
        "        dim_h, \n",
        "        max_seq_len, \n",
        "        node_encoder, \n",
        "        vocab2idx, \n",
        "        device\n",
        "    ).to(device)\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'#Params: {num_params}')\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)\n",
        "\n",
        "    multicls_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_curve = []\n",
        "    trainL_curve = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print (datetime.datetime.now().strftime('%Y.%m.%d-%H:%M:%S'))\n",
        "        print(\"Epoch {} training...\".format(epoch))\n",
        "        print (\"lr: \", optimizer.param_groups[0]['lr'])\n",
        "        train_loss = train(model, device, train_loader, optimizer, multicls_criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print('Evaluating...')\n",
        "        train_perf = eval(model, device, train_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "        valid_perf = eval(model, device, valid_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "        test_perf = eval(model, device, test_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "\n",
        "        print('Train:', train_perf[dataset.eval_metric],\n",
        "              'Validation:', valid_perf[dataset.eval_metric],\n",
        "              'Test:', test_perf[dataset.eval_metric],\n",
        "              'Train loss:', train_loss)\n",
        "\n",
        "        train_curve.append(train_perf[dataset.eval_metric])\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "        trainL_curve.append(train_loss)\n",
        "\n",
        "    print('F1')\n",
        "    best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    best_train = max(train_curve)\n",
        "    print('Finished training!')\n",
        "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
        "\n",
        "    print('Finished test: {}, Validation: {}, Train: {}, epoch: {}, best train: {}, best loss: {}'\n",
        "          .format(test_curve[best_val_epoch], valid_curve[best_val_epoch], train_curve[best_val_epoch],\n",
        "                  best_val_epoch, best_train, min(trainL_curve)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jrih46577BIZ",
        "outputId": "5f64ad22-5ca4-4b57-f5bb-a9237e1add90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/code2.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.91 GB: 100%|██████████| 934/934 [00:26<00:00, 34.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting code2.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 452741/452741 [00:01<00:00, 339626.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 452741/452741 [00:21<00:00, 21075.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "/content/drive/MyDrive/Colab Notebooks/venv_MLAP_test/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage of top 5000 vocabulary:\n",
            "0.9025832389087423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/venv_MLAP_test/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Params: 15655312\n",
            "2023.04.12-16:55:20\n",
            "Epoch 1 training...\n",
            "lr:  0.001\n",
            "New epoch:  loader size = 8160\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4370],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[    1,     5,     8,  ..., 10066, 10070, 10072],\n",
            "        [    5,     6,    15,  ..., 10062, 10066, 10071]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  4370],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[    1,     1,     2,  ..., 10070, 10072, 10076],\n",
            "        [    5,     7,     3,  ..., 10066, 10071, 10072]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 0: 62.33211135864258\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  2241],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   1,    1,    2,  ..., 5615, 5633, 5635],\n",
            "        [   2,    7,    5,  ..., 5611, 5630, 5633]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  2241],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    2,    2,  ..., 5627, 5630, 5633],\n",
            "        [   9,    3,    4,  ..., 5625, 5627, 5630]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 1: 49.41934013366699\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 8246, 8250, 8256],\n",
            "        [   4,    6,   14,  ..., 8245, 8246, 8255]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 8245, 8246, 8255],\n",
            "        [   1,    6,   40,  ..., 8242, 8245, 8252]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 2: 44.9311892191569\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [95,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6260, 6264, 6267],\n",
            "        [   1,    2,    5,  ..., 6256, 6262, 6264]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [95,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6256, 6264, 6267],\n",
            "        [   1,    5,   27,  ..., 6255, 6262, 6264]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 3: 50.58717632293701\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   61,  5805],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7042, 7052, 7055],\n",
            "        [   1,    4,   21,  ..., 7039, 7051, 7052]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  5805],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7047, 7051, 7052],\n",
            "        [   1,    6,   11,  ..., 7043, 7047, 7051]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 4: 48.96622085571289\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   79,  3408],\n",
            "        [   61, 10029],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 8459, 8467, 8469],\n",
            "        [   1,    2,    7,  ..., 8458, 8466, 8468]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  3408],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    5,  ..., 8464, 8467, 8469],\n",
            "        [   7,   21,    6,  ..., 8462, 8466, 8468]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 5: 44.197878201802574\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [73,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   1,    2,    5,  ..., 5873, 5883, 5886],\n",
            "        [  26,    3,    6,  ..., 5871, 5881, 5883]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [73,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    2,  ..., 5871, 5874, 5881],\n",
            "        [   1,    5,    4,  ..., 5868, 5873, 5878]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 6: 41.98566491263254\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  6684],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6139, 6142, 6146],\n",
            "        [   1,    6,    8,  ..., 6138, 6139, 6145]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  6684],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 6132, 6135, 6146],\n",
            "        [   6,    8,   56,  ..., 6129, 6132, 6145]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 7: 39.97666120529175\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [73,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4976, 4977, 4984],\n",
            "        [   1,    2,   18,  ..., 4972, 4976, 4981]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   73, 10028],\n",
            "        [   61, 10029],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4972, 4980, 4984],\n",
            "        [   1,    2,   18,  ..., 4968, 4977, 4981]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 8: 37.49495612250434\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [54,  0],\n",
            "        [73,  0],\n",
            "        [62,  0]], device='cuda:0'), tensor([[   0,    1,    7,  ..., 5060, 5065, 5067],\n",
            "        [   1,    7,   10,  ..., 5059, 5060, 5065]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [54,  0],\n",
            "        [73,  0],\n",
            "        [62,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5053, 5060, 5067],\n",
            "        [   1,    2,    5,  ..., 5050, 5059, 5065]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 9: 35.80590343475342\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  9770],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   1,    2,    2,  ..., 6315, 6318, 6320],\n",
            "        [   7,    3,    4,  ..., 6314, 6315, 6318]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  9770],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    2,  ..., 6315, 6318, 6320],\n",
            "        [   1,   24,    3,  ..., 6314, 6315, 6318]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 10: 34.3978271484375\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7829, 7845, 7848],\n",
            "        [   1,    2,    8,  ..., 7828, 7844, 7845]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7834, 7836, 7841],\n",
            "        [   1,    2,   27,  ..., 7829, 7835, 7836]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 11: 33.17727518081665\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  2895],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5102, 5117, 5120],\n",
            "        [   1,    7,   18,  ..., 5100, 5116, 5117]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  2895],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5116, 5117, 5120],\n",
            "        [   1,    2,    5,  ..., 5114, 5116, 5117]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 12: 31.996027286236103\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 7349, 7354, 7356],\n",
            "        [   2,    7,   21,  ..., 7348, 7352, 7354]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7348, 7349, 7354],\n",
            "        [   1,    2,    7,  ..., 7344, 7348, 7352]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 13: 30.8842807497297\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [54,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5814, 5821, 5832],\n",
            "        [   1,    2,    6,  ..., 5813, 5819, 5831]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [54,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5828, 5831, 5834],\n",
            "        [   1,   36,   44,  ..., 5824, 5828, 5832]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 14: 29.9605042775472\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7326, 7332, 7333],\n",
            "        [   1,    2,    7,  ..., 7325, 7329, 7332]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   61,  8153],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7326, 7332, 7333],\n",
            "        [   1,    7,    9,  ..., 7325, 7329, 7332]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 15: 29.0735884308815\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [61,  0],\n",
            "        [54,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6105, 6110, 6111],\n",
            "        [   1,    2,    6,  ..., 6102, 6106, 6110]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [61,  0],\n",
            "        [54,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6101, 6102, 6106],\n",
            "        [   1,    4,    6,  ..., 6097, 6101, 6105]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 16: 28.361607663771686\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 8147, 8153, 8155],\n",
            "        [   1,    2,    6,  ..., 8145, 8149, 8153]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 8137, 8141, 8149],\n",
            "        [   1,    2,    8,  ..., 8135, 8139, 8147]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 17: 27.682744608985054\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   61,  8153],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6141, 6146, 6150],\n",
            "        [   1,    4,    6,  ..., 6138, 6145, 6149]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6146, 6150, 6154],\n",
            "        [   1,    4,    6,  ..., 6145, 6149, 6150]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 18: 27.036171762566816\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   73, 10028],\n",
            "        [   61,  9759],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    2,  ..., 6473, 6477, 6478],\n",
            "        [   1,   32,    3,  ..., 6469, 6474, 6477]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [73,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6469, 6477, 6481],\n",
            "        [   1,    4,   32,  ..., 6468, 6474, 6480]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 19: 26.269828367233277\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [54,  0],\n",
            "        [73,  0],\n",
            "        [62,  0]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 5963, 5966, 5968],\n",
            "        [   2,    6,    8,  ..., 5962, 5963, 5966]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   73, 10028],\n",
            "        [   62, 10028]], device='cuda:0'), tensor([[   1,    2,    8,  ..., 5958, 5966, 5968],\n",
            "        [   2,    5,    9,  ..., 5955, 5963, 5966]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 20: 25.68633915129162\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4808],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    6,  ..., 7423, 7427, 7431],\n",
            "        [   1,    4,    7,  ..., 7420, 7423, 7428]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4808],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    6,    6,  ..., 7423, 7427, 7431],\n",
            "        [   1,    7,    9,  ..., 7420, 7423, 7428]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 21: 25.089351350610908\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [61,  0],\n",
            "        [54,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5895, 5900, 5915],\n",
            "        [   1,    2,    4,  ..., 5894, 5895, 5911]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [61,  0],\n",
            "        [54,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 5907, 5911, 5915],\n",
            "        [  21,   25,   49,  ..., 5906, 5907, 5911]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 22: 24.522610581439473\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7631, 7640, 7648],\n",
            "        [   1,    2,    6,  ..., 7630, 7635, 7647]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 7644, 7647, 7648],\n",
            "        [   1,    4,   18,  ..., 7643, 7644, 7647]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 23: 24.019909381866455\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   6,    6,    7,  ..., 5288, 5292, 5293],\n",
            "        [   7,   14,    8,  ..., 5281, 5288, 5292]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8153],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    6,    6,  ..., 5270, 5272, 5281],\n",
            "        [   1,    7,   14,  ..., 5266, 5270, 5278]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 24: 23.555009422302245\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  2183],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 9281, 9282, 9285],\n",
            "        [   1,    2,   12,  ..., 9277, 9281, 9282]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  2183],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 9282, 9285, 9287],\n",
            "        [   2,   14,   30,  ..., 9281, 9282, 9285]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 25: 23.1019397515517\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [62,  0],\n",
            "        [95,  0],\n",
            "        [62,  0]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4976, 4968, 4974],\n",
            "        [   1,    2,    8,  ..., 4978, 4965, 4970]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [62,  0],\n",
            "        [95,  0],\n",
            "        [62,  0]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 4970, 4976, 4978],\n",
            "        [   2,    8,   31,  ..., 4969, 4974, 4976]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "Average loss after batch 26: 22.655464702182346\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4262],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    2,    7,  ..., 5644, 5656, 5659],\n",
            "        [   1,    3,    8,  ..., 5639, 5651, 5657]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4262],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    7,    7,  ..., 5626, 5651, 5657],\n",
            "        [   7,    8,   67,  ..., 5625, 5650, 5656]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 27: 22.33868411609105\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   61,  6370],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4719, 4720, 4723],\n",
            "        [   1,    5,    7,  ..., 4717, 4719, 4720]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  6370],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4716, 4717, 4725],\n",
            "        [   1,    2,   18,  ..., 4713, 4716, 4723]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 28: 21.93302713591477\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   95, 10028],\n",
            "        [   61,  5473],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6403, 6405, 6407],\n",
            "        [   1,    2,   87,  ..., 6400, 6403, 6405]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  5473],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6405, 6407, 6410],\n",
            "        [   1,   87,  130,  ..., 6403, 6405, 6407]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 29: 21.560292975107828\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8305],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    2,  ..., 7503, 7505, 7509],\n",
            "        [   1,    2,    3,  ..., 7500, 7503, 7506]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   61,  8305],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   1,    2,    2,  ..., 7510, 7514, 7519],\n",
            "        [   2,    3,    4,  ..., 7509, 7510, 7517]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 30: 21.22553966891381\n",
            "(tensor([[59,  0],\n",
            "        [35,  0],\n",
            "        [93,  0],\n",
            "        ...,\n",
            "        [95,  0],\n",
            "        [61,  0],\n",
            "        [54,  0]], device='cuda:0'), tensor([[   1,    1,    2,  ..., 5781, 5785, 5793],\n",
            "        [   9,   11,    3,  ..., 5779, 5784, 5789]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   95, 10028],\n",
            "        [   61,  6617],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5776, 5785, 5793],\n",
            "        [   1,    2,   11,  ..., 5773, 5784, 5789]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 31: 20.868842780590057\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 6565, 6567, 6571],\n",
            "        [   2,    7,   27,  ..., 6562, 6565, 6567]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10029],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 6580, 6588, 6589],\n",
            "        [   2,    7,   27,  ..., 6576, 6584, 6588]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 32: 20.561614094358504\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  5566],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5251, 5253, 5268],\n",
            "        [   1,    6,   17,  ..., 5249, 5251, 5265]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  5566],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    6,  ..., 5262, 5280, 5282],\n",
            "        [   2,    4,    7,  ..., 5259, 5277, 5280]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 33: 20.2827327672173\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10028],\n",
            "        [    0,   159]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5454, 5458, 5461],\n",
            "        [   1,    5,   37,  ..., 5452, 5456, 5458]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   86, 10028],\n",
            "        [   85, 10028],\n",
            "        [   67,   159]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 5454, 5456, 5458],\n",
            "        [   2,    7,   93,  ..., 5452, 5454, 5456]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 34: 20.02857551574707\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   67,   129],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5725, 5729, 5732],\n",
            "        [   1,    2,   11,  ..., 5721, 5725, 5729]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,   129],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    1,  ..., 5710, 5729, 5732],\n",
            "        [   2,    6,   20,  ..., 5709, 5725, 5729]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 35: 19.74040921529134\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   61,  8198],\n",
            "        [   54, 10028],\n",
            "        [   54, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4474, 4482, 4483],\n",
            "        [   1,    2,    9,  ..., 4471, 4480, 4482]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0,  8198],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 4458, 4459, 4474],\n",
            "        [   1,    2,    7,  ..., 4454, 4458, 4471]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 36: 19.480572855150378\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4118],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5100, 5101, 5102],\n",
            "        [   1,    2,   39,  ..., 5096, 5100, 5101]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0,  4118],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    2,    2,  ..., 5101, 5102, 5106],\n",
            "        [   2,    4,    5,  ..., 5100, 5101, 5102]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 37: 19.261319938458893\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   1,    1,    6,  ..., 6259, 6266, 6287],\n",
            "        [   2,    6,    7,  ..., 6256, 6264, 6283]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10029],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 6260, 6266, 6269],\n",
            "        [   1,    2,   32,  ..., 6259, 6264, 6266]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0'))\n",
            "Average loss after batch 38: 18.98832844465207\n",
            "(tensor([[    0, 10028],\n",
            "        [    0,  1461],\n",
            "        [    0, 10028],\n",
            "        ...,\n",
            "        [    0, 10028],\n",
            "        [    0, 10028],\n",
            "        [    0, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5388, 5398, 5399],\n",
            "        [   1,    2,    5,  ..., 5387, 5395, 5398]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "(tensor([[   59, 10028],\n",
            "        [   35,  1461],\n",
            "        [   93, 10028],\n",
            "        ...,\n",
            "        [   54, 10028],\n",
            "        [   78, 10028],\n",
            "        [   62, 10028]], device='cuda:0'), tensor([[   0,    1,    1,  ..., 5383, 5392, 5399],\n",
            "        [   1,    2,   30,  ..., 5381, 5388, 5398]], device='cuda:0'), tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        ...,\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], device='cuda:0'))\n",
            "Average loss after batch 39: 18.790577816963197\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6be3a7051c0c>\u001b[0m in \u001b[0;36m<cell line: 195>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6be3a7051c0c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} training...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"lr: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticls_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6be3a7051c0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, loader, optimizer, multicls_criterion)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# pyg does batching in an interesting way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mpred_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/venv_MLAP_test/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-79a074b4178a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_data, labels, training)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/venv_MLAP_test/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-594a41b7e365>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# graph augmentation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_contrastive_graph_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7f9d3bffaac2>\u001b[0m in \u001b[0;36mget_contrastive_graph_pair\u001b[0;34m(data, drop_scheme, drop_feature_rates, drop_edge_rates)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;31m# use augmentation scheme to determine the weights of each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0;31m# i.e. pagerank, eigenvector centrality, node degree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m   \u001b[0mfeat_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_perturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_scheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;31m# apply drop edge according to computed features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7f9d3bffaac2>\u001b[0m in \u001b[0;36mgraph_perturb\u001b[0;34m(data, drop_scheme)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mfeature_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_drop_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdrop_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       \u001b[0mdrop_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr_drop_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sink'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mnode_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mfeature_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_drop_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_pr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7f9d3bffaac2>\u001b[0m in \u001b[0;36mpr_drop_weights\u001b[0;34m(edge_index, aggr, k)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpr_drop_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sink'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mpv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mpv_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mpv_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7f9d3bffaac2>\u001b[0m in \u001b[0;36mcompute_pr\u001b[0;34m(edge_index, damp, k)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0medge_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdeg_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0magg_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdamp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdamp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0magg_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtLjOUlzXwRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}