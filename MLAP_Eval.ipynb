{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = \"MLAP_test_eval\"\n",
        "\n",
        "# --- do not change below this ---\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "drive.mount(DRIVE_PATH, force_remount=True)\n",
        "\n",
        "# shell commands for directory with space must be\n",
        "# quoted, but not necessary in python\n",
        "COLAB_PATH = \"Colab Notebooks\"\n",
        "COLLAB_PATH_ESC = f\"\\\"{COLAB_PATH}\\\"\"\n",
        "\n",
        "# python path\n",
        "nb_path = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLAB_PATH, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "# shell path\n",
        "nb_path_bash = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLLAB_PATH_ESC, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "  os.makedirs(nb_path)\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Google Drive Folder already existed.\")\n",
        "\n",
        "try:\n",
        "  # create symlink from drive to workspace\n",
        "  os.symlink(nb_path, \"/content/notebooks\")\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Symlink already existed.\")\n",
        "\n",
        "sys.path.insert(0, nb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFQxATSSWofs",
        "outputId": "8142a45f-9fbc-44a7-eb2a-f47a0358b3c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Google Drive Folder already existed.\n",
            "Symlink already existed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch_geometric ogb"
      ],
      "metadata": {
        "id": "QJIm9RQtWpcW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "wkKs0jaewe9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "\n",
        "class ASTNodeEncoder(torch.nn.Module):\n",
        "    '''\n",
        "        Input:\n",
        "            x: default node feature. the first and second column represents node type and node attributes.\n",
        "            depth: The depth of the node in the AST.\n",
        "\n",
        "        Output:\n",
        "            emb_dim-dimensional vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, emb_dim, num_nodetypes, num_nodeattributes, max_depth):\n",
        "        super(ASTNodeEncoder, self).__init__()\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        self.type_encoder = torch.nn.Embedding(num_nodetypes, emb_dim)\n",
        "        self.attribute_encoder = torch.nn.Embedding(num_nodeattributes, emb_dim)\n",
        "        self.depth_encoder = torch.nn.Embedding(self.max_depth + 1, emb_dim)\n",
        "\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * emb_dim, 2 * emb_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2 * emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, depth):\n",
        "        depth[depth > self.max_depth] = self.max_depth\n",
        "        mlp_input = torch.hstack(\n",
        "            (\n",
        "                self.type_encoder(x[:,0]), \n",
        "                self.attribute_encoder(x[:,1]), \n",
        "                self.depth_encoder(depth)\n",
        "             )\n",
        "        )\n",
        "        return self.node_mlp(mlp_input)"
      ],
      "metadata": {
        "id": "F5LoicGwwajW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils - AST / MLAP\n",
        "\n",
        "Utilities for editing and parsing the AST inputs."
      ],
      "metadata": {
        "id": "FamuAT6rV3G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "def get_vocab_mapping(seq_list, num_vocab):\n",
        "    '''\n",
        "        Input:\n",
        "            seq_list: a list of sequences\n",
        "            num_vocab: vocabulary size\n",
        "        Output:\n",
        "            vocab2idx:\n",
        "                A dictionary that maps vocabulary into integer index.\n",
        "                Additioanlly, we also index '__UNK__' and '__EOS__'\n",
        "                '__UNK__' : out-of-vocabulary term\n",
        "                '__EOS__' : end-of-sentence\n",
        "\n",
        "            idx2vocab:\n",
        "                A list that maps idx to actual vocabulary.\n",
        "    '''\n",
        "\n",
        "    vocab_cnt = {}\n",
        "    vocab_list = []\n",
        "    for seq in seq_list:\n",
        "        for w in seq:\n",
        "            if w in vocab_cnt:\n",
        "                vocab_cnt[w] += 1\n",
        "            else:\n",
        "                vocab_cnt[w] = 1\n",
        "                vocab_list.append(w)\n",
        "\n",
        "    cnt_list = np.array([vocab_cnt[w] for w in vocab_list])\n",
        "    topvocab = np.argsort(-cnt_list, kind = 'stable')[:num_vocab]\n",
        "\n",
        "    print('Coverage of top {} vocabulary:'.format(num_vocab))\n",
        "    print(float(np.sum(cnt_list[topvocab]))/np.sum(cnt_list))\n",
        "\n",
        "    vocab2idx = {vocab_list[vocab_idx]: idx for idx, vocab_idx in enumerate(topvocab)}\n",
        "    idx2vocab = [vocab_list[vocab_idx] for vocab_idx in topvocab]\n",
        "\n",
        "    vocab2idx['__UNK__'] = num_vocab\n",
        "    idx2vocab.append('__UNK__')\n",
        "\n",
        "    vocab2idx['__EOS__'] = num_vocab + 1\n",
        "    idx2vocab.append('__EOS__')\n",
        "\n",
        "    # test the correspondence between vocab2idx and idx2vocab\n",
        "    for idx, vocab in enumerate(idx2vocab):\n",
        "        assert(idx == vocab2idx[vocab])\n",
        "\n",
        "    # test that the idx of '__EOS__' is len(idx2vocab) - 1.\n",
        "    # This fact will be used in decode_arr_to_seq, when finding __EOS__\n",
        "    assert(vocab2idx['__EOS__'] == len(idx2vocab) - 1)\n",
        "\n",
        "    return vocab2idx, idx2vocab\n",
        "\n",
        "def augment_edge(data):\n",
        "    '''\n",
        "        Input:\n",
        "            data: PyG data object\n",
        "        Output:\n",
        "            data (edges are augmented in the following ways):\n",
        "                data.edge_index: Added next-token edge. The inverse edges were also added.\n",
        "                data.edge_attr (torch.Long):\n",
        "                    data.edge_attr[:,0]: whether it is AST edge (0) for next-token edge (1)\n",
        "                    data.edge_attr[:,1]: whether it is original direction (0) or inverse direction (1)\n",
        "    '''\n",
        "    ##### AST edge\n",
        "    edge_index_ast = data.edge_index\n",
        "    edge_attr_ast = torch.zeros((edge_index_ast.size(1), 2))\n",
        "\n",
        "    ##### Inverse AST edge\n",
        "    edge_index_ast_inverse = torch.stack([edge_index_ast[1], edge_index_ast[0]], dim = 0)\n",
        "    edge_attr_ast_inverse = torch.cat([torch.zeros(edge_index_ast_inverse.size(1), 1), torch.ones(edge_index_ast_inverse.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Next-token edge\n",
        "\n",
        "    ## Obtain attributed nodes and get their indices in dfs order\n",
        "    # attributed_node_idx = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "    # attributed_node_idx_in_dfs_order = attributed_node_idx[torch.argsort(data.node_dfs_order[attributed_node_idx].view(-1,))]\n",
        "\n",
        "    ## Since the nodes are already sorted in dfs ordering in our case, we can just do the following.\n",
        "    attributed_node_idx_in_dfs_order = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "\n",
        "    ## build next token edge\n",
        "    # Given: attributed_node_idx_in_dfs_order\n",
        "    #        [1, 3, 4, 5, 8, 9, 12]\n",
        "    # Output:\n",
        "    #    [[1, 3, 4, 5, 8, 9]\n",
        "    #     [3, 4, 5, 8, 9, 12]\n",
        "    edge_index_nextoken = torch.stack([attributed_node_idx_in_dfs_order[:-1], attributed_node_idx_in_dfs_order[1:]], dim = 0)\n",
        "    edge_attr_nextoken = torch.cat([torch.ones(edge_index_nextoken.size(1), 1), torch.zeros(edge_index_nextoken.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Inverse next-token edge\n",
        "    edge_index_nextoken_inverse = torch.stack([edge_index_nextoken[1], edge_index_nextoken[0]], dim = 0)\n",
        "    edge_attr_nextoken_inverse = torch.ones((edge_index_nextoken.size(1), 2))\n",
        "\n",
        "    data.edge_index = torch.cat([edge_index_ast, edge_index_ast_inverse, edge_index_nextoken, edge_index_nextoken_inverse], dim = 1)\n",
        "    data.edge_attr = torch.cat([edge_attr_ast,   edge_attr_ast_inverse, edge_attr_nextoken,  edge_attr_nextoken_inverse], dim = 0)\n",
        "\n",
        "    return data\n",
        "\n",
        "def encode_y_to_arr(data, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        data: PyG graph object\n",
        "        output: add y_arr to data \n",
        "    '''\n",
        "    # PyG >= 1.5.0\n",
        "    seq = data.y\n",
        "    data.y_arr = encode_seq_to_arr(seq, vocab2idx, max_seq_len)\n",
        "    return data\n",
        "\n",
        "def encode_seq_to_arr(seq, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        seq: A list of words\n",
        "        output: add y_arr (torch.Tensor)\n",
        "    '''\n",
        "    augmented_seq = seq[:max_seq_len] + ['__EOS__'] * max(0, max_seq_len - len(seq))\n",
        "    return torch.tensor([[vocab2idx[w] if w in vocab2idx else vocab2idx['__UNK__'] for w in augmented_seq]], dtype = torch.long)\n",
        "\n",
        "\n",
        "def decode_arr_to_seq(arr, idx2vocab):\n",
        "    '''\n",
        "        Input: torch 1d array: y_arr\n",
        "        Output: a sequence of words.\n",
        "    '''\n",
        "    # find the position of __EOS__ (the last vocab in idx2vocab)\n",
        "    eos_idx_list = (arr == len(idx2vocab) - 1).nonzero() \n",
        "    if len(eos_idx_list) > 0:\n",
        "        # find the smallest __EOS__\n",
        "        clippted_arr = arr[: torch.min(eos_idx_list)] \n",
        "    else:\n",
        "        clippted_arr = arr\n",
        "\n",
        "    return list(map(lambda x: idx2vocab[x], clippted_arr.cpu()))"
      ],
      "metadata": {
        "id": "a8ZPzicd-pjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils - CAP\n",
        "Utilities for generating Graph Contrastive Pairs"
      ],
      "metadata": {
        "id": "DP9s6wJ1-0Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CAP functions ----\n",
        "# from: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/functional.py\n",
        "# and: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/utils.py\n",
        "def compute_pr(edge_index, damp: float = 0.85, k: int = 10):\n",
        "    # page rank\n",
        "    # interesting comment: https://github.com/CRIPAC-DIG/GCA/issues/4\n",
        "    num_nodes = edge_index.max().item() + 1\n",
        "    deg_out = degree(edge_index[0])\n",
        "    x = torch.ones((num_nodes, )).to(edge_index.device).to(torch.float32)\n",
        "\n",
        "    for i in range(k):\n",
        "        edge_msg = x[edge_index[0]] / deg_out[edge_index[0]]\n",
        "        agg_msg = scatter(edge_msg, edge_index[1], reduce='sum')\n",
        "\n",
        "        x = (1 - damp) * x + damp * agg_msg\n",
        "\n",
        "    return x\n",
        "\n",
        "def eigenvector_centrality(data):\n",
        "    graph = to_networkx(data)\n",
        "    x = nx.eigenvector_centrality_numpy(graph)\n",
        "    x = [x[i] for i in range(data.num_nodes)]\n",
        "    return torch.tensor(x, dtype=torch.float32).to(data.edge_index.device)\n",
        "\n",
        "\n",
        "def drop_feature(x, drop_prob):\n",
        "    drop_mask = torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1) < drop_prob\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_feature_weighted(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w.repeat(x.size(0)).view(x.size(0), -1)\n",
        "\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "def drop_feature_weighted_2(x, w, p: float, threshold: float = 0.7, dgi_task=False):\n",
        "    w = w / w.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w\n",
        "\n",
        "    if (dgi_task):\n",
        "        drop_mask = torch.bernoulli(1. - drop_prob).to(torch.bool)\n",
        "    else:\n",
        "        drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "def feature_drop_weights(x, node_c):\n",
        "    x = x.to(torch.bool).to(torch.float32)\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def feature_drop_weights_dense(x, node_c):\n",
        "    x = x.abs()\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def drop_edge_weighted(edge_index, edge_weights, p: float, threshold: float = 1., dgi_task=False):\n",
        "    edge_weights = edge_weights / edge_weights.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "\n",
        "    edge_weights = edge_weights.where(edge_weights < threshold, torch.ones_like(edge_weights) * threshold)\n",
        "\n",
        "    if (dgi_task): \n",
        "        # drop edges by importance\n",
        "        sel_mask = torch.bernoulli(edge_weights).to(torch.bool)\n",
        "    else:\n",
        "        sel_mask = torch.bernoulli(1. - edge_weights).to(torch.bool)\n",
        "\n",
        "    return edge_index[:, sel_mask]\n",
        "\n",
        "\n",
        "def degree_drop_weights(edge_index):\n",
        "    edge_index_ = to_undirected(edge_index)\n",
        "    deg = degree(edge_index_[1])\n",
        "    deg_col = deg[edge_index[1]].to(torch.float32)\n",
        "    s_col = torch.log(deg_col)\n",
        "    weights = (s_col.max() - s_col) / (s_col.max() - s_col.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def pr_drop_weights(edge_index, aggr: str = 'sink', k: int = 10):\n",
        "    pv = compute_pr(edge_index, k=k)\n",
        "    pv_row = pv[edge_index[0]].to(torch.float32)\n",
        "    pv_col = pv[edge_index[1]].to(torch.float32)\n",
        "    s_row = torch.log(pv_row)\n",
        "    s_col = torch.log(pv_col)\n",
        "    if aggr == 'sink':\n",
        "        s = s_col\n",
        "    elif aggr == 'source':\n",
        "        s = s_row\n",
        "    elif aggr == 'mean':\n",
        "        s = (s_col + s_row) * 0.5\n",
        "    else:\n",
        "        s = s_col\n",
        "    weights = (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def evc_drop_weights(data):\n",
        "    evc = eigenvector_centrality(data)\n",
        "    evc = evc.where(evc > 0, torch.zeros_like(evc))\n",
        "    evc = evc + 1e-8\n",
        "    s = evc.log()\n",
        "\n",
        "    edge_index = data.edge_index\n",
        "    s_row, s_col = s[edge_index[0]], s[edge_index[1]]\n",
        "    s = s_col\n",
        "\n",
        "    return (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "def graph_perturb(data, drop_scheme='pr'):\n",
        "  if drop_scheme == 'degree':\n",
        "      drop_weights = degree_drop_weights(data.edge_index)\n",
        "      edge_index_ = to_undirected(data.edge_index)\n",
        "      node_deg = degree(edge_index_[1])\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_deg)\n",
        "  elif drop_scheme == 'pr':\n",
        "      drop_weights = pr_drop_weights(data.edge_index, aggr='sink', k=200)\n",
        "      node_pr = compute_pr(data.edge_index)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_pr)\n",
        "  elif drop_scheme == 'evc':\n",
        "      drop_weights = evc_drop_weights(data)\n",
        "      node_evc = eigenvector_centrality(data)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_evc)\n",
        "  else:\n",
        "      feature_weights = torch.ones((data.x.size(1),))\n",
        "      drop_weights = None\n",
        "  \n",
        "  return feature_weights, drop_weights\n",
        "\n",
        "def drop_edge(data, drop_edge_rate, drop_weights, drop_scheme='pr', drop_edge_weighted_threshold=0.7, dgi_task=False):\n",
        "  if drop_scheme == 'uniform':\n",
        "      return dropout_edge(data.edge_index, p=drop_edge_rate)[0]\n",
        "  elif drop_scheme in ['degree', 'evc', 'pr']:\n",
        "      return drop_edge_weighted(\n",
        "          data.edge_index, \n",
        "          drop_weights, \n",
        "          p=drop_edge_rate, \n",
        "          threshold=drop_edge_weighted_threshold,\n",
        "          dgi_task=dgi_task\n",
        "        )\n",
        "  else:\n",
        "      raise Exception(f'undefined drop scheme: {drop_scheme}')\n",
        "\n",
        "def get_contrastive_graph_pair(data, drop_scheme='pr', drop_feature_rates=(0.7, 0.7), drop_edge_rates=(0.5, 0.5), dgi_task=False):\n",
        "  # use augmentation scheme to determine the weights of each node\n",
        "  # i.e. pagerank, eigenvector centrality, node degree\n",
        "  feat_weights, drop_weights = graph_perturb(data, drop_scheme)\n",
        "\n",
        "  # apply drop edge according to computed features\n",
        "  dr_e_1, dr_e_2 = drop_edge_rates\n",
        "  edge_index_1 = drop_edge(data, dr_e_1, drop_weights, drop_scheme, dgi_task=dgi_task)\n",
        "\n",
        "  if (not dgi_task):\n",
        "    edge_index_2 = drop_edge(data, dr_e_2, drop_weights, drop_scheme)\n",
        "\n",
        "  dr_f_1, dr_f_2 = drop_feature_rates\n",
        "\n",
        "  if drop_scheme in ['pr', 'degree', 'evc']:\n",
        "    # graph-aware drop feature\n",
        "    x_1 = drop_feature_weighted_2(data.x, feat_weights, dr_f_1, dgi_task=dgi_task)\n",
        "    #e_1 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_1)\n",
        "\n",
        "    if not dgi_task:\n",
        "        x_2 = drop_feature_weighted_2(data.x, feat_weights, dr_f_2, dgi_task=dgi_task)\n",
        "        #e_2 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_2)\n",
        "  else:\n",
        "    # naive drop feature\n",
        "    x_1 = drop_feature(data.x, dr_f_1)\n",
        "    #e_1 = drop_feature(data.edge_attr, dr_f_1)\n",
        "    \n",
        "    x_2 = drop_feature(data.x, dr_f_2)\n",
        "    e_2 = drop_feature(data.edge_attr, dr_f_2)\n",
        "  \n",
        "  if dgi_task:\n",
        "      return (x_1, edge_index_1)\n",
        "\n",
        "  return (\n",
        "      # graph 1\n",
        "      (x_1, edge_index_1),\n",
        "      # graph 2\n",
        "      (x_2, edge_index_2)\n",
        "  )"
      ],
      "metadata": {
        "id": "6QXwE3BnV2vf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GIN"
      ],
      "metadata": {
        "id": "ng-TvX2UVlWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, dim_h, mlp, **kwargs):\n",
        "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.mlp = mlp\n",
        "        self.bn = BatchNorm1d(dim_h)\n",
        "        self.edge_encoder = Linear(2, dim_h)\n",
        "\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        output = self.mlp(\n",
        "            self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "        )\n",
        "        return self.bn(output)\n",
        "    \n",
        "    def message(self, x_j, edge_attr):\n",
        "        return x_j + edge_attr\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        return aggr_out + x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ],
      "metadata": {
        "id": "8u0cRmJ1VkKV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoders"
      ],
      "metadata": {
        "id": "KSAs0mAoVjXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LinearDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        \"\"\"\n",
        "        Noted in the MLAP paper to have performed better than the LSTM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [nn.Linear(dim_h, len(vocab2idx)) for _ in range(max_seq_len)]\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        return [d(layer_reps[-1]) for d in self.decoders]\n",
        "\n",
        "\n",
        "class LSTMDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "        self.lstm = nn.LSTMCell(dim_h, dim_h)\n",
        "        self.w_hc = nn.Linear(dim_h * 2, dim_h)\n",
        "        self.layernorm = nn.LayerNorm(dim_h)\n",
        "        self.vocab_encoder = nn.Embedding(len(vocab2idx), dim_h)\n",
        "        self.vocab_bias = nn.Parameter(torch.zeros(len(vocab2idx)))\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        if (training):\n",
        "            batched_label = torch.vstack(\n",
        "                [\n",
        "                    encode_seq_to_arr(label, self.vocab2idx, self.max_seq_len - 1) \n",
        "                    for label in labels\n",
        "                ]\n",
        "            )\n",
        "            batched_label = torch.hstack((torch.zeros((batch_size, 1), dtype=torch.int64), batched_label))\n",
        "            true_emb = self.vocab_encoder(batched_label.to(device=self.device))\n",
        "        \n",
        "        h_t, c_t = layer_reps[-1].clone(), layer_reps[-1].clone()\n",
        "\n",
        "        layer_reps = layer_reps.transpose(0,1)\n",
        "        output = []\n",
        "\n",
        "        pred_emb = self.vocab_encoder(torch.zeros((batch_size), dtype=torch.int64, device=self.device))\n",
        "        vocab_mat = self.vocab_encoder(torch.arange(len(self.vocab2idx), dtype=torch.int64, device=self.device))\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            if (training): \n",
        "                # teacher forcing\n",
        "                input = true_emb[:, i]\n",
        "            else:\n",
        "                input = pred_emb\n",
        "            \n",
        "            h_t, c_t = self.lstm(input, (h_t, c_t))\n",
        "\n",
        "            # (batch_size, L + 1)\n",
        "            a = F.softmax(torch.bmm(layer_reps, h_t.unsqueeze(-1)).squeeze(-1), dim=1)  \n",
        "            context = torch.bmm(a.unsqueeze(1), layer_reps).squeeze(1)\n",
        "\n",
        "            # (batch_size, dim_h)\n",
        "            pred_emb = torch.tanh(self.layernorm(self.w_hc(torch.hstack((h_t, context)))))  \n",
        "\n",
        "            # (batch_size, len(vocab)) x max_seq_len\n",
        "            output.append(torch.matmul(pred_emb, vocab_mat.T) + self.vocab_bias.unsqueeze(0))\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "JhqdRjDJVpbo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLAP"
      ],
      "metadata": {
        "id": "WBwnEmwhVxwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear, Sequential, ReLU, ELU, Sigmoid\n",
        "\n",
        "from torch_geometric.nn.conv import GINConv\n",
        "from torch_geometric.nn.norm import GraphNorm\n",
        "from torch_geometric.nn.glob import AttentionalAggregation\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class DISC(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super(DISC, self).__init__()\n",
        "        W = torch.empty(dim_h, dim_h)\n",
        "        torch.nn.init.xavier_normal_(W)\n",
        "        self.W = torch.nn.Parameter(W)\n",
        "        self.W.requires_grad = True\n",
        "        self.sig = Sigmoid()\n",
        "    \n",
        "    def forward(self, h, s):\n",
        "        out = torch.matmul(self.W, s)\n",
        "        out = torch.matmul(h, out.unsqueeze(-1))\n",
        "        return self.sig(out)\n",
        "\n",
        "\n",
        "class MLAP_GIN(torch.nn.Module):\n",
        "    def __init__(self, dim_h, batch_size, depth, node_encoder, norm=False, residual=False, dropout=False):\n",
        "        super(MLAP_GIN, self).__init__()\n",
        "\n",
        "        self.dim_h = dim_h\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "\n",
        "        self.node_encoder = node_encoder\n",
        "\n",
        "        self.norm = norm\n",
        "        self.residual = residual\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.loss_fn = torch.nn.BCELoss(reduction='sum')\n",
        "        self.discriminator = DISC(dim_h)\n",
        "\n",
        "        # non-linear projection function for cl task\n",
        "        self.projection = Sequential(\n",
        "            Linear(dim_h, int(dim_h/8)),\n",
        "            ELU(),\n",
        "            Linear(int(dim_h/8), dim_h)\n",
        "        )\n",
        "\n",
        "        # GIN layers\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [GINConv(Sequential(\n",
        "                Linear(dim_h, dim_h),\n",
        "                ReLU(),\n",
        "                Linear(dim_h, dim_h))) for _ in range(depth)])\n",
        "            \n",
        "        # normalization layers\n",
        "        self.norm = torch.nn.ModuleList([GraphNorm(dim_h) for _ in range(self.depth)])\n",
        "        \n",
        "        # layer-wise attention poolings\n",
        "        self.att_poolings = torch.nn.ModuleList(\n",
        "            [\n",
        "                AttentionalAggregation(\n",
        "                Sequential(Linear(self.dim_h, 2*self.dim_h), \n",
        "                           ReLU(), \n",
        "                           Linear(2*self.dim_h, 1))) \n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def contrastive_loss(self, g1_x, g2_x):\n",
        "        # compute projections + L2 row-wise normalizations\n",
        "        g1_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g1_x), p=2, dim=1\n",
        "        )\n",
        "        g2_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g2_x), p=2, dim=1\n",
        "        )\n",
        "        \n",
        "        g1_proj_T = torch.transpose(g1_projections, 0, 1)\n",
        "        g2_proj_T = torch.transpose(g2_projections, 0, 1)\n",
        "\n",
        "        inter_g1 = torch.exp(torch.matmul(g1_projections, g1_proj_T))\n",
        "        inter_g2 = torch.exp(torch.matmul(g2_projections, g2_proj_T))\n",
        "        intra_view = torch.exp(torch.matmul(g1_projections, g2_proj_T))\n",
        "\n",
        "        # main diagonal\n",
        "        corresponding_terms = torch.diagonal(intra_view, 0) \n",
        "        non_matching_intra = torch.diagonal(intra_view, -1).sum()\n",
        "        non_matching_inter_g1 = torch.diagonal(inter_g1, -1).sum()\n",
        "        non_matching_inter_g2 = torch.diagonal(inter_g2, -1).sum()\n",
        "\n",
        "        # inter-view pairs using g1\n",
        "        corresponding_terms_g1 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g1 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g1 = torch.log(corresponding_terms_g1)\n",
        "\n",
        "        # inter-view pairs using g2\n",
        "        corresponding_terms_g2 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g2 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g2 = torch.log(corresponding_terms_g2)\n",
        "\n",
        "        # contrasting terms of both divided by total nodes\n",
        "        loss = (\n",
        "            corresponding_terms_g1.sum() + \n",
        "            corresponding_terms_g2.sum()\n",
        "        ) / (\n",
        "            g1_x.shape[0] + \n",
        "            g2_x.shape[0]\n",
        "        )\n",
        "        \n",
        "        loss = loss / self.batch_size\n",
        "        return loss\n",
        "    \n",
        "    def layer_loop(self, x, edge_index, batch, cl=False, cl_all=False, dgi_task=False):\n",
        "        cl_embs = []\n",
        "        for d in range(self.depth):\n",
        "            x_in = x\n",
        "\n",
        "            # get node representation at layer d\n",
        "            x = self.layers[d](x, edge_index)\n",
        "            \n",
        "            if self.norm:\n",
        "                x = self.norm[d](x, batch)\n",
        "            \n",
        "            if d < self.depth - 1:\n",
        "                x = F.relu(x)\n",
        "            \n",
        "            if self.dropout:\n",
        "                x = F.dropout(x)\n",
        "            \n",
        "            if self.residual:\n",
        "                x = x + x_in\n",
        "\n",
        "            if not cl:\n",
        "                # use attention pooling for given depth\n",
        "                h_g = self.att_poolings[d](x, batch)\n",
        "                self.graph_embs.append(h_g)\n",
        "\n",
        "            if (\n",
        "                (cl and cl_all) or \n",
        "                (cl and (d == self.depth-1)) or \n",
        "                (dgi_task and (d == self.depth-1))\n",
        "            ):\n",
        "                # if using contrastive learning or DGI\n",
        "                cl_embs += [x]\n",
        "            \n",
        "        return cl_embs\n",
        "\n",
        "    def forward(self, batched_data, cl=False, cl_all=False, dgi_task=False):\n",
        "        self.graph_embs = []\n",
        "        # non-augmented graph\n",
        "        # note: populates self.graph_embs\n",
        "\n",
        "        node_depth = batched_data.node_depth\n",
        "        x_emb = self.node_encoder(batched_data.x, node_depth.view(-1,))\n",
        "        edge_index = batched_data.edge_index\n",
        "        batch = batched_data.batch\n",
        "\n",
        "        self.layer_loop(x_emb, edge_index, batch, dgi_task=dgi_task)\n",
        "\n",
        "        agg = self.aggregate()\n",
        "        self.graph_embs.append(agg)\n",
        "        output = torch.stack(self.graph_embs, dim=0)\n",
        "\n",
        "        # dgi task\n",
        "        dgi_loss = 0\n",
        "        if dgi_task:\n",
        "            # batch size // 5 to perform additional objectives\n",
        "            # only on 1/5th of the batch, for speed reasons\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                nd = g.node_depth\n",
        "                b = g.batch\n",
        "                \n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=True)\n",
        "                g_diff_embs = self.layer_loop(g1, g2, b, dgi_task=True)[0]\n",
        "\n",
        "                g.x = self.node_encoder(g.x, nd.view(-1,).clone())\n",
        "                g_embs = self.layer_loop(g.x, g.edge_index, g.batch, dgi_task=True)[0]\n",
        "\n",
        "                # dgi objective on final_layer_embs, g_diff_embs, and output\n",
        "                agg = agg.clone()\n",
        "                positive = self.discriminator(g_embs, agg[i])\n",
        "                ones = torch.ones_like(positive)\n",
        "                negative = self.discriminator(g_diff_embs, agg[i])\n",
        "                zeros = torch.zeros_like(negative)\n",
        "\n",
        "                dgi_loss += (\n",
        "                    self.loss_fn(positive, ones) + self.loss_fn(negative, zeros)\n",
        "                ) / (positive.shape[0] + negative.shape[0])\n",
        "            \n",
        "            dgi_loss /= (self.batch_size // 5)\n",
        "\n",
        "        # contrastive learning task\n",
        "        cl_loss = 0\n",
        "        if cl:\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=False)\n",
        "                g1_embs = self.get_node_embedding(g.batch, g1, cl=True, cl_all=cl_all)\n",
        "                g2_embs = self.get_node_embedding(g.batch, g2, cl=True, cl_all=cl_all)\n",
        "\n",
        "                batch_cl_loss = 0\n",
        "                for j in range(len(g1_embs)):\n",
        "                    batch_cl_loss += self.contrastive_loss(g1_embs[j], g2_embs[j])\n",
        "                \n",
        "                batch_cl_loss = batch_cl_loss / len(g1_embs)\n",
        "                cl_loss += batch_cl_loss\n",
        "            \n",
        "            cl_loss /= (self.batch_size // 5)\n",
        "\n",
        "        return output, cl_loss, dgi_loss\n",
        "\n",
        "    def get_node_embedding(self, batch, g, cl, cl_all):\n",
        "        g_x, g_edge_index = g\n",
        "        return self.layer_loop(\n",
        "            g_x.clone(), \n",
        "            g_edge_index, \n",
        "            batch, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all\n",
        "        )\n",
        "\n",
        "    def get_contrastive_pair_from_batch(self, g, dgi_task=False):\n",
        "        g_clone = g.clone()\n",
        "        nd = g.node_depth\n",
        "        # encode the nodes in the clone of g using given encoding network\n",
        "        g_clone.x = self.node_encoder(g_clone.x, nd.view(-1,).clone())\n",
        "\n",
        "        # create contrastive pairs from the input graph\n",
        "        return get_contrastive_graph_pair(g_clone, dgi_task=dgi_task)\n",
        "\n",
        "    def aggregate(self):\n",
        "        pass\n",
        "\n",
        "class MLAP_Sum(MLAP_GIN):\n",
        "    def aggregate(self):\n",
        "        return torch.stack(self.graph_embs, dim=0).sum(dim=0)\n",
        "\n",
        "class MLAP_Weighted(MLAP_GIN):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weight = torch.nn.Parameter(torch.ones(self.depth, 1, 1))\n",
        "\n",
        "    def aggregate(self):\n",
        "        a = F.softmax(self.weight, dim=0)\n",
        "        h = torch.stack(self.graph_embs, dim=0)\n",
        "        return (a * h).sum(dim=0)"
      ],
      "metadata": {
        "id": "NDywjQj9VyVd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "6fnFG6_RV714"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, batch_size, depth, dim_h, max_seq_len, node_encoder, vocab2idx, device):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "        self.dim_h = dim_h\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # token to idx lookup\n",
        "        self.vocab2idx = vocab2idx \n",
        "        \n",
        "        # architecture choices\n",
        "        self.node_encoder = node_encoder\n",
        "        self.gnn = MLAP_Weighted(\n",
        "            dim_h, batch_size, depth, \n",
        "            node_encoder, \n",
        "            norm=True, \n",
        "            residual=True, \n",
        "            dropout=True\n",
        "        )\n",
        "        self.decoder = LinearDecoder(\n",
        "            dim_h, max_seq_len, vocab2idx, device\n",
        "        )\n",
        "\n",
        "    def forward(self, batched_data, labels, training=False, cl=False, cl_all=False, dgi_task=False):\n",
        "        # GNN layer, contrastive work done here\n",
        "        embeddings, cl_loss, dgi_loss = self.gnn(\n",
        "            batched_data, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task\n",
        "        )\n",
        "\n",
        "        predictions = self.decoder(len(labels), embeddings, labels, training=training)\n",
        "\n",
        "        # for each batch, the prediction for the ith word is a logit\n",
        "        # decoding each prediction to a word is done in the evaluation task in main\n",
        "        return predictions, cl_loss, dgi_loss"
      ],
      "metadata": {
        "id": "7ft4PTPWV8eO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $nb_path_bash && mkdir \"checkpoints\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iErB7DcYCXm",
        "outputId": "5192ece8-d19d-4351-cc37-5025db1a71bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main\n",
        "\n",
        "Model configuration and training loop."
      ],
      "metadata": {
        "id": "BwGxp_PRWAMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "\n",
        "\n",
        "def train(model, device, loader, optimizer, scheduler, multicls_criterion, epoch, \n",
        "          alpha=0.05, \n",
        "          cl=False, \n",
        "          cl_all=False, \n",
        "          dgi_task=False,\n",
        "          eval_hook=lambda x: x,\n",
        "    ):\n",
        "    # total loss for this epoch\n",
        "    loss_accum = 0\n",
        "\n",
        "    chkpt_folder = nb_path + '/checkpoints/epoch' + str(epoch)\n",
        "    if not os.path.exists(chkpt_folder):\n",
        "        os.mkdir(chkpt_folder)\n",
        "\n",
        "    if cl and dgi_task:\n",
        "        raise Exception(\"Cannot use both a contrastive and dgi loss term\\n\")\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        # run eval if requested\n",
        "        eval_hook(step)\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            # train\n",
        "            labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "            pred_list, cl_loss, dgi_loss = model(\n",
        "                batch, labels, training=True,\n",
        "                cl=cl, \n",
        "                cl_all=cl_all, \n",
        "                dgi_task=dgi_task\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss + update\n",
        "            loss = 0\n",
        "            for i in range(len(pred_list)):\n",
        "                loss += (1-alpha) * multicls_criterion(\n",
        "                    pred_list[i].to(torch.float32), \n",
        "                    batch.y_arr[:, i]\n",
        "                )\n",
        "\n",
        "            loss /= len(pred_list)\n",
        "            if cl:\n",
        "                loss -= alpha * cl_loss\n",
        "            if dgi_task:\n",
        "                loss -= alpha * dgi_loss\n",
        "\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # report loss after batch\n",
        "            loss_accum += loss.item()\n",
        "            print('Average loss after batch ' + str(step) + ': ' + str(loss_accum / (step + 1)))\n",
        "            print(f\"\\tContrastive Term: {cl_loss:.3f}\")\n",
        "        \n",
        "        if ((step+1) % 35 == 0 or step == len(loader)-1): \n",
        "            # save model after every 35 batches\n",
        "            print(\"Checkpoint saved.\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': loss_accum / (step + 1),\n",
        "            }, chkpt_folder + '/model' + str((step+1) // 35) + '.pt')\n",
        "\n",
        "    # end of this epoch\n",
        "    print('Average training loss: {}'.format(loss_accum / (step + 1)))\n",
        "    return loss_accum / (step + 1)\n",
        "\n",
        "def eval(model, device, loader, evaluator, arr_to_seq):\n",
        "    \"\"\"\n",
        "    Use official OGB evaluator to test results of model output\n",
        "    \"\"\"\n",
        "    seq_ref_list = []\n",
        "    seq_pred_list = []\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "                # no cl by default\n",
        "                pred_list, _, _ = model(batch, labels) \n",
        "\n",
        "            mat = []\n",
        "            for i in range(len(pred_list)):\n",
        "                # get model's predictions\n",
        "                mat.append(torch.argmax(pred_list[i], dim=1).view(-1, 1))\n",
        "            \n",
        "            # save for eval\n",
        "            seq_ref_list.extend(labels)\n",
        "            mat = torch.cat(mat, dim=1)\n",
        "            seq_pred = [arr_to_seq(arr) for arr in mat]\n",
        "            seq_pred_list.extend(seq_pred)\n",
        "\n",
        "    input_dict = {\"seq_ref\": seq_ref_list, \"seq_pred\": seq_pred_list}\n",
        "    return evaluator.eval(input_dict)\n",
        "\n",
        "def randomly_mask(dataset, size):\n",
        "    bool_mask = np.zeros(len(dataset), dtype=bool)\n",
        "    bool_mask[:size] = True\n",
        "    np.random.shuffle(bool_mask)\n",
        "    out = dataset[bool_mask]\n",
        "    return out\n",
        "\n",
        "\n",
        "def main(\n",
        "      starting_chkpt=None, \n",
        "      cl=False, \n",
        "      cl_all=False, \n",
        "      dgi_task=False, \n",
        "      run_eval_every_n_batches=None, \n",
        "      # CL hyperparameter\n",
        "      alpha=0.05\n",
        "  ):\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # model & training conf\n",
        "    depth = 3\n",
        "    epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    step_size = 10\n",
        "    decay_rate = 0.1\n",
        "    weight_decay = 0.00005\n",
        "    dim_h = 512\n",
        "\n",
        "    # model initialization\n",
        "    node_encoder = ASTNodeEncoder(\n",
        "        dim_h, \n",
        "        num_nodetypes=len(nodetypes_mapping['type']), \n",
        "        num_nodeattributes=len(nodeattributes_mapping['attr']), \n",
        "        max_depth=20\n",
        "    )\n",
        "    model = Model(\n",
        "        batch_size, \n",
        "        depth, \n",
        "        dim_h, \n",
        "        max_seq_len, \n",
        "        node_encoder, \n",
        "        vocab2idx, \n",
        "        DEVICE\n",
        "    ).to(DEVICE)\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model # Params: {num_params}')\n",
        "    print(\"-------------\\n\\n\\n\")\n",
        "\n",
        "    # training configuration\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)\n",
        "    multicls_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    starting_epoch = 1\n",
        "\n",
        "    if (starting_chkpt != None):\n",
        "        checkpoint = torch.load(starting_chkpt)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        starting_epoch = checkpoint['epoch']\n",
        "\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_curve = []\n",
        "    trainL_curve = []\n",
        "\n",
        "    def eval_hook():\n",
        "      print('Evaluating...')\n",
        "      train_perf = eval(model, DEVICE, train_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "      valid_perf = eval(model, DEVICE, valid_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "      test_perf = eval(model, DEVICE, test_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "\n",
        "      print(\n",
        "          'Train:', train_perf[dataset.eval_metric],\n",
        "          'Validation:', valid_perf[dataset.eval_metric],\n",
        "          'Test:', test_perf[dataset.eval_metric]\n",
        "      )\n",
        "      \n",
        "      return train_perf, valid_perf, test_perf\n",
        "\n",
        "    for epoch in range(starting_epoch, epochs + 1):\n",
        "        print (datetime.datetime.now().strftime('%Y.%m.%d-%H:%M:%S'))\n",
        "        print(\"Epoch {} training...\".format(epoch))\n",
        "        print (\"lr: \", optimizer.param_groups[0]['lr'])\n",
        "        \n",
        "        # training model\n",
        "        train_loss = train(\n",
        "            model, \n",
        "            DEVICE, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            multicls_criterion, \n",
        "            epoch, \n",
        "            cl=cl, \n",
        "            alpha=alpha,\n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task,\n",
        "            # run evaluation every n batches\n",
        "            eval_hook=lambda x: (\n",
        "                eval_hook() \n",
        "                  if run_eval_every_n_batches is not None and \n",
        "                  (x != 0 and x % run_eval_every_n_batches == 0) \n",
        "                else None\n",
        "            ),\n",
        "        )\n",
        "        scheduler.step()\n",
        "\n",
        "        # run evaluation after each epoch anyways\n",
        "        train_perf, valid_perf, test_perf = eval_hook()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss}\")\n",
        "\n",
        "        train_curve.append(train_perf[dataset.eval_metric])\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "        trainL_curve.append(train_loss)\n",
        "\n",
        "    print('F1')\n",
        "    best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    best_train = max(train_curve)\n",
        "    print('Finished training!')\n",
        "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
        "    print('Finished test: {}, Validation: {}, Train: {}, epoch: {}, best train: {}, best loss: {}'\n",
        "          .format(\n",
        "              test_curve[best_val_epoch], \n",
        "              valid_curve[best_val_epoch], \n",
        "              train_curve[best_val_epoch],\n",
        "              best_val_epoch, \n",
        "              best_train, \n",
        "              min(trainL_curve)\n",
        "          )\n",
        "    )"
      ],
      "metadata": {
        "id": "mh7dpE0hWAx3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_vocab = 5000\n",
        "max_seq_len = 5\n",
        "batch_size = 50\n",
        "\n",
        "# dataset objects\n",
        "# best to load these only once in colab\n",
        "# otherwise, memory never freed and runtime crashes\n",
        "dataset_name = \"ogbg-code2\"\n",
        "dataset = PygGraphPropPredDataset(dataset_name)\n",
        "evaluator = Evaluator(dataset_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "vocab2idx, idx2vocab = get_vocab_mapping([dataset.data.y[i] for i in split_idx['train']], num_vocab)\n",
        "dataset.transform = transforms.Compose([augment_edge, lambda data: encode_y_to_arr(data, vocab2idx, max_seq_len)])\n",
        "\n",
        "nodetypes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'typeidx2type.csv.gz'))\n",
        "nodeattributes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'attridx2attr.csv.gz'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmenm81UkSeJ",
        "outputId": "925a2e6e-b25a-4b5f-f109-26c2a7c3ece5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage of top 5000 vocabulary:\n",
            "0.9025832389087423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_training = randomly_mask(dataset[split_idx[\"train\"]], batch_size * 800)\n",
        "full_valid = randomly_mask(dataset[split_idx[\"valid\"]], batch_size * 800)\n",
        "full_test = randomly_mask(dataset[split_idx[\"test\"]], batch_size * 800)\n",
        "\n",
        "train_loader = DataLoader(full_training, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(full_valid, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(full_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EoDIrwXl9UUY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Leaderboard on OBG for code2: [LINK](https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-code2)"
      ],
      "metadata": {
        "id": "EXBqVk1Ap4QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import k_hop_subgraph\n",
        "import numpy as np\n",
        "\n",
        "def get_d(H):\n",
        "  \"\"\"\n",
        "  Pairwise Cosine Distance between nodes\n",
        "  \"\"\"\n",
        "  # asssuming row dimension are nodes\n",
        "  # column dim are hidden dimension\n",
        "  normed = torch.nn.functional.normalize(H, dim=1)\n",
        "  return (1 - normed @ normed.T)\n",
        "\n",
        "def get_mad(H, mask):\n",
        "  D = get_d(H)\n",
        "  n = torch.count_nonzero(D[mask])\n",
        "  return (D[mask].sum() / n).item()\n",
        "\n",
        "def get_mad_global(H):\n",
        "  # MAD: mean average distance\n",
        "  D = get_d(H)\n",
        "  return D.mean().item()\n",
        "\n",
        "def get_all_node_ids(num_nodes):\n",
        "  return torch.linspace(0, num_nodes - 1, steps=num_nodes, dtype=int)\n",
        "\n",
        "def get_mad_gap(node_id, embedding, data):\n",
        "  # MADgap\n",
        "  # find nodes that are 3 or fewer edges away for MAD_neb\n",
        "  neb, _, _, _ = k_hop_subgraph(node_id, 3, data.edge_index)\n",
        "  mad_neb = get_mad(embedding, neb)\n",
        "  \n",
        "  # find nodes that are 8 or more edges away for MAD_rmt\n",
        "  subset, _, _, _ = k_hop_subgraph(node_id, 7, data.edge_index)\n",
        "  \n",
        "  # get compliment of nodes within 7 steps of node_id\n",
        "  num_nodes = data.x.shape[0]\n",
        "  all_node_ids = get_all_node_ids(num_nodes)\n",
        "  mask = torch.ones_like(all_node_ids, dtype=bool)\n",
        "  mask[subset] = 0\n",
        "  rmt = all_node_ids[mask]\n",
        "\n",
        "  mad_rmt = get_mad(embedding, rmt)\n",
        "\n",
        "  return (mad_rmt - mad_neb)\n",
        "\n",
        "def randomly_sample_node_ids(num_samples, data):\n",
        "    num_nodes = data.x.shape[0]\n",
        "    all_node_ids = get_all_node_ids(num_nodes)\n",
        "    bool_mask = np.zeros_like(all_node_ids, dtype=bool)\n",
        "    bool_mask[:num_samples] = True\n",
        "    np.random.shuffle(bool_mask)\n",
        "    return all_node_ids[bool_mask]\n",
        "\n",
        "def get_sampled_mad_gap(num_samples, data):\n",
        "  \"\"\"\n",
        "  According to our assumption, large MADGap value indicates that the \n",
        "  useful information received by the node is more than noise. ...\n",
        "  On the contrary, small or negative MADGap means over-smoothing and \n",
        "  inferior performance. - Chen, Lin, Li, et. al (2019) pp. 4\n",
        "  \"\"\"\n",
        "  random_sample = randomly_sample_node_ids(num_samples, data)\n",
        "  mad_gaps = []\n",
        "  for node_id in random_sample:\n",
        "    mad_gaps.append(get_mad_gap(node_id.item(), embedding, data))\n",
        "  return np.array(mad_gaps).mean()\n",
        "\n",
        "def get_global_mad_gap(data, perc_to_sample=1.0):\n",
        "  num_samples = int(data.x.shape[0] * perc_to_sample)\n",
        "  return get_sampled_mad_gap(num_samples, data)"
      ],
      "metadata": {
        "id": "m4OYId2w8qMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for multi-layer CL to be enabled, cl and cl_all must both be True\n",
        "# default alpha is 0.05\n",
        "\n",
        "# very overfit\n",
        "# Train: 0.20 Validation: 0.069 Test: 0.072\n",
        "main(\n",
        "  cl=True, \n",
        "  # may anticipate better performance with only CL on final representations\n",
        "  cl_all=False, \n",
        "  dgi_task=False, \n",
        "  run_eval_every_n_batches=300, \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHPw1q10Xfvq",
        "outputId": "35953846-8e7b-4dcc-efbc-5bfdcfe538d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Checkpoint saved.\n",
            "Average loss after batch 385: 1.6673996593668052\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 386: 1.6672562305317369\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 387: 1.6682630761996986\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 388: 1.6681176044027726\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 389: 1.6685305531208332\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 390: 1.6683163630687976\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 391: 1.6688500432943811\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 392: 1.6686714293089229\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 393: 1.6683756533613059\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 394: 1.6690472219563737\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 395: 1.6692286058507784\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 396: 1.6689926016540912\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 397: 1.6693667146428746\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 398: 1.669382149414311\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 399: 1.6692710417509078\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 400: 1.6697093405925723\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 401: 1.6699647120575407\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 402: 1.6702101313446651\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 403: 1.6710296125695256\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 404: 1.6714008799305669\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 405: 1.6719448636905314\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 406: 1.6720647911474804\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 407: 1.67272476968812\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 408: 1.6728628328785045\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 409: 1.6736006713495022\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 410: 1.6735604424256187\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 411: 1.673898774443321\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 412: 1.6739175074325636\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 413: 1.6739863874255747\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 414: 1.673500432163836\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 415: 1.6735527197328897\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 416: 1.6737036785061696\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 417: 1.6738060547404312\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 418: 1.6741088344259876\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 419: 1.6741008508773076\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 420: 1.6738023058535652\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 421: 1.6745122374516528\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 422: 1.674583364603931\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 423: 1.6752517226169694\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 424: 1.674960556310766\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 425: 1.6751122303971662\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 426: 1.6753617178919165\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 427: 1.6757913463026564\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 428: 1.6757950474332262\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 429: 1.6760281981423844\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 430: 1.6765089239709061\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 431: 1.6770100957817502\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 432: 1.6773072153536333\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 433: 1.6771532950862762\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 434: 1.6780851906743544\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 435: 1.678222194177295\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 436: 1.6785347366223893\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 437: 1.678793876138452\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 438: 1.6795967684248314\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 439: 1.679874653707851\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 440: 1.6797214981650008\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 441: 1.6797153868826267\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 442: 1.6796625419341145\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 443: 1.6800538763269648\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 444: 1.6799800436148482\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 445: 1.679506408259473\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 446: 1.679770642478994\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 447: 1.6796508146716016\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 448: 1.680323524836177\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 449: 1.6807542271084255\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 450: 1.6803911451225535\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 451: 1.6808434176234017\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 452: 1.681151210077551\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 453: 1.681277615383333\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 454: 1.6810902019123455\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 455: 1.6814043644750327\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 456: 1.6818286670897669\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 457: 1.681332596524834\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 458: 1.681371081888286\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 459: 1.6813695295997289\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 460: 1.6817599462583628\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 461: 1.6819202469024823\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 462: 1.682025390876292\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 463: 1.6821824905687366\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 464: 1.6825105010822254\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 465: 1.6824313087524774\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 466: 1.6821161822537538\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 467: 1.6821735139585967\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 468: 1.6825780184792558\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 469: 1.6829036347409512\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 470: 1.6829056808143665\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 471: 1.683062152842344\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 472: 1.683113749133104\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 473: 1.6837595017147466\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 474: 1.6838440967860975\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 475: 1.6847422871769977\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 476: 1.6850323687059574\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 477: 1.6850727165593262\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 478: 1.6855232812666445\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 479: 1.685625361899535\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 480: 1.6860104786878811\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 481: 1.6861411926657333\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 482: 1.6858721502572607\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 483: 1.6859942401243635\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 484: 1.6861204688082037\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 485: 1.6862926885424327\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 486: 1.6865006418443558\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 487: 1.6871219445936014\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 488: 1.6873084411543084\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 489: 1.6871415522633766\n",
            "\tContrastive Term: -0.079\n",
            "Checkpoint saved.\n",
            "Average loss after batch 490: 1.6871261436439096\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 491: 1.6872631808121998\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 492: 1.6878435145043456\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 493: 1.6885680092973747\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 494: 1.6887592633565267\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 495: 1.6889018897087342\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 496: 1.6890330400985012\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 497: 1.689015834446413\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 498: 1.6895880531930254\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 499: 1.689704238653183\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 500: 1.6897556995917222\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 501: 1.6898510553447375\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 502: 1.6900260332327477\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 503: 1.6902196461719179\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 504: 1.6906205547918187\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 505: 1.6910716283462734\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 506: 1.6916241156749237\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 507: 1.6920952329954764\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 508: 1.6919755392318157\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 509: 1.6920010519962685\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 510: 1.692430062536624\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 511: 1.6926021766848862\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 512: 1.6928411292286178\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 513: 1.6926024864619809\n",
            "\tContrastive Term: -0.066\n",
            "Average loss after batch 514: 1.692931265738404\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 515: 1.6933181013247764\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 516: 1.6933097989231751\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 517: 1.6934542554685967\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 518: 1.693548188962918\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 519: 1.6935155322918525\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 520: 1.6937763372904509\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 521: 1.6936117883386284\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 522: 1.6935998365482454\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 523: 1.6941447025946987\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 524: 1.6946594887688047\n",
            "\tContrastive Term: -0.086\n",
            "Checkpoint saved.\n",
            "Average loss after batch 525: 1.6951070010888714\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 526: 1.6949926135209752\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 527: 1.6949182498184117\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 528: 1.6953088225398938\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 529: 1.6954458452620595\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 530: 1.6957593992380549\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 531: 1.6963458106033784\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 532: 1.6967917316179115\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 533: 1.6966239028655634\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 534: 1.6972109088273806\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 535: 1.6973849628398667\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 536: 1.6977696154592645\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 537: 1.6982756119678455\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 538: 1.6985133990286012\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 539: 1.6986245724889968\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 540: 1.699097484405292\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 541: 1.6996184112401027\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 542: 1.6997330008729825\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 543: 1.7002220429918344\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 544: 1.700160328401338\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 545: 1.7008764879170792\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 546: 1.7010423751374268\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 547: 1.701158598391679\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 548: 1.701491490305013\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 549: 1.7013199019432068\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 550: 1.7012059829196133\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 551: 1.7015542085619941\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 552: 1.701636256619776\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 553: 1.7020450589458864\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 554: 1.7024528670955348\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 555: 1.7026498437785416\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 556: 1.702819508538854\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 557: 1.7030852090927862\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 558: 1.7033475808039547\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 559: 1.703359491271632\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 560: 1.703150567523936\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 561: 1.7032109189288047\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 562: 1.7035184027246641\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 563: 1.7037564672476857\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 564: 1.7041158347003227\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 565: 1.7042041528351315\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 566: 1.7042818969309435\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 567: 1.70430882673868\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 568: 1.7040700005311749\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 569: 1.7040516545898037\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 570: 1.704254619711961\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 571: 1.7041827402748428\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 572: 1.70393507705309\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 573: 1.7037473115356125\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 574: 1.7034903779237167\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 575: 1.7035957568635542\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 576: 1.7043529207619679\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 577: 1.7046044646249923\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 578: 1.704801541947742\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 579: 1.7047592646089094\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 580: 1.7050828262778854\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 581: 1.7053581381991147\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 582: 1.705464855066499\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 583: 1.7060209284090015\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 584: 1.7064029334956765\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 585: 1.7065697090617626\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 586: 1.7066362486990387\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 587: 1.7071212901144612\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 588: 1.7075303440790226\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 589: 1.7080148365538\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 590: 1.7084871956136223\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 591: 1.7086912333159834\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 592: 1.7082518994506657\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 593: 1.7084221251885898\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 594: 1.7084965677822337\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 595: 1.7087046792043135\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 596: 1.7087393356128353\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 597: 1.7088156537866115\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 598: 1.7086505408279087\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 599: 1.7090136086940766\n",
            "\tContrastive Term: -0.077\n",
            "Evaluating...\n",
            "Train: 0.3028920121545121 Validation: 0.08547105234881892 Test: 0.09197930812884339\n",
            "Average loss after batch 600: 1.7092418303703905\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 601: 1.709334634863261\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 602: 1.7096392455978773\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 603: 1.7099154644849284\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 604: 1.7107346319955243\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 605: 1.7108313669465949\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 606: 1.7109996211980478\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 607: 1.7108723981992195\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 608: 1.7112665779289158\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 609: 1.711212335844509\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 610: 1.7114228515500132\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 611: 1.7115879070525075\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 612: 1.711823300475203\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 613: 1.711413838381876\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 614: 1.7116637714510041\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 615: 1.71169704779402\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 616: 1.7118547613848742\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 617: 1.711996736457047\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 618: 1.7118837970138945\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 619: 1.7118547812584908\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 620: 1.7119225305827535\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 621: 1.7119680531921877\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 622: 1.71186466488754\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 623: 1.7120104260169542\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 624: 1.7123480682373047\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 625: 1.7120218457886205\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 626: 1.7118202647524017\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 627: 1.7118791146263195\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 628: 1.7118066486758914\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 629: 1.7119310990212455\n",
            "\tContrastive Term: -0.070\n",
            "Checkpoint saved.\n",
            "Average loss after batch 630: 1.7121244141870369\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 631: 1.7120361184772057\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 632: 1.71210619529465\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 633: 1.712209363841108\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 634: 1.7122827834031713\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 635: 1.712061286909775\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 636: 1.7121537512101035\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 637: 1.7122999665131764\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 638: 1.7125949915585943\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 639: 1.712975939363241\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 640: 1.7130064620242662\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 641: 1.7134118037431783\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 642: 1.7138528821998584\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 643: 1.7141318463760873\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 644: 1.713995188705681\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 645: 1.714114646793519\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 646: 1.713908142247561\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 647: 1.7140414273297344\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 648: 1.7142502356016398\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 649: 1.714149118570181\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 650: 1.7144170560045726\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 651: 1.714530382785329\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 652: 1.7145792605500856\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 653: 1.7147926823079405\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 654: 1.7147329334084314\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 655: 1.7150834046485948\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 656: 1.7152440544314218\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 657: 1.7150674556526369\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 658: 1.7149952163464743\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 659: 1.7150873747738924\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 660: 1.7150435554097532\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 661: 1.7151632013637854\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 662: 1.7149603960201212\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 663: 1.7150964891336051\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 664: 1.7151376543188455\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 665: 1.7153105098563988\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 666: 1.7152884794675607\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 667: 1.715585285496569\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 668: 1.7161375341215832\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 669: 1.7162392788858556\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 670: 1.7160660250531579\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 671: 1.7160998882637137\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 672: 1.7160365810011684\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 673: 1.7163538791305588\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 674: 1.7162394384101585\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 675: 1.7160425625256532\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 676: 1.7160394557997958\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 677: 1.7160479380669496\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 678: 1.7166138929541226\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 679: 1.7164899670025882\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 680: 1.7165847939716736\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 681: 1.7167753652393642\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 682: 1.7167709226315053\n",
            "\tContrastive Term: -0.067\n",
            "Average loss after batch 683: 1.7170437959551115\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 684: 1.717238461014128\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 685: 1.7173879374220489\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 686: 1.7172649843724013\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 687: 1.7175761920421622\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 688: 1.7178177619015012\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 689: 1.7184996366500855\n",
            "\tContrastive Term: -0.067\n",
            "Average loss after batch 690: 1.7188615491869825\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 691: 1.7188428822969426\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 692: 1.7186572544846528\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 693: 1.7185213859555357\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 694: 1.7187153944866262\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 695: 1.719114429820543\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 696: 1.7191241678582716\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 697: 1.719435891483438\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 698: 1.7197035934792055\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 699: 1.7196439296858652\n",
            "\tContrastive Term: -0.071\n",
            "Checkpoint saved.\n",
            "Average loss after batch 700: 1.7196658038548838\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 701: 1.7196209316919333\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 702: 1.7197276043518848\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 703: 1.7199692494151266\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 704: 1.7199410808847306\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 705: 1.720147235873063\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 706: 1.7201751199099098\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 707: 1.7203689096337658\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 708: 1.7205224966969241\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 709: 1.7207089326751064\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 710: 1.7210315137305172\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 711: 1.7210985671603278\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 712: 1.721087693332121\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 713: 1.7208066781361897\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 714: 1.7207222851839932\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 715: 1.720684154406606\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 716: 1.7210264018057446\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 717: 1.7214842405491884\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 718: 1.7217002578172962\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 719: 1.7218000426888467\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 720: 1.7220306072420286\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 721: 1.7219861404057024\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 722: 1.7220702067450369\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 723: 1.7221689947072971\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 724: 1.722261446755508\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 725: 1.7224121032995954\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 726: 1.7220333068538072\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 727: 1.7220607118292168\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 728: 1.7225013345030273\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 729: 1.7227690997189038\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 730: 1.7229115858612896\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 731: 1.723307293295209\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 732: 1.723550917669663\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 733: 1.7237814973420602\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 734: 1.7239687467107967\n",
            "\tContrastive Term: -0.088\n",
            "Checkpoint saved.\n",
            "Average loss after batch 735: 1.7242397350137648\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 736: 1.724648395257533\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 737: 1.7248682108351854\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 738: 1.72517379888501\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 739: 1.725425854083654\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 740: 1.7255283894970028\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 741: 1.7256746073617446\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 742: 1.725910189976442\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 743: 1.7258499607604036\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 744: 1.7264553796524968\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 745: 1.7267881945055228\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 746: 1.7271527576957202\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 747: 1.7273214998092243\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 748: 1.7272668220649892\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 749: 1.7271232018470764\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 750: 1.7270674386449882\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 751: 1.7272585703337446\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 752: 1.7271639187814072\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 753: 1.7276802855081836\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 754: 1.7281418811406521\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 755: 1.728504269999802\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 756: 1.7287757798261656\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 757: 1.7290649198605077\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 758: 1.7289723734296514\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 759: 1.7289746995034971\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 760: 1.7291352966611864\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 761: 1.7289462339846793\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 762: 1.7289952397502735\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 763: 1.7292425442116424\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 764: 1.729594811582877\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 765: 1.7296007488788574\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 766: 1.7296282492840305\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 767: 1.7297040469323595\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 768: 1.7296895661496683\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 769: 1.729703984322486\n",
            "\tContrastive Term: -0.082\n",
            "Checkpoint saved.\n",
            "Average loss after batch 770: 1.7300290274403594\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 771: 1.7300348482601384\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 772: 1.730018899635078\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 773: 1.7303447483121888\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 774: 1.7306475311709988\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 775: 1.730598207601567\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 776: 1.7310523480522126\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 777: 1.7310751034545409\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 778: 1.7312460847017241\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 779: 1.7313968786826501\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 780: 1.7318198870147237\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 781: 1.7319656134871266\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 782: 1.731985683489881\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 783: 1.7320732831650851\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 784: 1.7319371425422134\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 785: 1.7321611821803125\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 786: 1.7322365628718723\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 787: 1.7323859995089206\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 788: 1.7326725215210812\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 789: 1.732709310326395\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 790: 1.7327994768297024\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 791: 1.7329785018557249\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 792: 1.732778748227248\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 793: 1.7330107685901055\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 794: 1.7331928307155393\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 795: 1.7333667842886555\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 796: 1.7335413618099735\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 797: 1.7333570912965857\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 798: 1.7332340190944744\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 799: 1.733413321375847\n",
            "\tContrastive Term: -0.073\n",
            "Checkpoint saved.\n",
            "Average training loss: 1.733413321375847\n",
            "Evaluating...\n",
            "Train: 0.28688382894882897 Validation: 0.08540023669094098 Test: 0.09354112212231401\n",
            "Train Loss: 1.733413321375847\n",
            "2023.04.25-00:27:25\n",
            "Epoch 8 training...\n",
            "lr:  0.001\n",
            "Average loss after batch 0: 1.224849820137024\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 1: 1.2958913445472717\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 2: 1.3056546847025554\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 3: 1.325347900390625\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 4: 1.3388262033462524\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 5: 1.3312590916951497\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 6: 1.3543058122907365\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 7: 1.3342378437519073\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 8: 1.3387447198232014\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 9: 1.345004630088806\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 10: 1.3433315537192605\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 11: 1.3226757049560547\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 12: 1.3181523084640503\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 13: 1.307288876601628\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 14: 1.3352656761805217\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 15: 1.3412568792700768\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 16: 1.3342749511494356\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 17: 1.3375800649325054\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 18: 1.335668538746081\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 19: 1.3331373751163482\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 20: 1.3300889446621849\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 21: 1.3245418505235151\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 22: 1.3235107867614082\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 23: 1.3182185937960942\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 24: 1.3213329267501832\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 25: 1.3208016523948083\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 26: 1.3162564480746235\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 27: 1.3192432267325265\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 28: 1.3176397249616425\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 29: 1.3149601419766743\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 30: 1.3222189872495589\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 31: 1.3205075524747372\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 32: 1.3175653833331484\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 33: 1.3151614946477554\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 34: 1.3137141159602574\n",
            "\tContrastive Term: -0.087\n",
            "Checkpoint saved.\n",
            "Average loss after batch 35: 1.3159520758522882\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 36: 1.3171824474592466\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 37: 1.3197541644698696\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 38: 1.3179729382197063\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 39: 1.320177549123764\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 40: 1.3200849294662476\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 41: 1.3220120214280628\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 42: 1.3245004997696987\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 43: 1.3188721510497006\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 44: 1.3214298566182454\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 45: 1.321607327979544\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 46: 1.3257997695435868\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 47: 1.3290271610021591\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 48: 1.3311733348029\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 49: 1.3288944149017334\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 50: 1.3284305287342446\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 51: 1.3330960021569178\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 52: 1.330904168902703\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 53: 1.3332710288189076\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 54: 1.3317699800838123\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 55: 1.3311672508716583\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 56: 1.33304076236591\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 57: 1.3372073769569397\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 58: 1.3387488813723547\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 59: 1.3405633509159087\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 60: 1.341819133914885\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 61: 1.3404055776134614\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 62: 1.3412986104450528\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 63: 1.3379626367241144\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 64: 1.3398539543151855\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 65: 1.340018006888303\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 66: 1.3438566887556618\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 67: 1.340190317700891\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 68: 1.3417507807413738\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 69: 1.3430683016777039\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 70: 1.3447055094678637\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 71: 1.344920857085122\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 72: 1.3482966716975382\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 73: 1.3469508918556008\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 74: 1.3464605220158894\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 75: 1.347287926234697\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 76: 1.346944141697574\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 77: 1.347364061918014\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 78: 1.3477023251449005\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 79: 1.3445909544825554\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 80: 1.3453860032705613\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 81: 1.3461731018089667\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 82: 1.345736345612859\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 83: 1.344423661629359\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 84: 1.3461086301242604\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 85: 1.3453499134196791\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 86: 1.347287083494252\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 87: 1.3478607535362244\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 88: 1.3476143191369732\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 89: 1.346754182709588\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 90: 1.3462428810832265\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 91: 1.3500638604164124\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 92: 1.3471766043734807\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 93: 1.3478655434669333\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 94: 1.3475923073919196\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 95: 1.349795492986838\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 96: 1.3496906241190803\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 97: 1.3511703196836977\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 98: 1.3510616981621943\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 99: 1.3512487030029297\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 100: 1.351449174456077\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 101: 1.3508113131803625\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 102: 1.352861491221826\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 103: 1.3524193866894796\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 104: 1.3507377283913748\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 105: 1.3508795004970622\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 106: 1.3511282718070199\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 107: 1.351804550047274\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 108: 1.3520343741145702\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 109: 1.3519452051682905\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 110: 1.3536035982338157\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 111: 1.3538045489362307\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 112: 1.354193560845029\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 113: 1.352772950080403\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 114: 1.3529499396033908\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 115: 1.353483927660975\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 116: 1.353929432029398\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 117: 1.355194786847648\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 118: 1.3547571025976615\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 119: 1.3551003515720368\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 120: 1.3526980965590674\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 121: 1.3513168575333767\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 122: 1.352177439666376\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 123: 1.351580613082455\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 124: 1.3533217391967773\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 125: 1.3524145275827437\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 126: 1.3525262639278501\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 127: 1.3546704184263945\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 128: 1.3542793333068375\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 129: 1.3539321367557233\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 130: 1.3542788529214058\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 131: 1.3542973787495585\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 132: 1.3543571430937689\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 133: 1.356659459533976\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 134: 1.3555259024655377\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 135: 1.3566627695279962\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 136: 1.3580322256923592\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 137: 1.3582604562026868\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 138: 1.3564851086774319\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 139: 1.3568736391408103\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 140: 1.3552447709631412\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 141: 1.3559803736041969\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 142: 1.355811595916748\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 143: 1.3564823816219966\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 144: 1.357133281641993\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 145: 1.3575861698960605\n",
            "\tContrastive Term: -0.067\n",
            "Average loss after batch 146: 1.35874786749989\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 147: 1.357778404210065\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 148: 1.3583296633406774\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 149: 1.3590890828768412\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 150: 1.3584487738198792\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 151: 1.35788494819089\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 152: 1.3589373032251995\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 153: 1.3584216322217668\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 154: 1.3588552482666507\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 155: 1.3599717074479811\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 156: 1.3608506819245163\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 157: 1.361441805392881\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 158: 1.3608318469809286\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 159: 1.3628727346658707\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 160: 1.3616188468399995\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 161: 1.3603184627897946\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 162: 1.3616816390511448\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 163: 1.362760686292881\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 164: 1.363761307253982\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 165: 1.3647222281938576\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 166: 1.3662132196083754\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 167: 1.3681323783738273\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 168: 1.3686503044952303\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 169: 1.3689157955786762\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 170: 1.3684642642562153\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 171: 1.3686025787231535\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 172: 1.3698570590487795\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 173: 1.3702753843932316\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 174: 1.370002179145813\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 175: 1.37031874331561\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 176: 1.3706187851684914\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 177: 1.370454977737384\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 178: 1.37199607041961\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 179: 1.372797421614329\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 180: 1.3743273355684227\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 181: 1.3729611595908364\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 182: 1.3729271133089327\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 183: 1.3729289925616721\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 184: 1.3734999508471102\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 185: 1.3732156324130234\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 186: 1.3731662117861172\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 187: 1.3741953487091876\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 188: 1.374498067078767\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 189: 1.3753024000870555\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 190: 1.374790573619423\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 191: 1.3747127366562684\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 192: 1.3752773765455255\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 193: 1.3763650729484165\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 194: 1.3770535352902535\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 195: 1.377074497694872\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 196: 1.3780615172410375\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 197: 1.377748981268719\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 198: 1.378945465063929\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 199: 1.3785560566186905\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 200: 1.378983149481057\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 201: 1.3797024754014346\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 202: 1.3797458316305\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 203: 1.3789174930722106\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 204: 1.3805625560807018\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 205: 1.3797020159878777\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 206: 1.3785088194741144\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 207: 1.3786975666880608\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 208: 1.3786882914994891\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 209: 1.3803404092788696\n",
            "\tContrastive Term: -0.082\n",
            "Checkpoint saved.\n",
            "Average loss after batch 210: 1.3795105049395449\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 211: 1.3798444445403117\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 212: 1.3789803976184325\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 213: 1.3797067052850098\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 214: 1.3803727521452793\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 215: 1.381082793076833\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 216: 1.3816529523392427\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 217: 1.381045705681547\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 218: 1.3809745458707416\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 219: 1.3816050133921884\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 220: 1.3818383966635794\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 221: 1.38256288487632\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 222: 1.38310048665701\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 223: 1.3830673034702028\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 224: 1.3831838115056356\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 225: 1.3838427188122167\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 226: 1.3842761968200952\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 227: 1.3853215669330798\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 228: 1.3859181253150041\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 229: 1.3873379391172658\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 230: 1.3868499658840558\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 231: 1.3868643615780205\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 232: 1.386928649419367\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 233: 1.3872826277700245\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 234: 1.3862049407147348\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 235: 1.3877270969293884\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 236: 1.387894356804055\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 237: 1.3877544723638968\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 238: 1.3887794107572804\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 239: 1.3900515094399453\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 240: 1.389892972356551\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 241: 1.3904132769127522\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 242: 1.3905997864994002\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 243: 1.3910137151108413\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 244: 1.3902905191693986\n",
            "\tContrastive Term: -0.087\n",
            "Checkpoint saved.\n",
            "Average loss after batch 245: 1.3905570104839355\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 246: 1.390803121362138\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 247: 1.3900347717346684\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 248: 1.3915186694348194\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 249: 1.390749137878418\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 250: 1.3914129943011766\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 251: 1.3910060127576191\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 252: 1.3906464011301636\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 253: 1.3911919762769083\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 254: 1.3911475494796155\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 255: 1.3914854773320258\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 256: 1.3914170603807798\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 257: 1.3908766217009967\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 258: 1.3904814867439417\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 259: 1.390357989531297\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 260: 1.390141996387321\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 261: 1.3901891794823509\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 262: 1.3901058911370687\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 263: 1.3906108235771006\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 264: 1.390733566374149\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 265: 1.3907385952490614\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 266: 1.3912952825817722\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 267: 1.3915950268951816\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 268: 1.3918089764712025\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 269: 1.3919159390308238\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 270: 1.3918486741196185\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 271: 1.3918281092363245\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 272: 1.3921241878153203\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 273: 1.3922819595267302\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 274: 1.3927380440451882\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 275: 1.3935611006142437\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 276: 1.3936909381232967\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 277: 1.394229139355447\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 278: 1.3946002632059076\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 279: 1.3945989080837795\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 280: 1.3944778136935523\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 281: 1.3943464853239398\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 282: 1.3944103738865667\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 283: 1.3942461085151618\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 284: 1.3945850589819122\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 285: 1.3949791809895655\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 286: 1.3953390678046889\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 287: 1.3963279252250989\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 288: 1.3967352825052597\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 289: 1.39686599723224\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 290: 1.3962304366822915\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 291: 1.3966616620756176\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 292: 1.3961340217460139\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 293: 1.3977594793248338\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 294: 1.3977456642409503\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 295: 1.3976143103193592\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 296: 1.398058897718436\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 297: 1.39805269241333\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 298: 1.3982275445325718\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 299: 1.398539822101593\n",
            "\tContrastive Term: -0.085\n",
            "Evaluating...\n",
            "Train: 0.3448149338855589 Validation: 0.07746469952500544 Test: 0.08628200996298319\n",
            "Average loss after batch 300: 1.39877769202489\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 301: 1.398715124224985\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 302: 1.3991598846888778\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 303: 1.3995326177070015\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 304: 1.3997868600438852\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 305: 1.4003520054754868\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 306: 1.4007110121972397\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 307: 1.4004314599873184\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 308: 1.4008150648530633\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 309: 1.4005076343013394\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 310: 1.400998161920014\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 311: 1.401573542218942\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 312: 1.4016771091820714\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 313: 1.4030526272810189\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 314: 1.4032532324866642\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 315: 1.4028254083440275\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 316: 1.4024055372653323\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 317: 1.4026763660352934\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 318: 1.4032305785481085\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 319: 1.403542671725154\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 320: 1.4041519179893802\n",
            "\tContrastive Term: -0.063\n",
            "Average loss after batch 321: 1.4043011813430313\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 322: 1.4049752518857597\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 323: 1.4056372465910736\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 324: 1.4057924714455239\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 325: 1.4057341243591777\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 326: 1.4065606841982687\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 327: 1.4067287339669903\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 328: 1.4070733474018364\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 329: 1.4079383073431073\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 330: 1.4078575801273128\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 331: 1.4080588810415153\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 332: 1.407950038308496\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 333: 1.4083045810282588\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 334: 1.4086199308509257\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 335: 1.4089750257276354\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 336: 1.4097059445140623\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 337: 1.4098088367450872\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 338: 1.410367475146741\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 339: 1.4106658998657675\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 340: 1.4108038434534829\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 341: 1.411227011541177\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 342: 1.4119142061072258\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 343: 1.4122633625601613\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 344: 1.4128487676813983\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 345: 1.41305742167324\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 346: 1.4136423405034397\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 347: 1.4145815026486057\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 348: 1.4148026445192048\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 349: 1.4148272378104074\n",
            "\tContrastive Term: -0.076\n",
            "Checkpoint saved.\n",
            "Average loss after batch 350: 1.4151461409707355\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 351: 1.4155485968698154\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 352: 1.416410716329688\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 353: 1.4169082011880174\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 354: 1.4173002615780899\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 355: 1.4175325109717551\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 356: 1.418063001472409\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 357: 1.4181998295490967\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 358: 1.4188632842225948\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 359: 1.418817902273602\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 360: 1.4188393648311373\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 361: 1.418896198272705\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 362: 1.41909673555495\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 363: 1.4194520092927492\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 364: 1.4202432465879884\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 365: 1.4206323164408323\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 366: 1.420934515038368\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 367: 1.421360913178195\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 368: 1.421778250839006\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 369: 1.4211577618444289\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 370: 1.4222579060217762\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 371: 1.4225981360481632\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 372: 1.4221414116688134\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 373: 1.4224782442664081\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 374: 1.4224428389867148\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 375: 1.4230587669509522\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 376: 1.4236328854801168\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 377: 1.4244093598512115\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 378: 1.4250783256616315\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 379: 1.4255269678015459\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 380: 1.426041822108071\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 381: 1.426294553654356\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 382: 1.426596517039964\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 383: 1.4265546761453152\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 384: 1.4266738235176384\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 385: 1.4269704395625258\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 386: 1.4274413407002924\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 387: 1.4272792102749814\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 388: 1.4278326641318118\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 389: 1.4280045396242387\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 390: 1.4283523306517345\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 391: 1.4284347347459014\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 392: 1.4290810705141257\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 393: 1.4287402769030653\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 394: 1.4288206209110308\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 395: 1.4287346423876406\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 396: 1.4289662738891333\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 397: 1.429106448463459\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 398: 1.429652456054114\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 399: 1.4297565296292305\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 400: 1.430256823351853\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 401: 1.4303854825484812\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 402: 1.4303331058611053\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 403: 1.4300478263066547\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 404: 1.4303303400675456\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 405: 1.4310249906455355\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 406: 1.4310055320913142\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 407: 1.430992579635452\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 408: 1.4314266332495766\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 409: 1.4316154276452413\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 410: 1.4319926917988017\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 411: 1.432545943746289\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 412: 1.4325588540361234\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 413: 1.4326465446591954\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 414: 1.4325947546097169\n",
            "\tContrastive Term: -0.066\n",
            "Average loss after batch 415: 1.432641317638067\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 416: 1.4332560869715483\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 417: 1.433420143058996\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 418: 1.4341945096245814\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 419: 1.434922994602294\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 420: 1.4354441500049873\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 421: 1.4351846448618089\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 422: 1.4354620036222128\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 423: 1.4354320413099144\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 424: 1.4355648329678703\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 425: 1.4355374210877039\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 426: 1.4365933868309932\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 427: 1.4361326677776942\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 428: 1.4365195855671986\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 429: 1.436987260607786\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 430: 1.436907329028278\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 431: 1.4371789674516078\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 432: 1.4374620391369968\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 433: 1.437719806273412\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 434: 1.4381607568126986\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 435: 1.4385981464057886\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 436: 1.4389333703152376\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 437: 1.4386103463499513\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 438: 1.4383457149622925\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 439: 1.4383742874318903\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 440: 1.4382850661569713\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 441: 1.438735072968772\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 442: 1.4383465369183646\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 443: 1.4382437089005031\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 444: 1.4383650852053353\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 445: 1.438661126010621\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 446: 1.4390515529069325\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 447: 1.4394012657659394\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 448: 1.4396225338789297\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 449: 1.4399015262391832\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 450: 1.439970933941674\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 451: 1.4398692722341655\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 452: 1.440174259360526\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 453: 1.4406621865764064\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 454: 1.4409437682602433\n",
            "\tContrastive Term: -0.070\n",
            "Checkpoint saved.\n",
            "Average loss after batch 455: 1.441073897637819\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 456: 1.4411068622601633\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 457: 1.4412254881130035\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 458: 1.4412287541204547\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 459: 1.441482810870461\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 460: 1.441629649242972\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 461: 1.44145678003113\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 462: 1.4411591672485384\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 463: 1.4407651845751137\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 464: 1.4409205716143372\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 465: 1.4408515993617634\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 466: 1.4414977636786548\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 467: 1.441739843187169\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 468: 1.4417266840619574\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 469: 1.4418235525171808\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 470: 1.442031094223071\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 471: 1.4423310332379098\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 472: 1.4424057841048936\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 473: 1.4426397565547926\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 474: 1.4421844976826719\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 475: 1.4422741700120334\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 476: 1.4423869873242807\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 477: 1.4420370077987097\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 478: 1.4423398202545707\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 479: 1.4423489302396775\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 480: 1.4419553307138708\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 481: 1.4424065173414238\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 482: 1.4429686071956627\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 483: 1.4431837060727364\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 484: 1.442786734374528\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 485: 1.442941209171044\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 486: 1.4429414507299967\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 487: 1.442944351522649\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 488: 1.4430650326371923\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 489: 1.4437624386378698\n",
            "\tContrastive Term: -0.079\n",
            "Checkpoint saved.\n",
            "Average loss after batch 490: 1.4442161290077473\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 491: 1.4442154862047212\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 492: 1.444426148464907\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 493: 1.4445324525659384\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 494: 1.4449153904963021\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 495: 1.4455118535026428\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 496: 1.445658975203992\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 497: 1.445946279299786\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 498: 1.4462471968664197\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 499: 1.446528101682663\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 500: 1.4459940166530494\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 501: 1.445969974139772\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 502: 1.4459576682589401\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 503: 1.446369095927193\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 504: 1.4467036410133438\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 505: 1.4472439571802795\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 506: 1.447566473507552\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 507: 1.4473629481210484\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 508: 1.4475187384075172\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 509: 1.4479018000995412\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 510: 1.4481688686546281\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 511: 1.448296104092151\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 512: 1.448420358912522\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 513: 1.4488471908791984\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 514: 1.4493195496716547\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 515: 1.4498956203460693\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 516: 1.4497938974683013\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 517: 1.4502502208050614\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 518: 1.450819409421873\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 519: 1.450738647350898\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 520: 1.4508117890403733\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 521: 1.4509851007625973\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 522: 1.4513171323170853\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 523: 1.4508516162861393\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 524: 1.4512565978368124\n",
            "\tContrastive Term: -0.079\n",
            "Checkpoint saved.\n",
            "Average loss after batch 525: 1.4515366166716746\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 526: 1.4518635327268596\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 527: 1.4520926218141208\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 528: 1.4522476450932724\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 529: 1.4524542055040035\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 530: 1.45289202418929\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 531: 1.4532945696124457\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 532: 1.453318892902997\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 533: 1.4531768033566992\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 534: 1.4537095729435716\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 535: 1.4541689273136764\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 536: 1.4544812141184034\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 537: 1.4548984854637912\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 538: 1.455042445593287\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 539: 1.4551340390134742\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 540: 1.4552286986722964\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 541: 1.4555956292416337\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 542: 1.4559129217492064\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 543: 1.455791828167789\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 544: 1.4558815558022316\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 545: 1.4559572153475695\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 546: 1.4559440811111897\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 547: 1.4563869098677253\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 548: 1.4562570204066014\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 549: 1.4565500077334317\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 550: 1.45654329695416\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 551: 1.4565439397010251\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 552: 1.4566154512315719\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 553: 1.4567270250957365\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 554: 1.4567060367481128\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 555: 1.4573703856348135\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 556: 1.4578990225732005\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 557: 1.457945976633325\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 558: 1.4580051069396127\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 559: 1.458175986153739\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 560: 1.4585667436348138\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 561: 1.458553236998697\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 562: 1.4588701727123499\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 563: 1.4588773749398847\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 564: 1.4590305651183677\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 565: 1.4596221213205964\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 566: 1.4602471026583743\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 567: 1.4605046265142065\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 568: 1.4607778632577986\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 569: 1.4611367317668178\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 570: 1.4611099150468922\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 571: 1.4614915222554774\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 572: 1.4620602844481276\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 573: 1.4620974186405487\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 574: 1.4621033971206001\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 575: 1.46223744067053\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 576: 1.462355746966714\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 577: 1.4630836576326496\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 578: 1.4632598134931918\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 579: 1.4635845192547503\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 580: 1.4638646090092224\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 581: 1.4638121851121437\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 582: 1.4637875370872981\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 583: 1.4639775953064227\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 584: 1.4639904329919406\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 585: 1.4643067475471887\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 586: 1.4645028999757523\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 587: 1.4649429536190162\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 588: 1.4650333514642635\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 589: 1.4651705887358066\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 590: 1.4654975625262445\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 591: 1.4654896404292133\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 592: 1.465916557014491\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 593: 1.466167804970083\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 594: 1.4665742068731484\n",
            "\tContrastive Term: -0.079\n",
            "Checkpoint saved.\n",
            "Average loss after batch 595: 1.4665759879870703\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 596: 1.466385297040444\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 597: 1.4668817418474815\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 598: 1.467532190138191\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 599: 1.467620431582133\n",
            "\tContrastive Term: -0.075\n",
            "Evaluating...\n",
            "Train: 0.37027777736152734 Validation: 0.0837122405821183 Test: 0.09113126409463204\n",
            "Average loss after batch 600: 1.4679209162510571\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 601: 1.467776753379657\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 602: 1.4678715374536973\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 603: 1.4681877142546194\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 604: 1.4688288842350983\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 605: 1.46896756954319\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 606: 1.4686475324473625\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 607: 1.4685303417867737\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 608: 1.4686784714900802\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 609: 1.4690637103846815\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 610: 1.4693635388013775\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 611: 1.469483725775301\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 612: 1.4698315359056482\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 613: 1.4698883281468569\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 614: 1.469956913808497\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 615: 1.470033279487065\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 616: 1.4700307668318044\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 617: 1.470749172577966\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 618: 1.4708003423903409\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 619: 1.4706150685587236\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 620: 1.470819488239749\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 621: 1.4709912360289472\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 622: 1.4709382843626828\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 623: 1.4707747516341698\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 624: 1.4712608791351318\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 625: 1.4721519006326937\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 626: 1.4721382481819896\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 627: 1.4722486035839009\n",
            "\tContrastive Term: -0.066\n",
            "Average loss after batch 628: 1.4726610477474802\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 629: 1.4727694997711787\n",
            "\tContrastive Term: -0.076\n",
            "Checkpoint saved.\n",
            "Average loss after batch 630: 1.4729939699172974\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 631: 1.4733043835510182\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 632: 1.473620400986212\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 633: 1.4736951166522991\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 634: 1.4734860568534671\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 635: 1.4739985147362236\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 636: 1.47426287701216\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 637: 1.4743610624235626\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 638: 1.474586350257408\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 639: 1.4744641378521919\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 640: 1.4744809944060588\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 641: 1.4744816498593007\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 642: 1.4744436959641902\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 643: 1.4739720721422516\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 644: 1.4740967159123384\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 645: 1.473931139283136\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 646: 1.474003214740311\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 647: 1.47431835256241\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 648: 1.4746652775442655\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 649: 1.4751616127674396\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 650: 1.4750820372694282\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 651: 1.4753595848390662\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 652: 1.4753724298287314\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 653: 1.4751666264796475\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 654: 1.4753026940440404\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 655: 1.475247683866722\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 656: 1.4754479052995075\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 657: 1.4757923636030643\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 658: 1.4760841739014898\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 659: 1.4764427360260126\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 660: 1.4768948199709317\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 661: 1.476843760452962\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 662: 1.4770440751431035\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 663: 1.4771820590438614\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 664: 1.4772055903771766\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 665: 1.477016058770028\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 666: 1.4771510119559703\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 667: 1.4775171617190994\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 668: 1.4777611367549242\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 669: 1.4777731740652624\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 670: 1.4778124694497325\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 671: 1.4778211407718205\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 672: 1.4778032474673697\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 673: 1.478054405673675\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 674: 1.4783715867996217\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 675: 1.4784577309027227\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 676: 1.4785814746620032\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 677: 1.4783341388786788\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 678: 1.4788792454617012\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 679: 1.479176159641322\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 680: 1.4795820147630576\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 681: 1.4796921341649947\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 682: 1.4799370257781155\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 683: 1.4802316683426238\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 684: 1.4805726159228025\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 685: 1.4809289910007843\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 686: 1.481476509067863\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 687: 1.481317086448503\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 688: 1.4815324896132824\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 689: 1.481823087775189\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 690: 1.4822382057454928\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 691: 1.4826004930658836\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 692: 1.48285189627913\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 693: 1.4829243558971614\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 694: 1.4833187542373327\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 695: 1.4831989948776947\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 696: 1.4831378608728243\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 697: 1.483147140560314\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 698: 1.4832641225686571\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 699: 1.4838635373115538\n",
            "\tContrastive Term: -0.087\n",
            "Checkpoint saved.\n",
            "Average loss after batch 700: 1.4837757614301716\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 701: 1.4839736865117\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 702: 1.4840398448629366\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 703: 1.4838505036790262\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 704: 1.4840462451285503\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 705: 1.484041308208498\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 706: 1.4838672915138988\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 707: 1.4840908498413818\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 708: 1.4843526192545387\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 709: 1.484534342691932\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 710: 1.4844051750400398\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 711: 1.484431726544091\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 712: 1.484600367753402\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 713: 1.4849605169616829\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 714: 1.4851485374090554\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 715: 1.4858788673105188\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 716: 1.4864304576126268\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 717: 1.486747766271607\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 718: 1.4871346759862463\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 719: 1.4876815074019962\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 720: 1.4876150059799216\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 721: 1.4875455226264171\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 722: 1.488031594090442\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 723: 1.4880991680187414\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 724: 1.4882383216660597\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 725: 1.488317727221602\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 726: 1.488662734976169\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 727: 1.4887651756897078\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 728: 1.48901074805868\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 729: 1.4889326124975126\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 730: 1.4889205966537205\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 731: 1.4893125823286713\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 732: 1.4894980367827058\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 733: 1.4899253429444024\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 734: 1.4902576933101732\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 735: 1.4907248028270577\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 736: 1.4906421354700137\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 737: 1.4911155293627483\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 738: 1.4915930071444892\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 739: 1.4919573274818627\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 740: 1.4921871774431503\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 741: 1.4922056469634537\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 742: 1.492372030364713\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 743: 1.4922433815976626\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 744: 1.492600918136187\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 745: 1.4927624764455227\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 746: 1.492846908658703\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 747: 1.4930562899711934\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 748: 1.4927975525047497\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 749: 1.4927796456019085\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 750: 1.4932442571764462\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 751: 1.4932494130223355\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 752: 1.4933489041816033\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 753: 1.4936282242640893\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 754: 1.493703680796339\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 755: 1.4938951180410134\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 756: 1.493908287826901\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 757: 1.4943285697682867\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 758: 1.4944415792960266\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 759: 1.4944920149288679\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 760: 1.4945618753834247\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 761: 1.4945876593664875\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 762: 1.4945713740970017\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 763: 1.4943886205788057\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 764: 1.4946482289071177\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 765: 1.4946586668646988\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 766: 1.4949257213724327\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 767: 1.495206475413094\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 768: 1.4952532097949163\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 769: 1.4956751470441942\n",
            "\tContrastive Term: -0.068\n",
            "Checkpoint saved.\n",
            "Average loss after batch 770: 1.4961149137462315\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 771: 1.4964013561374783\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 772: 1.4966984387357054\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 773: 1.4970404579657917\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 774: 1.4971914174479823\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 775: 1.497243162743824\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 776: 1.4977433297931764\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 777: 1.497806914974301\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 778: 1.4983617221895935\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 779: 1.4981969396273296\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 780: 1.4980608907154978\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 781: 1.498308280087493\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 782: 1.4986169268192764\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 783: 1.4987345041365039\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 784: 1.4989746775596764\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 785: 1.4990548427778345\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 786: 1.499258907656203\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 787: 1.4994077738469023\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 788: 1.4992214051489594\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 789: 1.4993811740150935\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 790: 1.4995076801020337\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 791: 1.4995735654927262\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 792: 1.4997239666057265\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 793: 1.5000239008620044\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 794: 1.5002617116244334\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 795: 1.5006790578964369\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 796: 1.5006699728098238\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 797: 1.5010740471663033\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 798: 1.501021051287502\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 799: 1.500906495898962\n",
            "\tContrastive Term: -0.092\n",
            "Checkpoint saved.\n",
            "Average training loss: 1.500906495898962\n",
            "Evaluating...\n",
            "Train: 0.3566685592185592 Validation: 0.08493764712154508 Test: 0.09049510899565574\n",
            "Train Loss: 1.500906495898962\n",
            "2023.04.25-00:43:35\n",
            "Epoch 9 training...\n",
            "lr:  0.001\n",
            "Average loss after batch 0: 1.2423980236053467\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 1: 1.3176456689834595\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 2: 1.238377332687378\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 3: 1.2133929133415222\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 4: 1.1945611476898192\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 5: 1.1774679223696392\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 6: 1.1921769210270472\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 7: 1.2061513513326645\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 8: 1.1955211162567139\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 9: 1.1942182898521423\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 10: 1.1907499811866067\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 11: 1.179914007584254\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 12: 1.1766222990476167\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 13: 1.165455801146371\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 14: 1.1570686101913452\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 15: 1.1499331891536713\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 16: 1.1508240769891178\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 17: 1.14702398247189\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 18: 1.1381995426981073\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 19: 1.1368984818458556\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 20: 1.1319572868801298\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 21: 1.1242275725711475\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 22: 1.1106463230174521\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 23: 1.1072956348458927\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 24: 1.1078585171699524\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 25: 1.118668475976357\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 26: 1.1173271227765966\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 27: 1.1239701701062066\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 28: 1.1266323430784817\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 29: 1.1201227764288584\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 30: 1.1187151843501675\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 31: 1.1179765816777945\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 32: 1.1143218333070928\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 33: 1.1165295576348024\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 34: 1.1144471287727356\n",
            "\tContrastive Term: -0.082\n",
            "Checkpoint saved.\n",
            "Average loss after batch 35: 1.1123482800192304\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 36: 1.1125472864589177\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 37: 1.1086335684123791\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 38: 1.104819152599726\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 39: 1.1033869877457618\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 40: 1.1022129858412393\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 41: 1.1029475600946517\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 42: 1.1058377734450406\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 43: 1.1037914847785777\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 44: 1.107517060968611\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 45: 1.112582135459651\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 46: 1.1105730850645836\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 47: 1.110585653533538\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 48: 1.1107010415622167\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 49: 1.1106664526462555\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 50: 1.1149519015760982\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 51: 1.1148297454302127\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 52: 1.114953120924392\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 53: 1.1158830512453008\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 54: 1.1173617352138867\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 55: 1.1187859933291162\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 56: 1.121182566149193\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 57: 1.1199420803579792\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 58: 1.1226310962337558\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 59: 1.122235577305158\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 60: 1.1223701893306168\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 61: 1.1221048495461863\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 62: 1.1223160435282995\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 63: 1.1219005258753896\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 64: 1.1202680688637954\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 65: 1.1216221858154645\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 66: 1.1258355299038674\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 67: 1.1242948393611347\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 68: 1.12450656424398\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 69: 1.1275826343468258\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 70: 1.1274170783204092\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 71: 1.1312034618523386\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 72: 1.1328150581007135\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 73: 1.133927644104571\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 74: 1.1346574203173319\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 75: 1.1348420452130468\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 76: 1.134627208307192\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 77: 1.1355975721126947\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 78: 1.139081251017655\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 79: 1.1401011116802693\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 80: 1.1391048321017512\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 81: 1.143312609050332\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 82: 1.1451048082615956\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 83: 1.143312896291415\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 84: 1.142594930003671\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 85: 1.1460672263489213\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 86: 1.1432255307833354\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 87: 1.1418421505527063\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 88: 1.1428689199886966\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 89: 1.1452767219808366\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 90: 1.1470093537162949\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 91: 1.148003291176713\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 92: 1.148683646032887\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 93: 1.148046688196507\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 94: 1.147827789030577\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 95: 1.1472596917301416\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 96: 1.1479093616770715\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 97: 1.1484807991251653\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 98: 1.148555540677273\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 99: 1.1489372581243515\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 100: 1.1480993888165691\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 101: 1.1507147252559662\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 102: 1.150868530991008\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 103: 1.1502485498785973\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 104: 1.152184630007971\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 105: 1.153127612370365\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 106: 1.1543724820992658\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 107: 1.1553566803534825\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 108: 1.1547525131374323\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 109: 1.1568011939525604\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 110: 1.15652615905882\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 111: 1.1585281209221907\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 112: 1.1568081885312511\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 113: 1.1564921025644268\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 114: 1.157591864337092\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 115: 1.1592487754492924\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 116: 1.1610993574827144\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 117: 1.1623575667203483\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 118: 1.164146209965233\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 119: 1.1638788471619288\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 120: 1.1640239796362633\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 121: 1.1650490125671762\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 122: 1.1660183542143039\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 123: 1.165498201885531\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 124: 1.1661235084533692\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 125: 1.1646168468490479\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 126: 1.1646471154971385\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 127: 1.1646614884957671\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 128: 1.1648115483365318\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 129: 1.165134456524482\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 130: 1.165256413794656\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 131: 1.1660023151022014\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 132: 1.1661144916276287\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 133: 1.1655650396845234\n",
            "\tContrastive Term: -0.067\n",
            "Average loss after batch 134: 1.1649429109361438\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 135: 1.16732278992148\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 136: 1.166097610536283\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 137: 1.167599177878836\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 138: 1.1672635352868828\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 139: 1.1668330652373178\n",
            "\tContrastive Term: -0.075\n",
            "Checkpoint saved.\n",
            "Average loss after batch 140: 1.167734552782478\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 141: 1.1667213985617733\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 142: 1.1672423244356276\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 143: 1.1666763805680804\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 144: 1.1660461195584002\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 145: 1.166300743410032\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 146: 1.1670110404085952\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 147: 1.1680840543798499\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 148: 1.1688274817178714\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 149: 1.1689740951855978\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 150: 1.1688649251761025\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 151: 1.1690533749367062\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 152: 1.1686909183178074\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 153: 1.1683056462894787\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 154: 1.1685994025199644\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 155: 1.1695190049134767\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 156: 1.1693942296277187\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 157: 1.1712096823921687\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 158: 1.1718598911597293\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 159: 1.1715972252190112\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 160: 1.1724108946249352\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 161: 1.1735230573901423\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 162: 1.1735050897656774\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 163: 1.1741198344928463\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 164: 1.1726952841787628\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 165: 1.1728471733001342\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 166: 1.1719453720275514\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 167: 1.17214212673051\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 168: 1.171568146118751\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 169: 1.172218230191399\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 170: 1.1725150346755981\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 171: 1.1725479606972184\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 172: 1.171643418383736\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 173: 1.1724218520624885\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 174: 1.1733228043147497\n",
            "\tContrastive Term: -0.074\n",
            "Checkpoint saved.\n",
            "Average loss after batch 175: 1.1733557785099202\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 176: 1.1732555924162353\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 177: 1.173048893387398\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 178: 1.1728538048333963\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 179: 1.1720668832461039\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 180: 1.173115573535308\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 181: 1.1721946734648485\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 182: 1.1721388049464407\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 183: 1.1732576314521872\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 184: 1.1738244752626161\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 185: 1.1737131290538336\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 186: 1.174095270467952\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 187: 1.1754787025299478\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 188: 1.176298287810472\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 189: 1.1758723798551058\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 190: 1.1771895966604742\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 191: 1.1765875536948442\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 192: 1.176789719206064\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 193: 1.1770512166711473\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 194: 1.1769124012727004\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 195: 1.1776198312944295\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 196: 1.178438018421231\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 197: 1.1785530435918556\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 198: 1.1788572725938193\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 199: 1.1785708820819856\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 200: 1.1788145606197529\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 201: 1.17790886877787\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 202: 1.1777035265133298\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 203: 1.1775122948136985\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 204: 1.1773002313404548\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 205: 1.177919711013442\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 206: 1.1780317834609948\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 207: 1.178871110941355\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 208: 1.1784817684780469\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 209: 1.1777919573443276\n",
            "\tContrastive Term: -0.091\n",
            "Checkpoint saved.\n",
            "Average loss after batch 210: 1.1777453504467463\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 211: 1.1779407243121345\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 212: 1.1779209769947427\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 213: 1.1790978125879699\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 214: 1.1797786244126254\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 215: 1.1798054799437523\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 216: 1.179302396038161\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 217: 1.1790986927824283\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 218: 1.1797942535517967\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 219: 1.1798866361379623\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 220: 1.180165379565226\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 221: 1.1804919428116567\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 222: 1.1817992233374728\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 223: 1.1822743477033717\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 224: 1.1827696167098152\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 225: 1.1834141572492312\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 226: 1.1833557008646658\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 227: 1.183426552435808\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 228: 1.1836904508578205\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 229: 1.1845852714517842\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 230: 1.1851404262827587\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 231: 1.1857075287864125\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 232: 1.1856650029640852\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 233: 1.1862645625558674\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 234: 1.1857321853333331\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 235: 1.184494547924753\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 236: 1.1848949415271293\n",
            "\tContrastive Term: -0.066\n",
            "Average loss after batch 237: 1.184960189486752\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 238: 1.1860790876165095\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 239: 1.1856653705239295\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 240: 1.1871344865110405\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 241: 1.1867073301441413\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 242: 1.1876601373217233\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 243: 1.1869055364952712\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 244: 1.1864305968187294\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 245: 1.185840714268568\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 246: 1.1865956928083288\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 247: 1.1866614655140908\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 248: 1.186926891047313\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 249: 1.1869553003311157\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 250: 1.186794355096095\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 251: 1.1871607317810966\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 252: 1.1869696011185176\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 253: 1.1868800023409325\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 254: 1.186479938731474\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 255: 1.1868571429513395\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 256: 1.186761886229311\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 257: 1.18678360114726\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 258: 1.1875054532496625\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 259: 1.1871820454414075\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 260: 1.1880157094348893\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 261: 1.1888225597279671\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 262: 1.1886464511486967\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 263: 1.1886090288559596\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 264: 1.1885391356810084\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 265: 1.1890178524461903\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 266: 1.1886859618769157\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 267: 1.1887305045305794\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 268: 1.1892572508425518\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 269: 1.1898152011412162\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 270: 1.189994440747363\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 271: 1.1911610229050411\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 272: 1.191383068378155\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 273: 1.1913127942676962\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 274: 1.1911820888519287\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 275: 1.190932425899782\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 276: 1.1910326885402418\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 277: 1.190759043470561\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 278: 1.191092767168544\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 279: 1.191067242196628\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 280: 1.1912644690041865\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 281: 1.191192679371394\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 282: 1.192123207523629\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 283: 1.1927479442576288\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 284: 1.192490549673114\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 285: 1.1927791400389238\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 286: 1.193128043766221\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 287: 1.193693449927701\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 288: 1.1941246487277601\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 289: 1.1950632185771548\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 290: 1.1956394513448079\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 291: 1.1956471400718167\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 292: 1.1956611611330468\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 293: 1.1955786002736513\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 294: 1.195862612885944\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 295: 1.1957707944754008\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 296: 1.1958066156817606\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 297: 1.196194089899127\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 298: 1.1969498487619252\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 299: 1.1968427610397339\n",
            "\tContrastive Term: -0.071\n",
            "Evaluating...\n",
            "Train: 0.4043049216061716 Validation: 0.07672273356850645 Test: 0.084161168954772\n",
            "Average loss after batch 300: 1.1963325802273925\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 301: 1.1968029129584103\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 302: 1.1970648560980366\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 303: 1.1968663742667751\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 304: 1.1979240839598608\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 305: 1.1989616412742465\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 306: 1.1986125443579707\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 307: 1.1991295624863019\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 308: 1.1993783079690532\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 309: 1.1999234280278606\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 310: 1.2000268732236512\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 311: 1.2006511344359472\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 312: 1.2011820660612453\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 313: 1.2007015970102541\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 314: 1.2007580859320504\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 315: 1.2009820379788363\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 316: 1.2017824563318247\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 317: 1.201744960538996\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 318: 1.2023730468600522\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 319: 1.2033337976783514\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 320: 1.2041321521236146\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 321: 1.2041887707591796\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 322: 1.2041279442908226\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 323: 1.204402020316065\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 324: 1.2047206438504732\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 325: 1.2047423658195449\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 326: 1.2057264281339966\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 327: 1.2061460348891049\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 328: 1.2064319700455592\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 329: 1.206503644134059\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 330: 1.2067162630421158\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 331: 1.2070505051727753\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 332: 1.2078728160342656\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 333: 1.2082330209766319\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 334: 1.2091179217865218\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 335: 1.2090699903312183\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 336: 1.2094669646254632\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 337: 1.2102864049595488\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 338: 1.2098570024721038\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 339: 1.2101566553115846\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 340: 1.2106813562930154\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 341: 1.2107031240797879\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 342: 1.2116140544240745\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 343: 1.2123647013375924\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 344: 1.2124613219413205\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 345: 1.2127193140156696\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 346: 1.213028297300641\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 347: 1.2131071549722519\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 348: 1.2133496299514115\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 349: 1.21378436088562\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 350: 1.2142694400586294\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 351: 1.2145972190932794\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 352: 1.2146665408320874\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 353: 1.2146985746372891\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 354: 1.2147239130987249\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 355: 1.2148470734612327\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 356: 1.2150672193334884\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 357: 1.2150827742821677\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 358: 1.2152095594778034\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 359: 1.215599904788865\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 360: 1.2160084538182394\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 361: 1.2162524577003817\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 362: 1.2170077069731784\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 363: 1.2176374642403571\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 364: 1.2177096249306039\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 365: 1.2182453782832037\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 366: 1.2179027727579226\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 367: 1.2185696301900821\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 368: 1.2189901508289946\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 369: 1.2188620483553088\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 370: 1.2189469549533813\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 371: 1.2190765863464725\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 372: 1.2194604678703698\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 373: 1.219445621584826\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 374: 1.2191885042190551\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 375: 1.2190848703080035\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 376: 1.2197838155281007\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 377: 1.2196854176344696\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 378: 1.2206552434407942\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 379: 1.2206886940880826\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 380: 1.2208454608917236\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 381: 1.220698847196489\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 382: 1.221239290100476\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 383: 1.2215267115583022\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 384: 1.2219772276940284\n",
            "\tContrastive Term: -0.073\n",
            "Checkpoint saved.\n",
            "Average loss after batch 385: 1.2225623665078316\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 386: 1.2230321196622627\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 387: 1.2235061535515737\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 388: 1.2237639883796476\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 389: 1.2239651597463168\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 390: 1.2239919928333642\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 391: 1.2243772973211444\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 392: 1.224536455921241\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 393: 1.2247213784813276\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 394: 1.2245122052446196\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 395: 1.2251188920603857\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 396: 1.2249526136768254\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 397: 1.2252243298981058\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 398: 1.2253053851593705\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 399: 1.2250762462615967\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 400: 1.2249190031441668\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 401: 1.2253954137735699\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 402: 1.2257291238302037\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 403: 1.2259879132898728\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 404: 1.226814075163853\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 405: 1.2271115556726315\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 406: 1.2269735002400541\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 407: 1.2271205359229855\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 408: 1.2275361749537126\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 409: 1.2277814580173028\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 410: 1.2281457362673869\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 411: 1.228411552396793\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 412: 1.228788329094432\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 413: 1.229146911614183\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 414: 1.2299052422305188\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 415: 1.2302700676597083\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 416: 1.2307326996640908\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 417: 1.2310205792125903\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 418: 1.231136133676497\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 419: 1.2316218208699\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 420: 1.2317286132350387\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 421: 1.2319375443232568\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 422: 1.23176406743115\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 423: 1.2320813471978564\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 424: 1.2320751998003792\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 425: 1.2322637130956695\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 426: 1.2324873764565176\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 427: 1.232896206256385\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 428: 1.2334909405741659\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 429: 1.233694573890331\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 430: 1.2334948531989431\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 431: 1.2337578104601965\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 432: 1.2338286106911323\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 433: 1.2336242308814405\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 434: 1.233601061229048\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 435: 1.2340270770252297\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 436: 1.235354252756324\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 437: 1.2352754830225419\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 438: 1.2354512114188123\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 439: 1.235301597551866\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 440: 1.2354759840197573\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 441: 1.235472536194918\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 442: 1.235828672790097\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 443: 1.2361090897439837\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 444: 1.2361423813894894\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 445: 1.236588441737564\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 446: 1.236731912732391\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 447: 1.2369682456233673\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 448: 1.2370840249985522\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 449: 1.2371811087926228\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 450: 1.2381342078523996\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 451: 1.2387127815622143\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 452: 1.2389597795393819\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 453: 1.238927813616093\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 454: 1.2396900732438643\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 455: 1.2403313888792407\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 456: 1.240545944520629\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 457: 1.2405061966467112\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 458: 1.241076590708398\n",
            "\tContrastive Term: -0.065\n",
            "Average loss after batch 459: 1.2414590623067774\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 460: 1.2414585646734837\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 461: 1.2415873155449375\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 462: 1.241764493173966\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 463: 1.2417266153056046\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 464: 1.242150797638842\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 465: 1.2422117465555411\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 466: 1.2425354045259365\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 467: 1.2428941953385997\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 468: 1.243114810762629\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 469: 1.2429454121183843\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 470: 1.2432112962070798\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 471: 1.243450216822705\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 472: 1.2438441812362027\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 473: 1.244287787861965\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 474: 1.2443447175778841\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 475: 1.2450014936322926\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 476: 1.2451475926165312\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 477: 1.2453979134060849\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 478: 1.2453757074033542\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 479: 1.2450655030707518\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 480: 1.2453969332879398\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 481: 1.2451344622121312\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 482: 1.2459706201316407\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 483: 1.2462598989325122\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 484: 1.246281769349403\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 485: 1.2466149847693895\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 486: 1.2463693134104201\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 487: 1.2466950240682384\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 488: 1.2467742719533252\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 489: 1.2470219033105032\n",
            "\tContrastive Term: -0.087\n",
            "Checkpoint saved.\n",
            "Average loss after batch 490: 1.2476195313051617\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 491: 1.247689669936653\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 492: 1.248059581066\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 493: 1.2481928388116814\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 494: 1.2483799040919603\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 495: 1.2484156489372253\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 496: 1.24849342004634\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 497: 1.248841445130038\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 498: 1.2489430003749107\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 499: 1.2490900580883026\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 500: 1.24907114239272\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 501: 1.249429973473112\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 502: 1.2495727797388794\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 503: 1.2494650468939827\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 504: 1.250285257679401\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 505: 1.2504138227979185\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 506: 1.2502620927916006\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 507: 1.2502481805996633\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 508: 1.2504015915051894\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 509: 1.2504515699311798\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 510: 1.2507232980485532\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 511: 1.2507890609558672\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 512: 1.2515381009955155\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 513: 1.2516262371252482\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 514: 1.2520226316544616\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 515: 1.252483677956485\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 516: 1.2523031921866323\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 517: 1.2522397448657563\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 518: 1.2523888370205212\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 519: 1.2522096643081078\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 520: 1.2525959536576226\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 521: 1.2528748112620065\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 522: 1.2533691070731694\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 523: 1.2534330029979006\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 524: 1.2532942226954868\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 525: 1.2531042479743522\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 526: 1.253473249965646\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 527: 1.2536541363506606\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 528: 1.2539282129031724\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 529: 1.2537964962563424\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 530: 1.254119570645909\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 531: 1.2543090265944488\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 532: 1.2542759956159466\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 533: 1.2547605941804607\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 534: 1.2546288900286238\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 535: 1.2551944582764782\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 536: 1.255230408141067\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 537: 1.255160802137453\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 538: 1.2552864246333022\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 539: 1.2561103061393455\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 540: 1.2559331659909316\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 541: 1.2558346190575744\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 542: 1.2561566935059774\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 543: 1.2560946437365867\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 544: 1.2564456462860107\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 545: 1.2565501620481303\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 546: 1.2563658444929384\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 547: 1.2565408951609673\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 548: 1.2565156594868783\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 549: 1.2565083638104526\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 550: 1.2566964234717313\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 551: 1.2569580430137939\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 552: 1.257344005336382\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 553: 1.2576710375207425\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 554: 1.2579101826693562\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 555: 1.2581884623002664\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 556: 1.2580982442390127\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 557: 1.2582521994054103\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 558: 1.2583461053043017\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 559: 1.2584508657455444\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 560: 1.258985430799066\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 561: 1.2587415875064945\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 562: 1.2587773126564907\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 563: 1.2590903162110783\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 564: 1.259316344809743\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 565: 1.259354856115348\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 566: 1.2591583316708788\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 567: 1.2595747507793802\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 568: 1.2598281419968562\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 569: 1.2602121252762644\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 570: 1.2601825706595506\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 571: 1.2603447195533273\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 572: 1.2603788848114679\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 573: 1.2601290012901254\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 574: 1.2602935347349746\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 575: 1.2609848965787225\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 576: 1.2613178534978804\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 577: 1.261633862879862\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 578: 1.262045972499617\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 579: 1.2624349265262997\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 580: 1.2626461418483426\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 581: 1.2627203433784013\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 582: 1.262942542956174\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 583: 1.2627416471504185\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 584: 1.2631419395789123\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 585: 1.262986468373712\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 586: 1.2631220257058866\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 587: 1.2630551445240876\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 588: 1.2629882307732652\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 589: 1.2628774485345615\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 590: 1.263481567757182\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 591: 1.263508533102435\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 592: 1.263504345742687\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 593: 1.2635802909581348\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 594: 1.263357881137303\n",
            "\tContrastive Term: -0.073\n",
            "Checkpoint saved.\n",
            "Average loss after batch 595: 1.2639159014161\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 596: 1.264283089981207\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 597: 1.264406094981675\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 598: 1.2641621975349464\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 599: 1.2640778221686682\n",
            "\tContrastive Term: -0.070\n",
            "Evaluating...\n",
            "Train: 0.42507242236929743 Validation: 0.07772074579718002 Test: 0.08643421838473779\n",
            "Average loss after batch 600: 1.2643660271227262\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 601: 1.2643444351975703\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 602: 1.2641889404499314\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 603: 1.264562076488078\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 604: 1.2649386035509347\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 605: 1.2649458056629295\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 606: 1.2647661964425931\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 607: 1.264612061020575\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 608: 1.264530960562194\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 609: 1.2643535871974758\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 610: 1.264702683384797\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 611: 1.2651377144981832\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 612: 1.2652919354679923\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 613: 1.2655856630701197\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 614: 1.2657027161218286\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 615: 1.2656290151856162\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 616: 1.2659272015384568\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 617: 1.2662165141800075\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 618: 1.2664380085102382\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 619: 1.2668275975411938\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 620: 1.2665296009773217\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 621: 1.2666405071399602\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 622: 1.266828517469893\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 623: 1.2669465964039166\n",
            "\tContrastive Term: -0.099\n",
            "Average loss after batch 624: 1.26717126121521\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 625: 1.2673201618103174\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 626: 1.2673441639357206\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 627: 1.2673386557466666\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 628: 1.2677532591766698\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 629: 1.2679230735415503\n",
            "\tContrastive Term: -0.071\n",
            "Checkpoint saved.\n",
            "Average loss after batch 630: 1.2679052774199593\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 631: 1.2679835205213934\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 632: 1.2685450526398694\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 633: 1.2687259654893484\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 634: 1.2686777032266452\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 635: 1.2688244406907063\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 636: 1.2687780795329406\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 637: 1.2688697202825994\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 638: 1.2692320749793253\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 639: 1.2695458073168993\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 640: 1.2694383123177635\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 641: 1.2696728951463074\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 642: 1.2697456281626391\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 643: 1.2698507645855779\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 644: 1.2699440102244532\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 645: 1.2705716872731967\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 646: 1.2705405368318514\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 647: 1.2706202303553804\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 648: 1.2706896303614776\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 649: 1.2707701033812302\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 650: 1.2708695257131222\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 651: 1.2710533081753854\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 652: 1.2710893384165274\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 653: 1.2714412321738147\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 654: 1.2713349984802362\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 655: 1.2720003208009207\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 656: 1.2720679320519737\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 657: 1.272148421288986\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 658: 1.2724723616934328\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 659: 1.2723682260874547\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 660: 1.2725737686416927\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 661: 1.2727092275806784\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 662: 1.272640213168045\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 663: 1.2729950064277074\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 664: 1.2732700312047973\n",
            "\tContrastive Term: -0.069\n",
            "Checkpoint saved.\n",
            "Average loss after batch 665: 1.2732500947631515\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 666: 1.2734640186158257\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 667: 1.2737858639862722\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 668: 1.2735843686957709\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 669: 1.2739073417079982\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 670: 1.2740246457243463\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 671: 1.27424075064205\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 672: 1.2744146985780962\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 673: 1.274437647898756\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 674: 1.274865944120619\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 675: 1.2751150640862934\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 676: 1.2749916427772894\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 677: 1.2749700136592605\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 678: 1.2752092077735597\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 679: 1.2756111970719168\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 680: 1.2759026481541593\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 681: 1.2761105695777624\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 682: 1.276280725857524\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 683: 1.2763637645899901\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 684: 1.2769136919592419\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 685: 1.2774190096396398\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 686: 1.2777275430062973\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 687: 1.277619826412478\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 688: 1.2778242252042573\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 689: 1.2781316492868506\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 690: 1.2782718973118385\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 691: 1.2785273511285726\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 692: 1.278994852846319\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 693: 1.2794196834138216\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 694: 1.2795116431421514\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 695: 1.2796322028527314\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 696: 1.279799981986091\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 697: 1.2799716308328688\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 698: 1.280184889897086\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 699: 1.2801380784170968\n",
            "\tContrastive Term: -0.083\n",
            "Checkpoint saved.\n",
            "Average loss after batch 700: 1.2804381542981267\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 701: 1.2805872878797373\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 702: 1.2806082356874837\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 703: 1.280840946530754\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 704: 1.2808746265181412\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 705: 1.2810330070111975\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 706: 1.281006802763932\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 707: 1.281143627766162\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 708: 1.2816077924749916\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 709: 1.2818102764411712\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 710: 1.2820554861036535\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 711: 1.282258684045813\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 712: 1.2825213980039407\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 713: 1.282705103315893\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 714: 1.2831376170778608\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 715: 1.283181429408782\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 716: 1.2834714882210874\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 717: 1.2838729940417086\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 718: 1.283915311968542\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 719: 1.2840294679005941\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 720: 1.2840522236698377\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 721: 1.2843008085961487\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 722: 1.284696047401692\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 723: 1.2848155900259703\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 724: 1.2845292375827657\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 725: 1.2845376519789051\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 726: 1.2847252265951163\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 727: 1.2847515534568619\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 728: 1.2848937354460666\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 729: 1.2849412021571642\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 730: 1.2851911515772099\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 731: 1.2854688102430334\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 732: 1.285781897594301\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 733: 1.2858220960853535\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 734: 1.286088067495904\n",
            "\tContrastive Term: -0.073\n",
            "Checkpoint saved.\n",
            "Average loss after batch 735: 1.286233292001745\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 736: 1.2865152145499614\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 737: 1.286791399080902\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 738: 1.2872031468983756\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 739: 1.287244258861284\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 740: 1.2877973885993081\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 741: 1.2879533745207876\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 742: 1.2880069819626223\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 743: 1.2881946680686807\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 744: 1.2886560558472704\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 745: 1.2890475954830487\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 746: 1.2890503532596063\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 747: 1.289078886496192\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 748: 1.2893486282376645\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 749: 1.2895524654388428\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 750: 1.2897631902986773\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 751: 1.2897690420455121\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 752: 1.2903466954490894\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 753: 1.2907232898299827\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 754: 1.2908689843108323\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 755: 1.291076196879937\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 756: 1.2912401433350074\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 757: 1.2915726604436506\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 758: 1.2919510868384119\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 759: 1.2919558454501001\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 760: 1.2921584759999198\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 761: 1.2921090149503993\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 762: 1.2921892531108732\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 763: 1.2921920875916306\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 764: 1.2922300916871214\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 765: 1.292153393475256\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 766: 1.292269610830205\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 767: 1.2923991163261235\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 768: 1.2927435807040826\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 769: 1.2928092327984897\n",
            "\tContrastive Term: -0.080\n",
            "Checkpoint saved.\n",
            "Average loss after batch 770: 1.2930532251659843\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 771: 1.2931496465453212\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 772: 1.2931679012241832\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 773: 1.2935102846887376\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 774: 1.2936908031279042\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 775: 1.2937186747482143\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 776: 1.293631166871757\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 777: 1.2938820696735136\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 778: 1.2939622867398146\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 779: 1.2943981817135444\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 780: 1.2945882069774535\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 781: 1.294916031763072\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 782: 1.2951068508213963\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 783: 1.2950808938060487\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 784: 1.2950986177298673\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 785: 1.2952021031282634\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 786: 1.2954708212061183\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 787: 1.295539599687315\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 788: 1.2960114607490665\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 789: 1.2961427045773857\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 790: 1.296328752592451\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 791: 1.2966012468542716\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 792: 1.2967276955011664\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 793: 1.2968013923174189\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 794: 1.2971199392522657\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 795: 1.2971480211720394\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 796: 1.297261989849574\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 797: 1.297118336036988\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 798: 1.2969579153574156\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 799: 1.2971121357381343\n",
            "\tContrastive Term: -0.073\n",
            "Checkpoint saved.\n",
            "Average training loss: 1.2971121357381343\n",
            "Evaluating...\n",
            "Train: 0.4409324083555333 Validation: 0.07592198697953179 Test: 0.08612656808392181\n",
            "Train Loss: 1.2971121357381343\n",
            "2023.04.25-00:59:44\n",
            "Epoch 10 training...\n",
            "lr:  0.001\n",
            "Average loss after batch 0: 1.0063735246658325\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 1: 0.9361546635627747\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 2: 0.9822456439336141\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 3: 0.9691739082336426\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 4: 0.9413631081581115\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 5: 0.9560130735238394\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 6: 0.9961195928709847\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 7: 1.0010284408926964\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 8: 0.9975208242734274\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 9: 0.9934194207191467\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 10: 1.002985423261469\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 11: 0.993172804514567\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 12: 0.9855000605949988\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 13: 0.9874518683978489\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 14: 0.98668399254481\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 15: 0.9900180362164974\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 16: 0.9781505745999953\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 17: 0.9693404701020982\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 18: 0.9646624232593336\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 19: 0.9661220043897629\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 20: 0.9657825288318452\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 21: 0.9721425825899298\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 22: 0.9694981212201326\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 23: 0.9688685759902\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 24: 0.972148883342743\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 25: 0.9760023332559146\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 26: 0.9753782086902194\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 27: 0.971171926174845\n",
            "\tContrastive Term: -0.100\n",
            "Average loss after batch 28: 0.9787170332053612\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 29: 0.9778907895088196\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 30: 0.9802429214600594\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 31: 0.9813828580081463\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 32: 0.9797767310431509\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 33: 0.9830808972611147\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 34: 0.9844471641949245\n",
            "\tContrastive Term: -0.086\n",
            "Checkpoint saved.\n",
            "Average loss after batch 35: 0.9826676961448457\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 36: 0.9846306507651871\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 37: 0.9855959807571611\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 38: 0.9860877364109724\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 39: 0.9913032934069633\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 40: 0.9948604557572341\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 41: 0.9961323326542264\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 42: 0.995480477809906\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 43: 0.9947424029762094\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 44: 0.9942043701807658\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 45: 0.9910862484703893\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 46: 0.9894806806077349\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 47: 0.9864841401576996\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 48: 0.9887066033421731\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 49: 0.9863008666038513\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 50: 0.9852903426862231\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 51: 0.9856287791178777\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 52: 0.9853536284194803\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 53: 0.9858323810277162\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 54: 0.9875096852129156\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 55: 0.9894502365163395\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 56: 0.9894242704960338\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 57: 0.9941304120524176\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 58: 0.9959538609294568\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 59: 0.9994334022204081\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 60: 0.9975212783109947\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 61: 0.9988482046511865\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 62: 0.9970191338705638\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 63: 0.9991695303469896\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 64: 0.9988126965669485\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 65: 0.9990874086365555\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 66: 1.0001840956175505\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 67: 1.000635251402855\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 68: 1.001565979010817\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 69: 1.0025582815919603\n",
            "\tContrastive Term: -0.078\n",
            "Checkpoint saved.\n",
            "Average loss after batch 70: 1.0011605410508706\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 71: 1.0003000208073192\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 72: 1.0000636781731698\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 73: 1.0000049777933069\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 74: 1.0028921937942505\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 75: 1.001695675285239\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 76: 1.0004619313524914\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 77: 1.0013479773814862\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 78: 1.0010932442508167\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 79: 1.0018758684396745\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 80: 1.002454341193776\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 81: 1.0025581673877995\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 82: 1.0045574372073254\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 83: 1.0047534548100971\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 84: 1.0033519660725314\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 85: 1.00398899095003\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 86: 1.0026789896789639\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 87: 1.00336512720043\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 88: 1.004809417751398\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 89: 1.0035744422011905\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 90: 1.0015314339281438\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 91: 1.0031694465357324\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 92: 1.0019775475225141\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 93: 1.0023904011604634\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 94: 1.0047206414373298\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 95: 1.004532441496849\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 96: 1.0049055040497141\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 97: 1.0048395802780075\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 98: 1.0041948249845793\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 99: 1.0031601816415787\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 100: 1.003678012012255\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 101: 1.0054600829002904\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 102: 1.004377381894195\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 103: 1.005392423616006\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 104: 1.0060612820443653\n",
            "\tContrastive Term: -0.090\n",
            "Checkpoint saved.\n",
            "Average loss after batch 105: 1.005871002404195\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 106: 1.0052951217811799\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 107: 1.0047988450085674\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 108: 1.0036430364355036\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 109: 1.0043775683099574\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 110: 1.0041002362698048\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 111: 1.0043632978839534\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 112: 1.0046323469254823\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 113: 1.0056487609419906\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 114: 1.00588051599005\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 115: 1.0045161098241806\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 116: 1.0055201476455753\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 117: 1.0062505910962314\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 118: 1.0085034625870841\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 119: 1.0077040245135624\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 120: 1.00829982264968\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 121: 1.0095775029698357\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 122: 1.0100015091702221\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 123: 1.010876634428578\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 124: 1.0111279020309447\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 125: 1.0121302424915253\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 126: 1.0112704639359722\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 127: 1.0105760241858661\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 128: 1.0109970370928447\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 129: 1.0126762376381802\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 130: 1.011861394379885\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 131: 1.0129946434136592\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 132: 1.0120209715420143\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 133: 1.0130177798555857\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 134: 1.0128100245087235\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 135: 1.0138770850265728\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 136: 1.0138539867679568\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 137: 1.013342165860577\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 138: 1.013100862931862\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 139: 1.013635773743902\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 140: 1.0128034657620368\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 141: 1.0125308779763504\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 142: 1.0123518772058553\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 143: 1.0123810511496332\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 144: 1.011963953231943\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 145: 1.0132780079155752\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 146: 1.014720121613976\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 147: 1.0160028898232691\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 148: 1.0171967796831323\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 149: 1.0176456717650095\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 150: 1.0179542070193006\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 151: 1.0164231489363469\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 152: 1.0157851779382991\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 153: 1.0163075029850006\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 154: 1.0166251924730116\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 155: 1.0162149534011498\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 156: 1.0160211241169341\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 157: 1.015315373864355\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 158: 1.0144912049455463\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 159: 1.0142244912683964\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 160: 1.013765533888562\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 161: 1.0141157070059834\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 162: 1.0143576907965304\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 163: 1.0147226463730743\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 164: 1.0145835742805944\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 165: 1.0145022915788444\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 166: 1.0157837828476273\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 167: 1.0163855769094967\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 168: 1.0164532516835003\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 169: 1.0161761059480554\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 170: 1.0154127914306017\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 171: 1.015454890423043\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 172: 1.016018164640217\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 173: 1.0167680026470929\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 174: 1.0166697359085084\n",
            "\tContrastive Term: -0.093\n",
            "Checkpoint saved.\n",
            "Average loss after batch 175: 1.0162062103098088\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 176: 1.0168326325335746\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 177: 1.016789655337173\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 178: 1.0156481082878965\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 179: 1.0152279208103816\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 180: 1.0162820667851695\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 181: 1.016926102258347\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 182: 1.0168618051732172\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 183: 1.0163355835753938\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 184: 1.016553261151185\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 185: 1.0158935698770708\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 186: 1.0154866404074399\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 187: 1.0157470668249942\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 188: 1.0155807818685259\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 189: 1.0150826930999757\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 190: 1.0160434046340863\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 191: 1.015627044873933\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 192: 1.0146991508612362\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 193: 1.0154797004670213\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 194: 1.016259376819317\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 195: 1.016686504592701\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 196: 1.0172335708201838\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 197: 1.0172043189857944\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 198: 1.017825273413155\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 199: 1.018811236023903\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 200: 1.019354529048673\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 201: 1.0187517575698324\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 202: 1.0189284532528204\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 203: 1.0194965478251963\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 204: 1.0201482871683631\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 205: 1.0203560544449148\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 206: 1.0197796942531199\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 207: 1.0192071061867933\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 208: 1.0195482353274332\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 209: 1.0194793315160842\n",
            "\tContrastive Term: -0.082\n",
            "Checkpoint saved.\n",
            "Average loss after batch 210: 1.0197980838929308\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 211: 1.0192495592922535\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 212: 1.019455011181988\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 213: 1.0201811564859944\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 214: 1.0214112739230312\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 215: 1.0214918727676074\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 216: 1.0226499416311765\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 217: 1.0230078442927895\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 218: 1.0226846318266707\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 219: 1.0219287249174984\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 220: 1.021879987479335\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 221: 1.0227477094074626\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 222: 1.0234521670191812\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 223: 1.0239506661891937\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 224: 1.0240988302230836\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 225: 1.0235937253563805\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 226: 1.023033898569939\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 227: 1.0228023006204974\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 228: 1.0239938976462752\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 229: 1.0244644574497057\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 230: 1.0242385441090638\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 231: 1.0244370447150593\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 232: 1.024532571882649\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 233: 1.0242365990948474\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 234: 1.024456359985027\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 235: 1.0248812826003058\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 236: 1.0241959102546112\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 237: 1.0241222168718065\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 238: 1.0242057712008266\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 239: 1.0248558647930621\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 240: 1.0258573899625247\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 241: 1.0269224419574108\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 242: 1.0266888276539712\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 243: 1.0274969869461217\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 244: 1.0276345114318692\n",
            "\tContrastive Term: -0.082\n",
            "Checkpoint saved.\n",
            "Average loss after batch 245: 1.0274156763301632\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 246: 1.0276303069311599\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 247: 1.0285404391827122\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 248: 1.028669986380152\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 249: 1.029125364780426\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 250: 1.029648193800117\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 251: 1.0296503674416315\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 252: 1.0295340157309067\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 253: 1.0296147756689176\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 254: 1.029288263648164\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 255: 1.0289814958814532\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 256: 1.0293599708071015\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 257: 1.030010154311971\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 258: 1.029968486559437\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 259: 1.0306752885763462\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 260: 1.0307430236732367\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 261: 1.0310775721801146\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 262: 1.0311183482975108\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 263: 1.0313610215530251\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 264: 1.0312214831136308\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 265: 1.0315723658952498\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 266: 1.0322046478589375\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 267: 1.0322162408882112\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 268: 1.0322343352558887\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 269: 1.0324125866095224\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 270: 1.0327550056235817\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 271: 1.0327840471530663\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 272: 1.0332600998354482\n",
            "\tContrastive Term: -0.070\n",
            "Average loss after batch 273: 1.033444709368866\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 274: 1.033036242615093\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 275: 1.0329583336909611\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 276: 1.0331521053606854\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 277: 1.0333735327497662\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 278: 1.0335720151556007\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 279: 1.0335823350719042\n",
            "\tContrastive Term: -0.077\n",
            "Checkpoint saved.\n",
            "Average loss after batch 280: 1.0339744911923527\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 281: 1.0340642204098667\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 282: 1.0340354284633595\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 283: 1.0342985523838393\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 284: 1.0340787768363953\n",
            "\tContrastive Term: -0.068\n",
            "Average loss after batch 285: 1.033987280997363\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 286: 1.0335895786717377\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 287: 1.0339890987508826\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 288: 1.0343027671728167\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 289: 1.0344967196727621\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 290: 1.0344692628408216\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 291: 1.0348820710835391\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 292: 1.0348002243367478\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 293: 1.035282093245967\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 294: 1.0362939802266784\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 295: 1.0373898035771139\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 296: 1.0379232348817768\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 297: 1.0380760571300582\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 298: 1.0383874233749797\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 299: 1.0380468342701594\n",
            "\tContrastive Term: -0.074\n",
            "Evaluating...\n",
            "Train: 0.46826154817404814 Validation: 0.07956457113664459 Test: 0.08845235698024108\n",
            "Average loss after batch 300: 1.0378004728361618\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 301: 1.0374564747541946\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 302: 1.0380688171968995\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 303: 1.0380553197311728\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 304: 1.039186179637909\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 305: 1.039588283284817\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 306: 1.0402121961310942\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 307: 1.0405149353402001\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 308: 1.040875380286121\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 309: 1.0409606477906628\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 310: 1.0414405020486888\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 311: 1.0419531733943865\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 312: 1.0419648986654921\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 313: 1.0422776525567292\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 314: 1.0426365767206465\n",
            "\tContrastive Term: -0.076\n",
            "Checkpoint saved.\n",
            "Average loss after batch 315: 1.0429503199043153\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 316: 1.0430889548939486\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 317: 1.043287139456227\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 318: 1.0427131740650788\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 319: 1.043213647417724\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 320: 1.044076407995551\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 321: 1.0441296069148165\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 322: 1.0440748503702713\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 323: 1.044194413004098\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 324: 1.0450583239702078\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 325: 1.045046322367674\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 326: 1.045227603628001\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 327: 1.0454684505011977\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 328: 1.045587063741539\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 329: 1.0459313849608103\n",
            "\tContrastive Term: -0.067\n",
            "Average loss after batch 330: 1.0461517156430962\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 331: 1.0457730404583805\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 332: 1.0462248747771208\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 333: 1.0464391537055284\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 334: 1.0464660253097762\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 335: 1.0469664292676109\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 336: 1.0471522542772378\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 337: 1.0474959897571767\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 338: 1.0475697563109496\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 339: 1.0481839278165033\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 340: 1.048629887642399\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 341: 1.048125939585312\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 342: 1.0487389044928481\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 343: 1.0486662905923156\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 344: 1.048952863354614\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 345: 1.0495214643161421\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 346: 1.049323156175421\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 347: 1.0494088561370456\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 348: 1.0498031678377386\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 349: 1.0498468160629273\n",
            "\tContrastive Term: -0.072\n",
            "Checkpoint saved.\n",
            "Average loss after batch 350: 1.0498521694770226\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 351: 1.0498580329797484\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 352: 1.0498757929707383\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 353: 1.0498088887855832\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 354: 1.049671902119274\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 355: 1.0499563227208812\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 356: 1.0502812912484176\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 357: 1.0504508567921942\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 358: 1.0508735578372286\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 359: 1.0509882599115372\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 360: 1.0507174096279197\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 361: 1.050396551578743\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 362: 1.050899961598977\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 363: 1.0509026471402618\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 364: 1.0511595750508242\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 365: 1.0508211502611964\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 366: 1.0516069650000381\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 367: 1.0518520629924277\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 368: 1.0516864691323382\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 369: 1.051927188924841\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 370: 1.0522224864548428\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 371: 1.0521522875755065\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 372: 1.0521309116890218\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 373: 1.0526018302070903\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 374: 1.0531454855600992\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 375: 1.053169809757395\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 376: 1.0537460289836247\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 377: 1.053530015169628\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 378: 1.0539622317832504\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 379: 1.054243993602301\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 380: 1.0549411762730656\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 381: 1.0548286136844396\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 382: 1.0553242763402257\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 383: 1.055502390023321\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 384: 1.0554250272837553\n",
            "\tContrastive Term: -0.085\n",
            "Checkpoint saved.\n",
            "Average loss after batch 385: 1.0553901373104728\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 386: 1.0557679007219714\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 387: 1.0562716063457667\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 388: 1.0564056703854343\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 389: 1.0569521012978675\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 390: 1.0566391245178555\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 391: 1.0570957531430283\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 392: 1.057167403115571\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 393: 1.0571438254136118\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 394: 1.0568281649034235\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 395: 1.057052175354476\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 396: 1.0571967484969036\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 397: 1.0571445869141487\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 398: 1.0576130774684418\n",
            "\tContrastive Term: -0.066\n",
            "Average loss after batch 399: 1.0575483198463917\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 400: 1.058040601505603\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 401: 1.0590844612513015\n",
            "\tContrastive Term: -0.069\n",
            "Average loss after batch 402: 1.0600358056962933\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 403: 1.059925306403991\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 404: 1.0607055948104387\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 405: 1.0606575025419884\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 406: 1.0609802533426214\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 407: 1.0613421273289942\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 408: 1.0618968533128865\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 409: 1.0620911367055847\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 410: 1.062233934292248\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 411: 1.0624246989349717\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 412: 1.0630264560766427\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 413: 1.0626321548713002\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 414: 1.0632087160305805\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 415: 1.063554879134664\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 416: 1.0631881722157521\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 417: 1.063780706607554\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 418: 1.0640553763771967\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 419: 1.0644687512091229\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 420: 1.0642140421335036\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 421: 1.06419134591993\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 422: 1.0643419371147247\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 423: 1.0650064084889754\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 424: 1.0649529751609355\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 425: 1.0650475321801056\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 426: 1.0648467232248544\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 427: 1.0650089793951711\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 428: 1.065417462156647\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 429: 1.065946866606557\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 430: 1.065798689898648\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 431: 1.0659551081006173\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 432: 1.0659439094347436\n",
            "\tContrastive Term: -0.084\n"
          ]
        }
      ]
    }
  ]
}