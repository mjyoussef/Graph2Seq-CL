{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "NOTEBOOK_NAME = \"MLAP_test_eval\"\n",
        "\n",
        "# --- do not change below this ---\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "drive.mount(DRIVE_PATH, force_remount=True)\n",
        "\n",
        "# shell commands for directory with space must be\n",
        "# quoted, but not necessary in python\n",
        "COLAB_PATH = \"Colab Notebooks\"\n",
        "COLLAB_PATH_ESC = f\"\\\"{COLAB_PATH}\\\"\"\n",
        "\n",
        "# python path\n",
        "nb_path = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLAB_PATH, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "# shell path\n",
        "nb_path_bash = (\n",
        "    \"/\".join(('drive/MyDrive', \n",
        "              COLLAB_PATH_ESC, \n",
        "              \"venv_\" + NOTEBOOK_NAME)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "  os.makedirs(nb_path)\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Google Drive Folder already existed.\")\n",
        "\n",
        "try:\n",
        "  # create symlink from drive to workspace\n",
        "  os.symlink(nb_path, \"/content/notebooks\")\n",
        "except FileExistsError:\n",
        "  # already created in G-drive\n",
        "  print(\"Symlink already existed.\")\n",
        "\n",
        "sys.path.insert(0, nb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFQxATSSWofs",
        "outputId": "8142a45f-9fbc-44a7-eb2a-f47a0358b3c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Google Drive Folder already existed.\n",
            "Symlink already existed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch_geometric ogb"
      ],
      "metadata": {
        "id": "QJIm9RQtWpcW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "wkKs0jaewe9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "\n",
        "class ASTNodeEncoder(torch.nn.Module):\n",
        "    '''\n",
        "        Input:\n",
        "            x: default node feature. the first and second column represents node type and node attributes.\n",
        "            depth: The depth of the node in the AST.\n",
        "\n",
        "        Output:\n",
        "            emb_dim-dimensional vector\n",
        "\n",
        "    '''\n",
        "    def __init__(self, emb_dim, num_nodetypes, num_nodeattributes, max_depth):\n",
        "        super(ASTNodeEncoder, self).__init__()\n",
        "\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        self.type_encoder = torch.nn.Embedding(num_nodetypes, emb_dim)\n",
        "        self.attribute_encoder = torch.nn.Embedding(num_nodeattributes, emb_dim)\n",
        "        self.depth_encoder = torch.nn.Embedding(self.max_depth + 1, emb_dim)\n",
        "\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(3 * emb_dim, 2 * emb_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2 * emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, depth):\n",
        "        depth[depth > self.max_depth] = self.max_depth\n",
        "        mlp_input = torch.hstack(\n",
        "            (\n",
        "                self.type_encoder(x[:,0]), \n",
        "                self.attribute_encoder(x[:,1]), \n",
        "                self.depth_encoder(depth)\n",
        "             )\n",
        "        )\n",
        "        return self.node_mlp(mlp_input)"
      ],
      "metadata": {
        "id": "F5LoicGwwajW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils - AST / MLAP\n",
        "\n",
        "Utilities for editing and parsing the AST inputs."
      ],
      "metadata": {
        "id": "FamuAT6rV3G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_edge, degree, to_undirected, scatter, to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "def get_vocab_mapping(seq_list, num_vocab):\n",
        "    '''\n",
        "        Input:\n",
        "            seq_list: a list of sequences\n",
        "            num_vocab: vocabulary size\n",
        "        Output:\n",
        "            vocab2idx:\n",
        "                A dictionary that maps vocabulary into integer index.\n",
        "                Additioanlly, we also index '__UNK__' and '__EOS__'\n",
        "                '__UNK__' : out-of-vocabulary term\n",
        "                '__EOS__' : end-of-sentence\n",
        "\n",
        "            idx2vocab:\n",
        "                A list that maps idx to actual vocabulary.\n",
        "    '''\n",
        "\n",
        "    vocab_cnt = {}\n",
        "    vocab_list = []\n",
        "    for seq in seq_list:\n",
        "        for w in seq:\n",
        "            if w in vocab_cnt:\n",
        "                vocab_cnt[w] += 1\n",
        "            else:\n",
        "                vocab_cnt[w] = 1\n",
        "                vocab_list.append(w)\n",
        "\n",
        "    cnt_list = np.array([vocab_cnt[w] for w in vocab_list])\n",
        "    topvocab = np.argsort(-cnt_list, kind = 'stable')[:num_vocab]\n",
        "\n",
        "    print('Coverage of top {} vocabulary:'.format(num_vocab))\n",
        "    print(float(np.sum(cnt_list[topvocab]))/np.sum(cnt_list))\n",
        "\n",
        "    vocab2idx = {vocab_list[vocab_idx]: idx for idx, vocab_idx in enumerate(topvocab)}\n",
        "    idx2vocab = [vocab_list[vocab_idx] for vocab_idx in topvocab]\n",
        "\n",
        "    vocab2idx['__UNK__'] = num_vocab\n",
        "    idx2vocab.append('__UNK__')\n",
        "\n",
        "    vocab2idx['__EOS__'] = num_vocab + 1\n",
        "    idx2vocab.append('__EOS__')\n",
        "\n",
        "    # test the correspondence between vocab2idx and idx2vocab\n",
        "    for idx, vocab in enumerate(idx2vocab):\n",
        "        assert(idx == vocab2idx[vocab])\n",
        "\n",
        "    # test that the idx of '__EOS__' is len(idx2vocab) - 1.\n",
        "    # This fact will be used in decode_arr_to_seq, when finding __EOS__\n",
        "    assert(vocab2idx['__EOS__'] == len(idx2vocab) - 1)\n",
        "\n",
        "    return vocab2idx, idx2vocab\n",
        "\n",
        "def augment_edge(data):\n",
        "    '''\n",
        "        Input:\n",
        "            data: PyG data object\n",
        "        Output:\n",
        "            data (edges are augmented in the following ways):\n",
        "                data.edge_index: Added next-token edge. The inverse edges were also added.\n",
        "                data.edge_attr (torch.Long):\n",
        "                    data.edge_attr[:,0]: whether it is AST edge (0) for next-token edge (1)\n",
        "                    data.edge_attr[:,1]: whether it is original direction (0) or inverse direction (1)\n",
        "    '''\n",
        "    ##### AST edge\n",
        "    edge_index_ast = data.edge_index\n",
        "    edge_attr_ast = torch.zeros((edge_index_ast.size(1), 2))\n",
        "\n",
        "    ##### Inverse AST edge\n",
        "    edge_index_ast_inverse = torch.stack([edge_index_ast[1], edge_index_ast[0]], dim = 0)\n",
        "    edge_attr_ast_inverse = torch.cat([torch.zeros(edge_index_ast_inverse.size(1), 1), torch.ones(edge_index_ast_inverse.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Next-token edge\n",
        "\n",
        "    ## Obtain attributed nodes and get their indices in dfs order\n",
        "    # attributed_node_idx = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "    # attributed_node_idx_in_dfs_order = attributed_node_idx[torch.argsort(data.node_dfs_order[attributed_node_idx].view(-1,))]\n",
        "\n",
        "    ## Since the nodes are already sorted in dfs ordering in our case, we can just do the following.\n",
        "    attributed_node_idx_in_dfs_order = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n",
        "\n",
        "    ## build next token edge\n",
        "    # Given: attributed_node_idx_in_dfs_order\n",
        "    #        [1, 3, 4, 5, 8, 9, 12]\n",
        "    # Output:\n",
        "    #    [[1, 3, 4, 5, 8, 9]\n",
        "    #     [3, 4, 5, 8, 9, 12]\n",
        "    edge_index_nextoken = torch.stack([attributed_node_idx_in_dfs_order[:-1], attributed_node_idx_in_dfs_order[1:]], dim = 0)\n",
        "    edge_attr_nextoken = torch.cat([torch.ones(edge_index_nextoken.size(1), 1), torch.zeros(edge_index_nextoken.size(1), 1)], dim = 1)\n",
        "\n",
        "    ##### Inverse next-token edge\n",
        "    edge_index_nextoken_inverse = torch.stack([edge_index_nextoken[1], edge_index_nextoken[0]], dim = 0)\n",
        "    edge_attr_nextoken_inverse = torch.ones((edge_index_nextoken.size(1), 2))\n",
        "\n",
        "    data.edge_index = torch.cat([edge_index_ast, edge_index_ast_inverse, edge_index_nextoken, edge_index_nextoken_inverse], dim = 1)\n",
        "    data.edge_attr = torch.cat([edge_attr_ast,   edge_attr_ast_inverse, edge_attr_nextoken,  edge_attr_nextoken_inverse], dim = 0)\n",
        "\n",
        "    return data\n",
        "\n",
        "def encode_y_to_arr(data, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        data: PyG graph object\n",
        "        output: add y_arr to data \n",
        "    '''\n",
        "    # PyG >= 1.5.0\n",
        "    seq = data.y\n",
        "    data.y_arr = encode_seq_to_arr(seq, vocab2idx, max_seq_len)\n",
        "    return data\n",
        "\n",
        "def encode_seq_to_arr(seq, vocab2idx, max_seq_len):\n",
        "    '''\n",
        "    Input:\n",
        "        seq: A list of words\n",
        "        output: add y_arr (torch.Tensor)\n",
        "    '''\n",
        "    augmented_seq = seq[:max_seq_len] + ['__EOS__'] * max(0, max_seq_len - len(seq))\n",
        "    return torch.tensor([[vocab2idx[w] if w in vocab2idx else vocab2idx['__UNK__'] for w in augmented_seq]], dtype = torch.long)\n",
        "\n",
        "\n",
        "def decode_arr_to_seq(arr, idx2vocab):\n",
        "    '''\n",
        "        Input: torch 1d array: y_arr\n",
        "        Output: a sequence of words.\n",
        "    '''\n",
        "    # find the position of __EOS__ (the last vocab in idx2vocab)\n",
        "    eos_idx_list = (arr == len(idx2vocab) - 1).nonzero() \n",
        "    if len(eos_idx_list) > 0:\n",
        "        # find the smallest __EOS__\n",
        "        clippted_arr = arr[: torch.min(eos_idx_list)] \n",
        "    else:\n",
        "        clippted_arr = arr\n",
        "\n",
        "    return list(map(lambda x: idx2vocab[x], clippted_arr.cpu()))"
      ],
      "metadata": {
        "id": "a8ZPzicd-pjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils - CAP\n",
        "Utilities for generating Graph Contrastive Pairs"
      ],
      "metadata": {
        "id": "DP9s6wJ1-0Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CAP functions ----\n",
        "# from: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/functional.py\n",
        "# and: https://github.com/CRIPAC-DIG/GCA/blob/cd6a9f0cf06c0b8c48e108a6aab743585f6ba6f1/pGRACE/utils.py\n",
        "def compute_pr(edge_index, damp: float = 0.85, k: int = 10):\n",
        "    # page rank\n",
        "    # interesting comment: https://github.com/CRIPAC-DIG/GCA/issues/4\n",
        "    num_nodes = edge_index.max().item() + 1\n",
        "    deg_out = degree(edge_index[0])\n",
        "    x = torch.ones((num_nodes, )).to(edge_index.device).to(torch.float32)\n",
        "\n",
        "    for i in range(k):\n",
        "        edge_msg = x[edge_index[0]] / deg_out[edge_index[0]]\n",
        "        agg_msg = scatter(edge_msg, edge_index[1], reduce='sum')\n",
        "\n",
        "        x = (1 - damp) * x + damp * agg_msg\n",
        "\n",
        "    return x\n",
        "\n",
        "def eigenvector_centrality(data):\n",
        "    graph = to_networkx(data)\n",
        "    x = nx.eigenvector_centrality_numpy(graph)\n",
        "    x = [x[i] for i in range(data.num_nodes)]\n",
        "    return torch.tensor(x, dtype=torch.float32).to(data.edge_index.device)\n",
        "\n",
        "\n",
        "def drop_feature(x, drop_prob):\n",
        "    drop_mask = torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1) < drop_prob\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_feature_weighted(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w.repeat(x.size(0)).view(x.size(0), -1)\n",
        "\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "def drop_feature_weighted_2(x, w, p: float, threshold: float = 0.7, dgi_task=False):\n",
        "    w = w / w.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w\n",
        "\n",
        "    if (dgi_task):\n",
        "        drop_mask = torch.bernoulli(1. - drop_prob).to(torch.bool)\n",
        "    else:\n",
        "        drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0.\n",
        "\n",
        "    return x\n",
        "\n",
        "def feature_drop_weights(x, node_c):\n",
        "    x = x.to(torch.bool).to(torch.float32)\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def feature_drop_weights_dense(x, node_c):\n",
        "    x = x.abs()\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def drop_edge_weighted(edge_index, edge_weights, p: float, threshold: float = 1., dgi_task=False):\n",
        "    edge_weights = edge_weights / edge_weights.mean() * p\n",
        "    # if (dgi_task):\n",
        "    #     threshold = 0.9\n",
        "\n",
        "    edge_weights = edge_weights.where(edge_weights < threshold, torch.ones_like(edge_weights) * threshold)\n",
        "\n",
        "    if (dgi_task): \n",
        "        # drop edges by importance\n",
        "        sel_mask = torch.bernoulli(edge_weights).to(torch.bool)\n",
        "    else:\n",
        "        sel_mask = torch.bernoulli(1. - edge_weights).to(torch.bool)\n",
        "\n",
        "    return edge_index[:, sel_mask]\n",
        "\n",
        "\n",
        "def degree_drop_weights(edge_index):\n",
        "    edge_index_ = to_undirected(edge_index)\n",
        "    deg = degree(edge_index_[1])\n",
        "    deg_col = deg[edge_index[1]].to(torch.float32)\n",
        "    s_col = torch.log(deg_col)\n",
        "    weights = (s_col.max() - s_col) / (s_col.max() - s_col.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def pr_drop_weights(edge_index, aggr: str = 'sink', k: int = 10):\n",
        "    pv = compute_pr(edge_index, k=k)\n",
        "    pv_row = pv[edge_index[0]].to(torch.float32)\n",
        "    pv_col = pv[edge_index[1]].to(torch.float32)\n",
        "    s_row = torch.log(pv_row)\n",
        "    s_col = torch.log(pv_col)\n",
        "    if aggr == 'sink':\n",
        "        s = s_col\n",
        "    elif aggr == 'source':\n",
        "        s = s_row\n",
        "    elif aggr == 'mean':\n",
        "        s = (s_col + s_row) * 0.5\n",
        "    else:\n",
        "        s = s_col\n",
        "    weights = (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def evc_drop_weights(data):\n",
        "    evc = eigenvector_centrality(data)\n",
        "    evc = evc.where(evc > 0, torch.zeros_like(evc))\n",
        "    evc = evc + 1e-8\n",
        "    s = evc.log()\n",
        "\n",
        "    edge_index = data.edge_index\n",
        "    s_row, s_col = s[edge_index[0]], s[edge_index[1]]\n",
        "    s = s_col\n",
        "\n",
        "    return (s.max() - s) / (s.max() - s.mean())\n",
        "\n",
        "def graph_perturb(data, drop_scheme='pr'):\n",
        "  if drop_scheme == 'degree':\n",
        "      drop_weights = degree_drop_weights(data.edge_index)\n",
        "      edge_index_ = to_undirected(data.edge_index)\n",
        "      node_deg = degree(edge_index_[1])\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_deg)\n",
        "  elif drop_scheme == 'pr':\n",
        "      drop_weights = pr_drop_weights(data.edge_index, aggr='sink', k=200)\n",
        "      node_pr = compute_pr(data.edge_index)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_pr)\n",
        "  elif drop_scheme == 'evc':\n",
        "      drop_weights = evc_drop_weights(data)\n",
        "      node_evc = eigenvector_centrality(data)\n",
        "      feature_weights = feature_drop_weights(data.x, node_c=node_evc)\n",
        "  else:\n",
        "      feature_weights = torch.ones((data.x.size(1),))\n",
        "      drop_weights = None\n",
        "  \n",
        "  return feature_weights, drop_weights\n",
        "\n",
        "def drop_edge(data, drop_edge_rate, drop_weights, drop_scheme='pr', drop_edge_weighted_threshold=0.7, dgi_task=False):\n",
        "  if drop_scheme == 'uniform':\n",
        "      return dropout_edge(data.edge_index, p=drop_edge_rate)[0]\n",
        "  elif drop_scheme in ['degree', 'evc', 'pr']:\n",
        "      return drop_edge_weighted(\n",
        "          data.edge_index, \n",
        "          drop_weights, \n",
        "          p=drop_edge_rate, \n",
        "          threshold=drop_edge_weighted_threshold,\n",
        "          dgi_task=dgi_task\n",
        "        )\n",
        "  else:\n",
        "      raise Exception(f'undefined drop scheme: {drop_scheme}')\n",
        "\n",
        "def get_contrastive_graph_pair(data, drop_scheme='pr', drop_feature_rates=(0.7, 0.7), drop_edge_rates=(0.5, 0.5), dgi_task=False):\n",
        "  # use augmentation scheme to determine the weights of each node\n",
        "  # i.e. pagerank, eigenvector centrality, node degree\n",
        "  feat_weights, drop_weights = graph_perturb(data, drop_scheme)\n",
        "\n",
        "  # apply drop edge according to computed features\n",
        "  dr_e_1, dr_e_2 = drop_edge_rates\n",
        "  edge_index_1 = drop_edge(data, dr_e_1, drop_weights, drop_scheme, dgi_task=dgi_task)\n",
        "\n",
        "  if (not dgi_task):\n",
        "    edge_index_2 = drop_edge(data, dr_e_2, drop_weights, drop_scheme)\n",
        "\n",
        "  dr_f_1, dr_f_2 = drop_feature_rates\n",
        "\n",
        "  if drop_scheme in ['pr', 'degree', 'evc']:\n",
        "    # graph-aware drop feature\n",
        "    x_1 = drop_feature_weighted_2(data.x, feat_weights, dr_f_1, dgi_task=dgi_task)\n",
        "    #e_1 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_1)\n",
        "\n",
        "    if not dgi_task:\n",
        "        x_2 = drop_feature_weighted_2(data.x, feat_weights, dr_f_2, dgi_task=dgi_task)\n",
        "        #e_2 = drop_feature_weighted_2(data.edge_attr, feat_weights, dr_f_2)\n",
        "  else:\n",
        "    # naive drop feature\n",
        "    x_1 = drop_feature(data.x, dr_f_1)\n",
        "    #e_1 = drop_feature(data.edge_attr, dr_f_1)\n",
        "    \n",
        "    x_2 = drop_feature(data.x, dr_f_2)\n",
        "    e_2 = drop_feature(data.edge_attr, dr_f_2)\n",
        "  \n",
        "  if dgi_task:\n",
        "      return (x_1, edge_index_1)\n",
        "\n",
        "  return (\n",
        "      # graph 1\n",
        "      (x_1, edge_index_1),\n",
        "      # graph 2\n",
        "      (x_2, edge_index_2)\n",
        "  )"
      ],
      "metadata": {
        "id": "6QXwE3BnV2vf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GIN"
      ],
      "metadata": {
        "id": "ng-TvX2UVlWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, dim_h, mlp, **kwargs):\n",
        "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.mlp = mlp\n",
        "        self.bn = BatchNorm1d(dim_h)\n",
        "        self.edge_encoder = Linear(2, dim_h)\n",
        "\n",
        "    \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        output = self.mlp(\n",
        "            self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "        )\n",
        "        return self.bn(output)\n",
        "    \n",
        "    def message(self, x_j, edge_attr):\n",
        "        return x_j + edge_attr\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        return aggr_out + x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ],
      "metadata": {
        "id": "8u0cRmJ1VkKV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoders"
      ],
      "metadata": {
        "id": "KSAs0mAoVjXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LinearDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        \"\"\"\n",
        "        Noted in the MLAP paper to have performed better than the LSTM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [nn.Linear(dim_h, len(vocab2idx)) for _ in range(max_seq_len)]\n",
        "        )\n",
        "\n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        return [d(layer_reps[-1]) for d in self.decoders]\n",
        "\n",
        "\n",
        "class LSTMDecoder(torch.nn.Module):\n",
        "    def __init__(self, dim_h, max_seq_len, vocab2idx, device):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "        self.lstm = nn.LSTMCell(dim_h, dim_h)\n",
        "        self.w_hc = nn.Linear(dim_h * 2, dim_h)\n",
        "        self.layernorm = nn.LayerNorm(dim_h)\n",
        "        self.vocab_encoder = nn.Embedding(len(vocab2idx), dim_h)\n",
        "        self.vocab_bias = nn.Parameter(torch.zeros(len(vocab2idx)))\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, batch_size, layer_reps, labels, training=False):\n",
        "        if (training):\n",
        "            batched_label = torch.vstack(\n",
        "                [\n",
        "                    encode_seq_to_arr(label, self.vocab2idx, self.max_seq_len - 1) \n",
        "                    for label in labels\n",
        "                ]\n",
        "            )\n",
        "            batched_label = torch.hstack((torch.zeros((batch_size, 1), dtype=torch.int64), batched_label))\n",
        "            true_emb = self.vocab_encoder(batched_label.to(device=self.device))\n",
        "        \n",
        "        h_t, c_t = layer_reps[-1].clone(), layer_reps[-1].clone()\n",
        "\n",
        "        layer_reps = layer_reps.transpose(0,1)\n",
        "        output = []\n",
        "\n",
        "        pred_emb = self.vocab_encoder(torch.zeros((batch_size), dtype=torch.int64, device=self.device))\n",
        "        vocab_mat = self.vocab_encoder(torch.arange(len(self.vocab2idx), dtype=torch.int64, device=self.device))\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            if (training): \n",
        "                # teacher forcing\n",
        "                input = true_emb[:, i]\n",
        "            else:\n",
        "                input = pred_emb\n",
        "            \n",
        "            h_t, c_t = self.lstm(input, (h_t, c_t))\n",
        "\n",
        "            # (batch_size, L + 1)\n",
        "            a = F.softmax(torch.bmm(layer_reps, h_t.unsqueeze(-1)).squeeze(-1), dim=1)  \n",
        "            context = torch.bmm(a.unsqueeze(1), layer_reps).squeeze(1)\n",
        "\n",
        "            # (batch_size, dim_h)\n",
        "            pred_emb = torch.tanh(self.layernorm(self.w_hc(torch.hstack((h_t, context)))))  \n",
        "\n",
        "            # (batch_size, len(vocab)) x max_seq_len\n",
        "            output.append(torch.matmul(pred_emb, vocab_mat.T) + self.vocab_bias.unsqueeze(0))\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "JhqdRjDJVpbo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLAP"
      ],
      "metadata": {
        "id": "WBwnEmwhVxwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear, Sequential, ReLU, ELU, Sigmoid\n",
        "\n",
        "from torch_geometric.nn.conv import GINConv\n",
        "from torch_geometric.nn.norm import GraphNorm\n",
        "from torch_geometric.nn.glob import AttentionalAggregation\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class DISC(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super(DISC, self).__init__()\n",
        "\n",
        "        W = torch.empty(dim_h, dim_h)\n",
        "        torch.nn.init.xavier_normal_(W)\n",
        "\n",
        "        self.W = torch.nn.Parameter(W)\n",
        "        self.W.requires_grad = True\n",
        "\n",
        "        self.sig = Sigmoid()\n",
        "    \n",
        "    def forward(self, h, s):\n",
        "        out = torch.matmul(self.W, s)\n",
        "        out = torch.matmul(h, out.unsqueeze(-1))\n",
        "        return self.sig(out)\n",
        "\n",
        "\n",
        "class MLAP_GIN(torch.nn.Module):\n",
        "    def __init__(self, dim_h, batch_size, depth, node_encoder, norm=False, residual=False, dropout=False):\n",
        "        super(MLAP_GIN, self).__init__()\n",
        "\n",
        "        self.dim_h = dim_h\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "\n",
        "        self.node_encoder = node_encoder\n",
        "\n",
        "        self.norm = norm\n",
        "        self.residual = residual\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.loss_fn = torch.nn.BCELoss(reduction='sum')\n",
        "        self.discriminator = DISC(dim_h)\n",
        "\n",
        "        # non-linear projection function for cl task\n",
        "        self.projection = Sequential(\n",
        "            Linear(dim_h, int(dim_h/8)),\n",
        "            ELU(),\n",
        "            Linear(int(dim_h/8), dim_h)\n",
        "        )\n",
        "\n",
        "        # GIN layers\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [GINConv(Sequential(\n",
        "                Linear(dim_h, dim_h),\n",
        "                ReLU(),\n",
        "                Linear(dim_h, dim_h))) for _ in range(depth)])\n",
        "            \n",
        "        # normalization layers\n",
        "        self.norm = torch.nn.ModuleList([GraphNorm(dim_h) for _ in range(self.depth)])\n",
        "        \n",
        "        # layer-wise attention poolings\n",
        "        self.att_poolings = torch.nn.ModuleList(\n",
        "            [\n",
        "                AttentionalAggregation(\n",
        "                Sequential(Linear(self.dim_h, 2*self.dim_h), \n",
        "                           ReLU(), \n",
        "                           Linear(2*self.dim_h, 1))) \n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def contrastive_loss(self, g1_x, g2_x):\n",
        "        # compute projections + L2 row-wise normalizations\n",
        "        g1_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g1_x), p=2, dim=1\n",
        "        )\n",
        "        g2_projections = torch.nn.functional.normalize(\n",
        "            self.projection(g2_x), p=2, dim=1\n",
        "        )\n",
        "        \n",
        "        g1_proj_T = torch.transpose(g1_projections, 0, 1)\n",
        "        g2_proj_T = torch.transpose(g2_projections, 0, 1)\n",
        "\n",
        "        inter_g1 = torch.exp(torch.matmul(g1_projections, g1_proj_T))\n",
        "        inter_g2 = torch.exp(torch.matmul(g2_projections, g2_proj_T))\n",
        "        intra_view = torch.exp(torch.matmul(g1_projections, g2_proj_T))\n",
        "\n",
        "        # main diagonal\n",
        "        corresponding_terms = torch.diagonal(intra_view, 0) \n",
        "        non_matching_intra = torch.diagonal(intra_view, -1).sum()\n",
        "        non_matching_inter_g1 = torch.diagonal(inter_g1, -1).sum()\n",
        "        non_matching_inter_g2 = torch.diagonal(inter_g2, -1).sum()\n",
        "\n",
        "        # inter-view pairs using g1\n",
        "        corresponding_terms_g1 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g1 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g1 = torch.log(corresponding_terms_g1)\n",
        "\n",
        "        # inter-view pairs using g2\n",
        "        corresponding_terms_g2 = corresponding_terms / (\n",
        "            corresponding_terms + \n",
        "            non_matching_inter_g2 + \n",
        "            non_matching_intra\n",
        "        )\n",
        "        corresponding_terms_g2 = torch.log(corresponding_terms_g2)\n",
        "\n",
        "        # contrasting terms of both divided by total nodes\n",
        "        loss = (\n",
        "            corresponding_terms_g1.sum() + \n",
        "            corresponding_terms_g2.sum()\n",
        "        ) / (\n",
        "            g1_x.shape[0] + \n",
        "            g2_x.shape[0]\n",
        "        )\n",
        "        \n",
        "        loss = loss / self.batch_size\n",
        "        return loss\n",
        "    \n",
        "    def layer_loop(self, x, edge_index, batch, cl=False, cl_all=False, dgi_task=False):\n",
        "        cl_embs = []\n",
        "        for d in range(self.depth):\n",
        "            x_in = x\n",
        "\n",
        "            # get node representation at layer d\n",
        "            x = self.layers[d](x, edge_index)\n",
        "            \n",
        "            if self.norm:\n",
        "                x = self.norm[d](x, batch)\n",
        "            \n",
        "            if d < self.depth - 1:\n",
        "                x = F.relu(x)\n",
        "            \n",
        "            if self.dropout:\n",
        "                x = F.dropout(x)\n",
        "            \n",
        "            if self.residual:\n",
        "                x = x + x_in\n",
        "\n",
        "            if not cl:\n",
        "                # use attention pooling for given depth\n",
        "                h_g = self.att_poolings[d](x, batch)\n",
        "                self.graph_embs.append(h_g)\n",
        "\n",
        "            if (\n",
        "                (cl and cl_all) or \n",
        "                (cl and (d == self.depth-1)) or \n",
        "                (dgi_task and (d == self.depth-1))\n",
        "            ):\n",
        "                # if using contrastive learning or DGI\n",
        "                cl_embs += [x]\n",
        "            \n",
        "        return cl_embs\n",
        "\n",
        "    def forward(self, batched_data, cl=False, cl_all=False, dgi_task=False):\n",
        "        self.graph_embs = []\n",
        "        # non-augmented graph\n",
        "        # note: populates self.graph_embs\n",
        "\n",
        "        node_depth = batched_data.node_depth\n",
        "        x_emb = self.node_encoder(batched_data.x, node_depth.view(-1,))\n",
        "        edge_index = batched_data.edge_index\n",
        "        batch = batched_data.batch\n",
        "\n",
        "        self.layer_loop(x_emb, edge_index, batch, dgi_task=dgi_task)\n",
        "\n",
        "        agg = self.aggregate()\n",
        "        self.graph_embs.append(agg)\n",
        "        output = torch.stack(self.graph_embs, dim=0)\n",
        "\n",
        "        # dgi task\n",
        "        dgi_loss = 0\n",
        "        if dgi_task:\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                nd = g.node_depth\n",
        "                b = g.batch\n",
        "                \n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=True)\n",
        "                g_diff_embs = self.layer_loop(g1, g2, b, dgi_task=True)[0]\n",
        "\n",
        "                g.x = self.node_encoder(g.x, nd.view(-1,).clone())\n",
        "                g_embs = self.layer_loop(g.x, g.edge_index, g.batch, dgi_task=True)[0]\n",
        "\n",
        "                # dgi objective on final_layer_embs, g_diff_embs, and output\n",
        "                agg = agg.clone()\n",
        "                positive = self.discriminator(g_embs, agg[i])\n",
        "                ones = torch.ones_like(positive)\n",
        "                negative = self.discriminator(g_diff_embs, agg[i])\n",
        "                zeros = torch.zeros_like(negative)\n",
        "\n",
        "                dgi_loss += (\n",
        "                    self.loss_fn(positive, ones) + self.loss_fn(negative, zeros)\n",
        "                ) / (positive.shape[0] + negative.shape[0])\n",
        "            \n",
        "            dgi_loss /= (self.batch_size // 5)\n",
        "\n",
        "        # contrastive learning task\n",
        "        cl_loss = 0\n",
        "        if cl:\n",
        "            for i in range(self.batch_size // 5):\n",
        "                g = batched_data.get_example(i)\n",
        "\n",
        "                # contrastive pair\n",
        "                g1, g2 = self.get_contrastive_pair_from_batch(g, dgi_task=False)\n",
        "                g1_embs = self.get_node_embedding(g.batch, g1, cl=True, cl_all=cl_all)\n",
        "                g2_embs = self.get_node_embedding(g.batch, g2, cl=True, cl_all=cl_all)\n",
        "\n",
        "                batch_cl_loss = 0\n",
        "                for j in range(len(g1_embs)):\n",
        "                    batch_cl_loss += self.contrastive_loss(g1_embs[j], g2_embs[j])\n",
        "                \n",
        "                batch_cl_loss = batch_cl_loss / len(g1_embs)\n",
        "                cl_loss += batch_cl_loss\n",
        "            \n",
        "            cl_loss /= (self.batch_size // 5)\n",
        "\n",
        "        return output, cl_loss, dgi_loss\n",
        "\n",
        "    def get_node_embedding(self, batch, g, cl, cl_all):\n",
        "        g_x, g_edge_index = g\n",
        "        return self.layer_loop(\n",
        "            g_x.clone(), \n",
        "            g_edge_index, \n",
        "            batch, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all\n",
        "        )\n",
        "\n",
        "    def get_contrastive_pair_from_batch(self, g, dgi_task=False):\n",
        "        g_clone = g.clone()\n",
        "        nd = g.node_depth\n",
        "        # encode the nodes in the clone of g using given encoding network\n",
        "        g_clone.x = self.node_encoder(g_clone.x, nd.view(-1,).clone())\n",
        "\n",
        "        # create contrastive pairs from the input graph\n",
        "        return get_contrastive_graph_pair(g_clone, dgi_task=dgi_task)\n",
        "\n",
        "    def aggregate(self):\n",
        "        pass\n",
        "\n",
        "class MLAP_Sum(MLAP_GIN):\n",
        "    def aggregate(self):\n",
        "        return torch.stack(self.graph_embs, dim=0).sum(dim=0)\n",
        "\n",
        "class MLAP_Weighted(MLAP_GIN):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weight = torch.nn.Parameter(torch.ones(self.depth, 1, 1))\n",
        "\n",
        "    def aggregate(self):\n",
        "        a = F.softmax(self.weight, dim=0)\n",
        "        h = torch.stack(self.graph_embs, dim=0)\n",
        "        return (a * h).sum(dim=0)"
      ],
      "metadata": {
        "id": "NDywjQj9VyVd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "6fnFG6_RV714"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, batch_size, depth, dim_h, max_seq_len, node_encoder, vocab2idx, device):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.depth = depth\n",
        "        self.dim_h = dim_h\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # token to idx lookup\n",
        "        self.vocab2idx = vocab2idx \n",
        "        \n",
        "        # architecture choices\n",
        "        self.node_encoder = node_encoder\n",
        "        self.gnn = MLAP_Weighted(\n",
        "            dim_h, batch_size, depth, \n",
        "            node_encoder, \n",
        "            norm=True, \n",
        "            residual=True, \n",
        "            dropout=True\n",
        "        )\n",
        "        self.decoder = LinearDecoder(\n",
        "            dim_h, max_seq_len, vocab2idx, device\n",
        "        )\n",
        "\n",
        "    def forward(self, batched_data, labels, training=False, cl=False, cl_all=False, dgi_task=False):\n",
        "        # GNN layer, contrastive work done here\n",
        "        embeddings, cl_loss, dgi_loss = self.gnn(\n",
        "            batched_data, \n",
        "            cl=cl, \n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task\n",
        "        )\n",
        "\n",
        "        predictions = self.decoder(len(labels), embeddings, labels, training=training)\n",
        "\n",
        "        # for each batch, the prediction for the ith word is a logit\n",
        "        # decoding each prediction to a word is done in the evaluation task in main\n",
        "        return predictions, cl_loss, dgi_loss"
      ],
      "metadata": {
        "id": "7ft4PTPWV8eO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $nb_path_bash && mkdir \"checkpoints\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iErB7DcYCXm",
        "outputId": "5192ece8-d19d-4351-cc37-5025db1a71bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main\n",
        "\n",
        "Model configuration and training loop."
      ],
      "metadata": {
        "id": "BwGxp_PRWAMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "\n",
        "\n",
        "def train(model, device, loader, optimizer, scheduler, multicls_criterion, epoch, \n",
        "          alpha=0.05, \n",
        "          cl=False, \n",
        "          cl_all=False, \n",
        "          dgi_task=False,\n",
        "          eval_hook=lambda x: x,\n",
        "    ):\n",
        "    # total loss for this epoch\n",
        "    loss_accum = 0\n",
        "\n",
        "    chkpt_folder = nb_path + '/checkpoints/epoch' + str(epoch)\n",
        "    if not os.path.exists(chkpt_folder):\n",
        "        os.mkdir(chkpt_folder)\n",
        "\n",
        "    if cl and dgi_task:\n",
        "        raise Exception(\"Cannot use both a contrastive and dgi loss term\\n\")\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        # run eval if requested\n",
        "        eval_hook(step)\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            # train\n",
        "            labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "            pred_list, cl_loss, dgi_loss = model(\n",
        "                batch, labels, training=True,\n",
        "                cl=cl, \n",
        "                cl_all=cl_all, \n",
        "                dgi_task=dgi_task\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss + update\n",
        "            loss = 0\n",
        "            for i in range(len(pred_list)):\n",
        "                loss += (1-alpha) * multicls_criterion(\n",
        "                    pred_list[i].to(torch.float32), \n",
        "                    batch.y_arr[:, i]\n",
        "                )\n",
        "\n",
        "            loss /= len(pred_list)\n",
        "            if cl:\n",
        "                loss -= alpha * cl_loss\n",
        "            if dgi_task:\n",
        "                loss -= alpha * dgi_loss\n",
        "\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # report loss after batch\n",
        "            loss_accum += loss.item()\n",
        "            print('Average loss after batch ' + str(step) + ': ' + str(loss_accum / (step + 1)))\n",
        "            print(f\"\\tContrastive Term: {cl_loss:.3f}\")\n",
        "        \n",
        "        if ((step+1) % 35 == 0 or step == len(loader)-1): \n",
        "            # save model after every 35 batches\n",
        "            print(\"Checkpoint saved.\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': loss_accum / (step + 1),\n",
        "            }, chkpt_folder + '/model' + str((step+1) // 35) + '.pt')\n",
        "\n",
        "    # end of this epoch\n",
        "    print('Average training loss: {}'.format(loss_accum / (step + 1)))\n",
        "    return loss_accum / (step + 1)\n",
        "\n",
        "def eval(model, device, loader, evaluator, arr_to_seq):\n",
        "    \"\"\"\n",
        "    Use official OGB evaluator to test results of model output\n",
        "    \"\"\"\n",
        "    seq_ref_list = []\n",
        "    seq_pred_list = []\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                labels = [batch.y[i] for i in range(len(batch.y))]\n",
        "                # no cl by default\n",
        "                pred_list, _, _ = model(batch, labels) \n",
        "\n",
        "            mat = []\n",
        "            for i in range(len(pred_list)):\n",
        "                # get model's predictions\n",
        "                mat.append(torch.argmax(pred_list[i], dim=1).view(-1, 1))\n",
        "            \n",
        "            # save for eval\n",
        "            seq_ref_list.extend(labels)\n",
        "            mat = torch.cat(mat, dim=1)\n",
        "            seq_pred = [arr_to_seq(arr) for arr in mat]\n",
        "            seq_pred_list.extend(seq_pred)\n",
        "\n",
        "    input_dict = {\"seq_ref\": seq_ref_list, \"seq_pred\": seq_pred_list}\n",
        "    return evaluator.eval(input_dict)\n",
        "\n",
        "def randomly_mask(dataset, size):\n",
        "    bool_mask = np.zeros(len(dataset), dtype=bool)\n",
        "    bool_mask[:size] = True\n",
        "    np.random.shuffle(bool_mask)\n",
        "    out = dataset[bool_mask]\n",
        "    return out\n",
        "\n",
        "\n",
        "def main(\n",
        "      starting_chkpt=None, \n",
        "      cl=False, \n",
        "      cl_all=False, \n",
        "      dgi_task=False, \n",
        "      run_eval_every_n_batches=None, \n",
        "      # CL hyperparameter\n",
        "      alpha=0.05\n",
        "  ):\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # model & training conf\n",
        "    depth = 3\n",
        "    epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    step_size = 10\n",
        "    decay_rate = 0.1\n",
        "    weight_decay = 0.00005\n",
        "    dim_h = 512\n",
        "\n",
        "    # model initialization\n",
        "    node_encoder = ASTNodeEncoder(\n",
        "        dim_h, \n",
        "        num_nodetypes=len(nodetypes_mapping['type']), \n",
        "        num_nodeattributes=len(nodeattributes_mapping['attr']), \n",
        "        max_depth=20\n",
        "    )\n",
        "    model = Model(\n",
        "        batch_size, \n",
        "        depth, \n",
        "        dim_h, \n",
        "        max_seq_len, \n",
        "        node_encoder, \n",
        "        vocab2idx, \n",
        "        DEVICE\n",
        "    ).to(DEVICE)\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model # Params: {num_params}')\n",
        "    print(\"-------------\\n\\n\\n\")\n",
        "\n",
        "    # training configuration\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)\n",
        "    multicls_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    starting_epoch = 1\n",
        "\n",
        "    if (starting_chkpt != None):\n",
        "        checkpoint = torch.load(starting_chkpt)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        starting_epoch = checkpoint['epoch']\n",
        "\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_curve = []\n",
        "    trainL_curve = []\n",
        "\n",
        "    def eval_hook():\n",
        "      print('Evaluating...')\n",
        "      train_perf = eval(model, DEVICE, train_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "      valid_perf = eval(model, DEVICE, valid_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "      test_perf = eval(model, DEVICE, test_loader, evaluator, arr_to_seq=lambda arr: decode_arr_to_seq(arr, idx2vocab))\n",
        "\n",
        "      print(\n",
        "          'Train:', train_perf[dataset.eval_metric],\n",
        "          'Validation:', valid_perf[dataset.eval_metric],\n",
        "          'Test:', test_perf[dataset.eval_metric]\n",
        "      )\n",
        "      \n",
        "      return train_perf, valid_perf, test_perf\n",
        "\n",
        "    for epoch in range(starting_epoch, epochs + 1):\n",
        "        print (datetime.datetime.now().strftime('%Y.%m.%d-%H:%M:%S'))\n",
        "        print(\"Epoch {} training...\".format(epoch))\n",
        "        print (\"lr: \", optimizer.param_groups[0]['lr'])\n",
        "        \n",
        "        # training model\n",
        "        train_loss = train(\n",
        "            model, \n",
        "            DEVICE, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            multicls_criterion, \n",
        "            epoch, \n",
        "            cl=cl, \n",
        "            alpha=alpha,\n",
        "            cl_all=cl_all, \n",
        "            dgi_task=dgi_task,\n",
        "            # run evaluation every n batches\n",
        "            eval_hook=lambda x: (\n",
        "                eval_hook() \n",
        "                  if run_eval_every_n_batches is not None and \n",
        "                  (x != 0 and x % run_eval_every_n_batches == 0) \n",
        "                else None\n",
        "            ),\n",
        "        )\n",
        "        scheduler.step()\n",
        "\n",
        "        # run evaluation after each epoch anyways\n",
        "        train_perf, valid_perf, test_perf = eval_hook()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss}\")\n",
        "\n",
        "        train_curve.append(train_perf[dataset.eval_metric])\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "        trainL_curve.append(train_loss)\n",
        "\n",
        "    print('F1')\n",
        "    best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    best_train = max(train_curve)\n",
        "    print('Finished training!')\n",
        "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
        "    print('Finished test: {}, Validation: {}, Train: {}, epoch: {}, best train: {}, best loss: {}'\n",
        "          .format(\n",
        "              test_curve[best_val_epoch], \n",
        "              valid_curve[best_val_epoch], \n",
        "              train_curve[best_val_epoch],\n",
        "              best_val_epoch, \n",
        "              best_train, \n",
        "              min(trainL_curve)\n",
        "          )\n",
        "    )"
      ],
      "metadata": {
        "id": "mh7dpE0hWAx3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_vocab = 5000\n",
        "max_seq_len = 5\n",
        "batch_size = 50\n",
        "\n",
        "# dataset objects\n",
        "# best to load these only once in colab\n",
        "# otherwise, memory never freed and runtime crashes\n",
        "dataset_name = \"ogbg-code2\"\n",
        "dataset = PygGraphPropPredDataset(dataset_name)\n",
        "evaluator = Evaluator(dataset_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "vocab2idx, idx2vocab = get_vocab_mapping([dataset.data.y[i] for i in split_idx['train']], num_vocab)\n",
        "dataset.transform = transforms.Compose([augment_edge, lambda data: encode_y_to_arr(data, vocab2idx, max_seq_len)])\n",
        "\n",
        "nodetypes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'typeidx2type.csv.gz'))\n",
        "nodeattributes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'attridx2attr.csv.gz'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmenm81UkSeJ",
        "outputId": "925a2e6e-b25a-4b5f-f109-26c2a7c3ece5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage of top 5000 vocabulary:\n",
            "0.9025832389087423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_training = randomly_mask(dataset[split_idx[\"train\"]], batch_size * 800)\n",
        "full_valid = randomly_mask(dataset[split_idx[\"valid\"]], batch_size * 800)\n",
        "full_test = randomly_mask(dataset[split_idx[\"test\"]], batch_size * 800)\n",
        "\n",
        "train_loader = DataLoader(full_training, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(full_valid, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(full_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EoDIrwXl9UUY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Leaderboard on OBG for code2: [LINK](https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-code2)"
      ],
      "metadata": {
        "id": "EXBqVk1Ap4QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for multi-layer CL to be enabled, cl and cl_all must both be True\n",
        "# default alpha is 0.05\n",
        "\n",
        "# very overfit\n",
        "# Train: 0.20 Validation: 0.069 Test: 0.072\n",
        "main(\n",
        "  cl=True, \n",
        "  cl_all=True, \n",
        "  dgi_task=False, \n",
        "  run_eval_every_n_batches=300, \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHPw1q10Xfvq",
        "outputId": "83e02695-44a4-46de-8a7f-277a1ed1bb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model # Params: 23612920\n",
            "-------------\n",
            "\n",
            "\n",
            "\n",
            "2023.04.24-20:33:10\n",
            "Epoch 1 training...\n",
            "lr:  0.001\n",
            "Average loss after batch 0: 8.101700782775879\n",
            "\tContrastive Term: -0.103\n",
            "Average loss after batch 1: 7.766380310058594\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 2: 7.057017644246419\n",
            "\tContrastive Term: -0.106\n",
            "Average loss after batch 3: 6.399185299873352\n",
            "\tContrastive Term: -0.108\n",
            "Average loss after batch 4: 5.960888767242432\n",
            "\tContrastive Term: -0.104\n",
            "Average loss after batch 5: 5.561765074729919\n",
            "\tContrastive Term: -0.106\n",
            "Average loss after batch 6: 5.279287678854806\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 7: 5.044170379638672\n",
            "\tContrastive Term: -0.102\n",
            "Average loss after batch 8: 4.883655839496189\n",
            "\tContrastive Term: -0.107\n",
            "Average loss after batch 9: 4.76292052268982\n",
            "\tContrastive Term: -0.101\n",
            "Average loss after batch 10: 4.6109444878318095\n",
            "\tContrastive Term: -0.102\n",
            "Average loss after batch 11: 4.479612906773885\n",
            "\tContrastive Term: -0.104\n",
            "Average loss after batch 12: 4.3923354882460375\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 13: 4.3171162605285645\n",
            "\tContrastive Term: -0.099\n",
            "Average loss after batch 14: 4.263953336079916\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 15: 4.222659558057785\n",
            "\tContrastive Term: -0.099\n",
            "Average loss after batch 16: 4.173451016930973\n",
            "\tContrastive Term: -0.104\n",
            "Average loss after batch 17: 4.14870920446184\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 18: 4.117716425343564\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 19: 4.061030149459839\n",
            "\tContrastive Term: -0.098\n",
            "Average loss after batch 20: 4.03045612289792\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 21: 4.000270117412914\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 22: 3.967923371688179\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 23: 3.9640718698501587\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 24: 3.9374377727508545\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 25: 3.9340239763259888\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 26: 3.9124956749103688\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 27: 3.8851151892117093\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 28: 3.867424701822215\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 29: 3.8498691002527874\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 30: 3.8326118223128782\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 31: 3.809287503361702\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 32: 3.8009694923054087\n",
            "\tContrastive Term: -0.100\n",
            "Average loss after batch 33: 3.784780593479381\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 34: 3.783844757080078\n",
            "\tContrastive Term: -0.093\n",
            "Checkpoint saved.\n",
            "Average loss after batch 35: 3.772787027888828\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 36: 3.7634819842673637\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 37: 3.749799609184265\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 38: 3.7283591429392495\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 39: 3.717928469181061\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 40: 3.7040436093400166\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 41: 3.6907311223802113\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 42: 3.686813609544621\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 43: 3.688205204226754\n",
            "\tContrastive Term: -0.100\n",
            "Average loss after batch 44: 3.67092105017768\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 45: 3.666165284488512\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 46: 3.6568706897979086\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 47: 3.6479446589946747\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 48: 3.637051981322619\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 49: 3.6272246503829955\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 50: 3.6207179836198393\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 51: 3.6135646930107703\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 52: 3.6088541948570394\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 53: 3.6079047609258583\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 54: 3.597812219099565\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 55: 3.5951774333204543\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 56: 3.580587776083695\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 57: 3.5770437388584533\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 58: 3.5698896707114525\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 59: 3.564381770292918\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 60: 3.5537260790340235\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 61: 3.55348495129616\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 62: 3.5465010347820463\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 63: 3.5448634400963783\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 64: 3.539931392669678\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 65: 3.5354372118458604\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 66: 3.5324518075629845\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 67: 3.5306766699342167\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 68: 3.5286085640174756\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 69: 3.519887730053493\n",
            "\tContrastive Term: -0.098\n",
            "Checkpoint saved.\n",
            "Average loss after batch 70: 3.5161506122266744\n",
            "\tContrastive Term: -0.110\n",
            "Average loss after batch 71: 3.5180027385552726\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 72: 3.5144429631429177\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 73: 3.5133805113869743\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 74: 3.5080631828308104\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 75: 3.508754187508633\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 76: 3.510806164184174\n",
            "\tContrastive Term: -0.100\n",
            "Average loss after batch 77: 3.5045700134375157\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 78: 3.5011510124689416\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 79: 3.4950644999742506\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 80: 3.49524689309391\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 81: 3.4917826797904037\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 82: 3.488212533743985\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 83: 3.4859658508073714\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 84: 3.481744785869823\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 85: 3.477776580078657\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 86: 3.476800926800432\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 87: 3.4742506850849497\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 88: 3.471286079856787\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 89: 3.468595552444458\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 90: 3.4690570385901482\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 91: 3.470511283563531\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 92: 3.471446032165199\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 93: 3.464694030741428\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 94: 3.4590561088762786\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 95: 3.454068996012211\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 96: 3.4566292689018643\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 97: 3.450937047296641\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 98: 3.4463535414801703\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 99: 3.4398551654815672\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 100: 3.432425893179261\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 101: 3.4308475162468706\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 102: 3.4318961796251317\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 103: 3.4321206028644857\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 104: 3.4279593354179747\n",
            "\tContrastive Term: -0.087\n",
            "Checkpoint saved.\n",
            "Average loss after batch 105: 3.426101072779242\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 106: 3.4231434046665083\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 107: 3.419171752753081\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 108: 3.414892365079407\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 109: 3.412104845046997\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 110: 3.405601116988036\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 111: 3.4035957136324475\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 112: 3.4026996899495083\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 113: 3.404421095262494\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 114: 3.4026700144228728\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 115: 3.397810023406456\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 116: 3.394822764600444\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 117: 3.3881453013016007\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 118: 3.3910170723410213\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 119: 3.388233502705892\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 120: 3.3874987767747613\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 121: 3.3871539029918734\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 122: 3.386437076863235\n",
            "\tContrastive Term: -0.096\n",
            "Average loss after batch 123: 3.38297878349981\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 124: 3.382887331008911\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 125: 3.380698953356062\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 126: 3.382048280220332\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 127: 3.3803325090557337\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 128: 3.378484605818756\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 129: 3.3770106884149405\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 130: 3.3784484062486024\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 131: 3.3763164498589258\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 132: 3.3726205969215335\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 133: 3.372948639428438\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 134: 3.3741468288280347\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 135: 3.3724525062476887\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 136: 3.373235103857778\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 137: 3.368845504263173\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 138: 3.368152606401512\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 139: 3.367572055544172\n",
            "\tContrastive Term: -0.092\n",
            "Checkpoint saved.\n",
            "Average loss after batch 140: 3.367392599159944\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 141: 3.3630886430471714\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 142: 3.361354264346036\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 143: 3.3605382955736585\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 144: 3.3571298878768396\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 145: 3.3562902489753617\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 146: 3.353292264094969\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 147: 3.352714156782305\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 148: 3.351281977339879\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 149: 3.3507668860753377\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 150: 3.3486208505188393\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 151: 3.3460352201210823\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 152: 3.3463368431415432\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 153: 3.343987359629049\n",
            "\tContrastive Term: -0.098\n",
            "Average loss after batch 154: 3.3423962423878333\n",
            "\tContrastive Term: -0.097\n",
            "Average loss after batch 155: 3.3396879067787757\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 156: 3.3374586166090268\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 157: 3.3359031526348257\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 158: 3.3305225567247883\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 159: 3.329492561519146\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 160: 3.3289659230605415\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 161: 3.327851707552686\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 162: 3.326249989995196\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 163: 3.3243655140806987\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 164: 3.3224511262142298\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 165: 3.3220072912882608\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 166: 3.320205532862041\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 167: 3.3179145412785664\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 168: 3.3145012926067827\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 169: 3.312263881458956\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 170: 3.3120139718752855\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 171: 3.3107643889826397\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 172: 3.3083839375159645\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 173: 3.306805538034987\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 174: 3.3050883047921316\n",
            "\tContrastive Term: -0.093\n",
            "Checkpoint saved.\n",
            "Average loss after batch 175: 3.3045542497526514\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 176: 3.3027202099730064\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 177: 3.301639120230514\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 178: 3.2995415906000405\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 179: 3.2969437691900465\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 180: 3.2974859100679006\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 181: 3.2965427152403106\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 182: 3.2946318029705943\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 183: 3.2926840924698375\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 184: 3.290508200671222\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 185: 3.2924965940495974\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 186: 3.2931033302755917\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 187: 3.294085464578994\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 188: 3.2938667062729126\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 189: 3.2920428539577284\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 190: 3.2911694711415556\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 191: 3.29106392711401\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 192: 3.2872309277094707\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 193: 3.287731696649925\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 194: 3.2857661944169263\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 195: 3.2838525346347263\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 196: 3.2847471636563994\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 197: 3.2824359650563713\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 198: 3.2801901179941457\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 199: 3.27993238568306\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 200: 3.2785468540381437\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 201: 3.2790034879552254\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 202: 3.2751164812172573\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 203: 3.2749062098708808\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 204: 3.271617167170455\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 205: 3.271451954702729\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 206: 3.272472874553883\n",
            "\tContrastive Term: -0.099\n",
            "Average loss after batch 207: 3.2721686088121853\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 208: 3.27182860807939\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 209: 3.2717546020235333\n",
            "\tContrastive Term: -0.095\n",
            "Checkpoint saved.\n",
            "Average loss after batch 210: 3.269523830775401\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 211: 3.269240106051823\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 212: 3.2688989538541984\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 213: 3.2680863064026164\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 214: 3.2670056875361952\n",
            "\tContrastive Term: -0.095\n",
            "Average loss after batch 215: 3.2672625184059143\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 216: 3.2669424703044276\n",
            "\tContrastive Term: -0.093\n",
            "Average loss after batch 217: 3.266667906297456\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 218: 3.2684243324140434\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 219: 3.266892147064209\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 220: 3.267166540094091\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 221: 3.2652987585411415\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 222: 3.2630079551662563\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 223: 3.2626267043607577\n",
            "\tContrastive Term: -0.100\n",
            "Average loss after batch 224: 3.2617768828074136\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 225: 3.2597458774009636\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 226: 3.2574636390030647\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 227: 3.257441386841891\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 228: 3.2566321968511724\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 229: 3.25688519374184\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 230: 3.255766043931375\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 231: 3.2550453502556373\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 232: 3.2556445158602343\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 233: 3.255472790481698\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 234: 3.254722793051537\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 235: 3.2552499407428805\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 236: 3.2540630378803623\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 237: 3.2548441736638045\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 238: 3.25515593045925\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 239: 3.2545845756928125\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 240: 3.2555929931862226\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 241: 3.2555051293254884\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 242: 3.255093355728275\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 243: 3.255315547106696\n",
            "\tContrastive Term: -0.074\n",
            "Average loss after batch 244: 3.254915509905134\n",
            "\tContrastive Term: -0.084\n",
            "Checkpoint saved.\n",
            "Average loss after batch 245: 3.2541545920255706\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 246: 3.2557961129949162\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 247: 3.25593380197402\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 248: 3.2556963340345635\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 249: 3.254202386856079\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 250: 3.254386001374142\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 251: 3.254289259986272\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 252: 3.2557353219495933\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 253: 3.2560849274237325\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 254: 3.255071371676875\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 255: 3.2546518240123987\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 256: 3.253928285628441\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 257: 3.253207203029662\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 258: 3.2542737266732\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 259: 3.253417965082022\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 260: 3.25297121709334\n",
            "\tContrastive Term: -0.072\n",
            "Average loss after batch 261: 3.252247864963444\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 262: 3.2525468897003638\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 263: 3.2531022745551486\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 264: 3.252745478108244\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 265: 3.252391885097762\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 266: 3.2500970122519504\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 267: 3.2487665450395045\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 268: 3.2478429827991473\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 269: 3.2472063029253926\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 270: 3.2470179123192255\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 271: 3.245921673143611\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 272: 3.2459047705262574\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 273: 3.245874480609476\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 274: 3.245588354630904\n",
            "\tContrastive Term: -0.071\n",
            "Average loss after batch 275: 3.2446944964104802\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 276: 3.2441218252216437\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 277: 3.2437228367483018\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 278: 3.2432212197225154\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 279: 3.2427471118313926\n",
            "\tContrastive Term: -0.089\n",
            "Checkpoint saved.\n",
            "Average loss after batch 280: 3.24206436147045\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 281: 3.241272674384692\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 282: 3.239898475235848\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 283: 3.23923521814212\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 284: 3.240062902684797\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 285: 3.2405666713114387\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 286: 3.23957739474466\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 287: 3.2393031898472042\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 288: 3.239744239199945\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 289: 3.2394098931345447\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 290: 3.240480422154325\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 291: 3.2403727778016704\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 292: 3.239236695774586\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 293: 3.237703035477878\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 294: 3.238434292098223\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 295: 3.2373218359173954\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 296: 3.2367030988237273\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 297: 3.2363285514332305\n",
            "\tContrastive Term: -0.075\n",
            "Average loss after batch 298: 3.235808924289053\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 299: 3.2367555626233417\n",
            "\tContrastive Term: -0.080\n",
            "Evaluating...\n",
            "Train: 0.05441544733044732 Validation: 0.04986024065876807 Test: 0.04828281316116198\n",
            "Average loss after batch 300: 3.236750152023924\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 301: 3.235230197180186\n",
            "\tContrastive Term: -0.076\n",
            "Average loss after batch 302: 3.234605138451353\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 303: 3.2338217363545767\n",
            "\tContrastive Term: -0.077\n",
            "Average loss after batch 304: 3.2349845550099356\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 305: 3.2344907101462868\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 306: 3.234280142023043\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 307: 3.2327590902130323\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 308: 3.232026917263142\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 309: 3.231097641298848\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 310: 3.229647295268019\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 311: 3.23056813616019\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 312: 3.230627744723433\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 313: 3.2298392156127154\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 314: 3.229041105603415\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 315: 3.229366677471354\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 316: 3.2288326134064973\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 317: 3.2284679780216337\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 318: 3.2279513749209316\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 319: 3.228048783540726\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 320: 3.2269202816152127\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 321: 3.226477919157988\n",
            "\tContrastive Term: -0.092\n",
            "Average loss after batch 322: 3.2260083295981583\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 323: 3.2253474835996276\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 324: 3.227018019602849\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 325: 3.2266319726873762\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 326: 3.226619980021719\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 327: 3.226532807437385\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 328: 3.2254752960610897\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 329: 3.224759057073882\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 330: 3.223270293088838\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 331: 3.22185486339661\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 332: 3.2210321691301136\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 333: 3.220258239500537\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 334: 3.2195606779696337\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 335: 3.2196467923266545\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 336: 3.21962372839274\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 337: 3.219100334235197\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 338: 3.2193638110934457\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 339: 3.2188490573097677\n",
            "\tContrastive Term: -0.090\n",
            "Average loss after batch 340: 3.218663080696486\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 341: 3.2185391090069597\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 342: 3.218062676076639\n",
            "\tContrastive Term: -0.086\n",
            "Average loss after batch 343: 3.217198134161705\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 344: 3.2172989513563075\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 345: 3.2159627724245103\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 346: 3.216120405224627\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 347: 3.2153589232214568\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 348: 3.2158223387163485\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 349: 3.214752947262355\n",
            "\tContrastive Term: -0.081\n",
            "Checkpoint saved.\n",
            "Average loss after batch 350: 3.2142110393937156\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 351: 3.213351851159876\n",
            "\tContrastive Term: -0.082\n",
            "Average loss after batch 352: 3.2128775734401627\n",
            "\tContrastive Term: -0.088\n",
            "Average loss after batch 353: 3.212503040577732\n",
            "\tContrastive Term: -0.081\n",
            "Average loss after batch 354: 3.211988334252801\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 355: 3.21214875210537\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 356: 3.2116752285249426\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 357: 3.211879981962662\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 358: 3.2111352751846103\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 359: 3.2115762677457598\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 360: 3.2108837640186425\n",
            "\tContrastive Term: -0.089\n",
            "Average loss after batch 361: 3.210826493758523\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 362: 3.2101470458606056\n",
            "\tContrastive Term: -0.078\n",
            "Average loss after batch 363: 3.2095873873312395\n",
            "\tContrastive Term: -0.073\n",
            "Average loss after batch 364: 3.208181969107014\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 365: 3.207455448765572\n",
            "\tContrastive Term: -0.080\n",
            "Average loss after batch 366: 3.2071455181132222\n",
            "\tContrastive Term: -0.079\n",
            "Average loss after batch 367: 3.2061405894548995\n",
            "\tContrastive Term: -0.087\n",
            "Average loss after batch 368: 3.205764694265557\n",
            "\tContrastive Term: -0.094\n",
            "Average loss after batch 369: 3.204552826365909\n",
            "\tContrastive Term: -0.084\n",
            "Average loss after batch 370: 3.205216610849386\n",
            "\tContrastive Term: -0.091\n",
            "Average loss after batch 371: 3.2049549869311753\n",
            "\tContrastive Term: -0.083\n",
            "Average loss after batch 372: 3.2044405087069596\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 373: 3.2035180489647197\n",
            "\tContrastive Term: -0.085\n",
            "Average loss after batch 374: 3.202641098022461\n",
            "\tContrastive Term: -0.095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wndF4IApYz9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}